%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{General}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}


\newcommand\thesymbolfootnote{\fnsymbol{footnote}}\let\thenumberfootnote\thefootnote

\title{kerch}
\date{May 24, 2024}
\release{0.3.1}
\author{HENRI DE PLAEN}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


\sphinxAtStartPar
This package is meant for various kernel\sphinxhyphen{}based operations. For the moment,
only the kernel module is available, but a lot of models will follow soon.
If you experience any bug, please let me now: this is only a pre\sphinxhyphen{}alpha version.

\sphinxstepscope


\chapter{Why Kerch?}
\label{\detokenize{general/index:why-kerch}}\label{\detokenize{general/index::doc}}

\section{Easy\sphinxhyphen{}to\sphinxhyphen{}Use}
\label{\detokenize{general/index:easy-to-use}}

\section{High and Low Level}
\label{\detokenize{general/index:high-and-low-level}}

\section{Visualization and Monitoring}
\label{\detokenize{general/index:visualization-and-monitoring}}

\section{Comprehensible Errors}
\label{\detokenize{general/index:comprehensible-errors}}

\section{Efficient GPU Computation}
\label{\detokenize{general/index:efficient-gpu-computation}}

\section{Cache Management}
\label{\detokenize{general/index:cache-management}}

\section{Stochastic Training}
\label{\detokenize{general/index:stochastic-training}}

\section{Manifold Optimization}
\label{\detokenize{general/index:manifold-optimization}}

\section{Fully Customizable}
\label{\detokenize{general/index:fully-customizable}}
\sphinxstepscope


\chapter{Install}
\label{\detokenize{general/install:install}}\label{\detokenize{general/install::doc}}
\sphinxAtStartPar
As for now, there are two ways to install the package.


\section{Using PIP}
\label{\detokenize{general/install:using-pip}}
\sphinxAtStartPar
Using pip, it suffices to run \sphinxcode{\sphinxupquote{pip install kerch}}. Just rerun this command to update the package to its newest version.


\section{From source}
\label{\detokenize{general/install:from-source}}
\sphinxAtStartPar
You can also install the package directly from the GitHub repository.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
git\PYG{+w}{ }clone\PYG{+w}{ }\PYGZhy{}\PYGZhy{}recursive\PYG{+w}{ }https://github.com/hdeplaen/kerch
\PYG{n+nb}{cd}\PYG{+w}{ }kerch
python\PYG{+w}{ }setup.py\PYG{+w}{ }install
\end{sphinxVerbatim}

\sphinxstepscope


\chapter{First Steps}
\label{\detokenize{general/first_steps:first-steps}}\label{\detokenize{general/first_steps::doc}}
\sphinxstepscope


\chapter{Contribute to Kerch}
\label{\detokenize{general/contribute:contribute-to-kerch}}\label{\detokenize{general/contribute::doc}}
\sphinxstepscope


\chapter{Using Kernels}
\label{\detokenize{examples/kernels:using-kernels}}\label{\detokenize{examples/kernels::doc}}

\section{Out\sphinxhyphen{}of\sphinxhyphen{}sample normalized and centered kernels}
\label{\detokenize{examples/kernels:out-of-sample-normalized-and-centered-kernels}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{polynomial}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{center}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{}Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{kernels-1}.pdf}
\end{figure}

\sphinxstepscope


\chapter{Working with Levels}
\label{\detokenize{examples/levels:working-with-levels}}\label{\detokenize{examples/levels::doc}}
\sphinxstepscope


\chapter{Models}
\label{\detokenize{examples/models:models}}\label{\detokenize{examples/models::doc}}

\section{Built\sphinxhyphen{}In Models}
\label{\detokenize{examples/models:built-in-models}}

\subsection{Training and tuning an LS\sphinxhyphen{}SVM}
\label{\detokenize{examples/models:training-and-tuning-an-ls-svm}}

\section{New Models}
\label{\detokenize{examples/models:new-models}}
\sphinxstepscope


\chapter{Kernel Module}
\label{\detokenize{kernel/index:kernel-module}}\label{\detokenize{kernel/index::doc}}

\section{Introduction}
\label{\detokenize{kernel/index:introduction}}
\sphinxAtStartPar
This module contains many different types of kernels. Each kernel is created based on some hyperparameters and a sample
dataset.


\subsection{General}
\label{\detokenize{kernel/index:general}}
\sphinxAtStartPar
If no sample dataset is provided, a random one will be initialized. This dataset can always be reinitialized
(\sphinxtitleref{init\_sample}) or the alue of the datapoints can be updated updated (\sphinxtitleref{update\_sample}). In the latter case, the
dimensions have to be matching. Furthermore, the sample dataset can also work in a stochastic manner, of which the
indices can be controlled through the \sphinxtitleref{reset} method.

\sphinxAtStartPar
Both the value of the sample datapoints as the hyperparameters are compatible with gradient graphs of PyTorch. If such
a graph is to be computed, this has to be specifically specified during constructions.

\sphinxAtStartPar
All kernels can be centered, either implicitly using statistics on the kernel matrix of the sample dataset, either
explicitly using a statistic on the explicit feature map. In the former case, this cannot be extended to fully
out\sphinxhyphen{}of\sphinxhyphen{}sample computations.

\sphinxAtStartPar
At last, a Nystrom kernel is also implemented, which created an explicit feature map based on any kernel (possibly
implicit), using eigendocomposition. Among other things, this can serve as a solution for centering fully out\sphinxhyphen{}of\sphinxhyphen{}sample
kernel matrices of implicitly defined kernels.

\sphinxAtStartPar
The general structure of the module is based around an abstract kernel class \sphinxtitleref{base}, of which
\sphinxtitleref{kerch.kernle.implicit} and \sphinxtitleref{explicit} inherit. All other kernels inherit of one of these two at the exception
of \sphinxtitleref{polynomial} which directly inherits \sphinxtitleref{base} as it has a primal formulation and a dual
formulation which can be computed otherwise than with an inner product of the explicit feature map.


\subsection{Kernel Factory}
\label{\detokenize{kernel/index:kernel-factory}}\index{factory() (in module kerch.kernel)@\spxentry{factory()}\spxextra{in module kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/index:kerch.kernel.factory}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{factory}}}{\emph{\DUrole{n}{kernel\_type}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}rbf\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{kerch.kernel.kernel.Kernel}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Creates a kernel based on the specified name with the specified arguments. This is the same as
calling \sphinxtitleref{kerch.kernel.name(*args, **kwargs)} (if \sphinxtitleref{name} is not a string here). This allows for the creation of kernel where
the name of kernel is passed as a string.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_type}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Type of kernel chosen. For the possible choices, please refer to the \sphinxtitleref{Factory Type} column of the
{\hyperref[\detokenize{kernel/index::doc}]{\sphinxcrossref{\DUrole{doc}{Kernel Module}}}} documentation. Defaults to \sphinxcode{\sphinxupquote{kerch.DEFAULT\_KERNEL\_TYPE}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Arguments to be passed to the kernel constructor, such as \sphinxtitleref{sample} or \sphinxtitleref{sigma}. If an argument is
passed that does not exist (e.g. \sphinxtitleref{sigma} to a \sphinxtitleref{linear} kernel), it will just be neglected. For the default
values, please refer to the default values of the requested kernel.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
An instantiation of the specified kernel.

\end{description}\end{quote}

\end{fulllineitems}



\section{Examples}
\label{\detokenize{kernel/index:examples}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{polynomial}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{center}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{normalize}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{index-1}.pdf}
\end{figure}


\section{Different Kernels}
\label{\detokenize{kernel/index:different-kernels}}

\subsection{Generic Kernels}
\label{\detokenize{kernel/index:generic-kernels}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Explicit Feature Map
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Kernel
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameters
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Factory Type
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/linear:kerch.kernel.Linear}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Linear}}}}}
&
\sphinxAtStartPar
\(\phi(x) = x\)
&
\sphinxAtStartPar
\(k(x,y)=\phi(x)^\top\phi(y)\)
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’linear’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RBF}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)=\exp\left(-\frac{\left\lVert x-y\right\rVert^2_2}{2\sigma^2}\right)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’rbf’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Laplacian}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)=\exp\left(-\frac{\left\lVert x-y\right\rVert_2}{2\sigma^2}\right)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’laplacian’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Polynomial}}}}}
&
\sphinxAtStartPar
See documentation
&
\sphinxAtStartPar
\(k(x,y) = \left(x^\top y + \beta\right)^\alpha\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{alpha}}, \sphinxcode{\sphinxupquote{beta}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’polynomial’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/cosine:kerch.kernel.Cosine}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Cosine}}}}}
&
\sphinxAtStartPar
\(\phi(x) = \frac{x}{\left\lVert x\right\rVert_2}\)
&
\sphinxAtStartPar
\(k(x,y)=\phi(x)^\top\phi(y)\)
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’cosine’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Sigmoid}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y) = \sigma\left( a (x^\top y) + b \right)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{a}}, \sphinxcode{\sphinxupquote{b}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’sigmoid’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/rff:kerch.kernel.RFF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RFF}}}}}
&
\sphinxAtStartPar
See documentation
&
\sphinxAtStartPar
\(k(x,y)=\phi(x)^\top\phi(y)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{num\_weights}}, \sphinxcode{\sphinxupquote{sigma}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’rff’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Nystrom}}}}}
&
\sphinxAtStartPar
See documentation
&
\sphinxAtStartPar
See documentation
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{dim}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’nystrom’}}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\subsubsection{Linear Kernel}
\label{\detokenize{kernel/generic/linear:linear-kernel}}\label{\detokenize{kernel/generic/linear::doc}}

\paragraph{Class}
\label{\detokenize{kernel/generic/linear:class}}\index{Linear (class in kerch.kernel)@\spxentry{Linear}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Linear}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.explicit.Explicit}}}}}

\sphinxAtStartPar
Linear kernel.
\begin{equation*}
\begin{split}k(x,y) = x^\top y.\end{split}
\end{equation*}
\sphinxAtStartPar
To this kernel also corresponds the explicit finite dimensional feature map \(\phi(x)=x\).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.Linear property)@\spxentry{C}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.Linear property)@\spxentry{Corr}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Linear property)@\spxentry{Cov}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Linear property)@\spxentry{K}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/linear:kerch.kernel.Linear.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Linear property)@\spxentry{Phi}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/generic/linear:kerch.kernel.Linear.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Linear method)@\spxentry{c()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Linear method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Linear property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Linear property)@\spxentry{centered}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Linear method)@\spxentry{corr()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Linear method)@\spxentry{cov()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Linear property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Linear property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Linear property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Linear property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Linear property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Linear property)@\spxentry{explicit}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Linear method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.Linear method)@\spxentry{forward()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed() (kerch.kernel.Linear method)@\spxentry{hparams\_fixed()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.hparams_fixed}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Linear property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Linear property)@\spxentry{idx}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Linear method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Linear method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Linear method)@\spxentry{k()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Linear property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Linear property)@\spxentry{normalized}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Linear property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Linear property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.Linear method)@\spxentry{phi()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Linear method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Linear method)@\spxentry{reset()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Linear property)@\spxentry{sample}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Linear property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Linear property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Linear property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Linear method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Linear method)@\spxentry{train()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Linear method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Linear method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Linear method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Linear method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/linear:kerch.kernel.Linear.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/generic/linear:examples}}

\subparagraph{Sine}
\label{\detokenize{kernel/generic/linear:sine}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1.5}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{linear-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{linear-1_01}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/linear:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/linear:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-a40672f349c8dfcebbc5bd0ad946f86bc33e2823.pdf}

\sphinxstepscope


\subsubsection{RBF Kernel}
\label{\detokenize{kernel/generic/rbf:rbf-kernel}}\label{\detokenize{kernel/generic/rbf::doc}}

\paragraph{Class}
\label{\detokenize{kernel/generic/rbf:class}}\index{RBF (class in kerch.kernel)@\spxentry{RBF}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{RBF}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.statistics.exponential.Exponential}}}}}

\sphinxAtStartPar
RBF kernel (radial basis function) of bandwidth \(\sigma>0\).
\begin{equation*}
\begin{split}k(x,y) = \exp\left( -\frac{\lVert x-y \rVert_2^2}{2\sigma^2} \right).\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If working with big datasets, one may consider an explicit approximation of the RBF kernel using
Random Fourier Features ({\hyperref[\detokenize{kernel/generic/rff:kerch.kernel.RFF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{RFF}}}}}). This will be faster provided \(2\times\texttt{num_weights} < n\),
where \(\texttt{num_weights}\) is the number of weights used to control the RFF approximation and \(n\) is
the number of datapoints. The latter class however does not offer so much flexibility, as the automatic determination
of the bandwidth \(\sigma\) using a heuristic for example.

\sphinxAtStartPar
Other considerations may come into play. If a centered or normalized kernel on an out\sphinxhyphen{}of\sphinxhyphen{}sample is required, this may require extra
computations when directly using the kernel matrix as doing it on the explicit feature is more straightforward.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The norm inside the exponential is always squared. If you wish a non\sphinxhyphen{}squared norm, this corresponds to the
{\hyperref[\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Laplacian}}}}} kernel. If another distance than the Euclidean one is required, we refer to
the more generic {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Exponential}}}}} kernel.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.RBF property)@\spxentry{Corr}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.RBF property)@\spxentry{Cov}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.RBF property)@\spxentry{K}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.RBF property)@\spxentry{Phi}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.RBF method)@\spxentry{c()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.RBF method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.RBF property)@\spxentry{cache\_level}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.RBF property)@\spxentry{centered}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.RBF method)@\spxentry{corr()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.RBF method)@\spxentry{cov()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.RBF property)@\spxentry{current\_sample}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.RBF property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.RBF property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.RBF property)@\spxentry{dim\_input}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.RBF property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.RBF property)@\spxentry{explicit}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.RBF method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.RBF method)@\spxentry{forward()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.RBF property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.RBF property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.RBF property)@\spxentry{idx}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.RBF method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.RBF method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.RBF method)@\spxentry{k()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.RBF property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.RBF property)@\spxentry{normalized}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.RBF property)@\spxentry{num\_idx}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.RBF property)@\spxentry{num\_sample}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.RBF method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.RBF method)@\spxentry{reset()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.RBF property)@\spxentry{sample}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.RBF property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.RBF property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.RBF property)@\spxentry{sigma}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.RBF property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{squared (kerch.kernel.RBF property)@\spxentry{squared}\spxextra{kerch.kernel.RBF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.squared}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{squared}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the norm in the exponential is squared.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.RBF method)@\spxentry{stochastic()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.RBF method)@\spxentry{train()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.RBF method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.RBF method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.RBF method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.RBF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rbf:kerch.kernel.RBF.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/generic/rbf:examples}}

\subparagraph{Sine}
\label{\detokenize{kernel/generic/rbf:sine}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sigma = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sigma = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{rbf-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{rbf-1_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{rbf-1_02}.pdf}
\end{figure}


\subparagraph{Time}
\label{\detokenize{kernel/generic/rbf:time}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RBF with sigma }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{rbf-2}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/rbf:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/rbf:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-598d9a99c2229fe8e98fc4c10f3e1504ea9e4a21.pdf}

\sphinxstepscope


\subsubsection{Laplacian Kernel}
\label{\detokenize{kernel/generic/laplacian:laplacian-kernel}}\label{\detokenize{kernel/generic/laplacian::doc}}\index{Laplacian (class in kerch.kernel)@\spxentry{Laplacian}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Laplacian}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.statistics.exponential.Exponential}}}}}

\sphinxAtStartPar
Laplacian kernel.
\begin{equation*}
\begin{split}k(x,y) = \exp\left( -\frac{\lVert x-y \rVert_2}{\sqrt{2}\sigma} \right).\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The norm inside the exponential is never squared. If you wish a squared norm, this corresponds to the
{\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{RBF}}}}} kernel. If another distance than the Euclidean one is required, we refer to
the more generic {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Exponential}}}}} kernel.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Laplacian property)@\spxentry{Corr}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Laplacian property)@\spxentry{Cov}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Laplacian property)@\spxentry{K}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Laplacian property)@\spxentry{Phi}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Laplacian method)@\spxentry{c()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Laplacian method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Laplacian property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Laplacian property)@\spxentry{centered}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Laplacian method)@\spxentry{corr()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Laplacian method)@\spxentry{cov()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Laplacian property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Laplacian property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Laplacian property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Laplacian property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Laplacian property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Laplacian property)@\spxentry{explicit}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Laplacian method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Laplacian method)@\spxentry{forward()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Laplacian property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Laplacian property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Laplacian property)@\spxentry{idx}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Laplacian method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Laplacian method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Laplacian method)@\spxentry{k()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Laplacian property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Laplacian property)@\spxentry{normalized}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Laplacian property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Laplacian property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Laplacian method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Laplacian method)@\spxentry{reset()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Laplacian property)@\spxentry{sample}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Laplacian property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Laplacian property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Laplacian property)@\spxentry{sigma}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Laplacian property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{squared (kerch.kernel.Laplacian property)@\spxentry{squared}\spxextra{kerch.kernel.Laplacian property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.squared}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{squared}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the norm in the exponential is squared.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Laplacian method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Laplacian method)@\spxentry{train()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Laplacian method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Laplacian method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Laplacian method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Laplacian method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/generic/laplacian:examples}}

\subparagraph{Sine}
\label{\detokenize{kernel/generic/laplacian:sine}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sigma = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mi}{2}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sigma = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{laplacian-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{laplacian-1_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{laplacian-1_02}.pdf}
\end{figure}


\subparagraph{Comparison with RBF}
\label{\detokenize{kernel/generic/laplacian:comparison-with-rbf}}
\sphinxAtStartPar
As the 2\sphinxhyphen{}norm between the inputs is not squared, the result is essentially more drastically descreasing in the bulk,
but heavier in the tail compared to the {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{RBF}}}}} kernel. This will also lead to a proportionally
higher sigma for a “similar” result.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} automatic bandwidth with heuristic}
\PYG{n}{k\PYGZus{}laplacian} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{k\PYGZus{}rbf} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{fig1}\PYG{p}{,} \PYG{n}{axs1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}laplacian}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplacian (\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}laplacian}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im1} \PYG{o}{=} \PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}rbf}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RBF (\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}rbf}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{fig1}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs1}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}  unity bandwidth}
\PYG{n}{k\PYGZus{}laplacian\PYGZus{}sigma1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{f\PYGZus{}rbf\PYGZus{}sigma1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{fig2}\PYG{p}{,} \PYG{n}{axs2} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}laplacian\PYGZus{}sigma1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplacian (\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}laplacian\PYGZus{}sigma1}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im2} \PYG{o}{=} \PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{f\PYGZus{}rbf\PYGZus{}sigma1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RBF (\PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{f\PYGZus{}rbf\PYGZus{}sigma1}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{fig2}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs2}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{laplacian-2_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{laplacian-2_01}.pdf}
\end{figure}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{k\PYGZus{}rbf} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{k\PYGZus{}laplacian} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{(}\PYG{n}{k\PYGZus{}rbf}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{k\PYGZus{}laplacian}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RBF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Laplacian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{laplacian-3}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/laplacian:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{laplacian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/laplacian:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-63cd7d7d462606fc2ab8385588c61a6fc78539ed.pdf}

\sphinxstepscope


\subsubsection{Polynomial Kernel}
\label{\detokenize{kernel/generic/polynomial:polynomial-kernel}}\label{\detokenize{kernel/generic/polynomial::doc}}

\paragraph{Class}
\label{\detokenize{kernel/generic/polynomial:class}}\index{Polynomial (class in kerch.kernel)@\spxentry{Polynomial}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Polynomial}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}, {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.explicit.Explicit}}}}}

\sphinxAtStartPar
Polynomial kernel of degree \(\alpha \geq 0\) and parameter \(\beta\).
\begin{equation*}
\begin{split}k(x,y) = \left(x^\top y + \beta\right)^\alpha.\end{split}
\end{equation*}
\sphinxAtStartPar
Provided the degree \(\alpha\) is a natural number, this kernel accepts both an explicit feature map and an equivalent kernel formulation not depending on the
inner product of the explicit feature maps (implicit). Its components are given by
\begin{equation*}
\begin{split}\left[\phi(x)\right]_k = \sqrt{\frac{\alpha!}{j_0!j_1! \ldots j_\texttt{dim_input}!}}x_0^{j_0}x_1^{j_1}\ldots x_{\texttt{dim_input}-1}^{j_{\texttt{dim_input}-1}}\sqrt{\beta}^{j_\texttt{dim_input}},\end{split}
\end{equation*}
\sphinxAtStartPar
where \(k = 0, \ldots, \texttt{dim_feature}-1\) correspond to all permutations satisfying
\begin{equation*}
\begin{split}j_0 + j_1 + \ldots + j_{\texttt{dim_input}} = \alpha,\end{split}
\end{equation*}
\sphinxAtStartPar
and
\begin{equation*}
\begin{split}\texttt{dim_feature} = \left(\begin{array}{c}\texttt{dim_input} + \alpha \\ \alpha\end{array}\right).\end{split}
\end{equation*}
\sphinxAtStartPar
One can verify that \(k(x,y) = \phi(x)^\top\phi(y)\). An example is also given in the Example section of its
documentation.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
For a natural number degree \(\alpha\) . The computation of a kernel matrix of \(n\) points is typically
more efficient to compute as an inner product of the explicit feature map if \(\texttt{dim_feature} < n\)
and using the kernel formula otherwise. If the degree is not a natural number, only the latter is possible as
the explicit feature map does not exist.

\sphinxAtStartPar
This can be specified when calling \sphinxcode{\sphinxupquote{Polynomial.k()}} by specifying the boolean \sphinxcode{\sphinxupquote{explicit}} to
\sphinxcode{\sphinxupquote{True}} (using the explicit feature map) or \sphinxcode{\sphinxupquote{False}} (directly using the kernel formula).

\sphinxAtStartPar
Other considerations may come into play. If a centered or normalized kernel on an out\sphinxhyphen{}of\sphinxhyphen{}sample is required, this may require extra
computations when directly using the kernel matrix as doing it on the explicit feature is more straightforward.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{alpha}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Degree \(\alpha\) of the polynomial kernel. Defaults to 2.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{beta}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Value \(\beta\) of the polynomial kernel. Defaults to 1.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{alpha\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the degree is to be computed. If so, a graph is computed
and the degree can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{beta\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the degree is to be computed. If so, a graph is computed
and the degree can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.Polynomial property)@\spxentry{C}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.Polynomial property)@\spxentry{Corr}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Polynomial property)@\spxentry{Cov}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Polynomial property)@\spxentry{K}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Polynomial property)@\spxentry{Phi}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{alpha (kerch.kernel.Polynomial property)@\spxentry{alpha}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.alpha}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{alpha}}}
\pysigstopsignatures
\sphinxAtStartPar
Degree of the polynomial. This is argument plays a similar role to the bandwidth of
an exponential kernel, such as the RBF kernel.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The explicit feature map only exists if the degree is a natural number.
\end{sphinxadmonition}

\end{fulllineitems}

\index{alpha\_trainable (kerch.kernel.Polynomial property)@\spxentry{alpha\_trainable}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.alpha_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{alpha\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating if the alpha/degree is trainable. In other words, this argument provides or not a gradient
to the degree for potential gradient\sphinxhyphen{}based training.

\end{fulllineitems}

\index{beta (kerch.kernel.Polynomial property)@\spxentry{beta}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.beta}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{beta}}}
\pysigstopsignatures
\sphinxAtStartPar
Beta of the polynomial.

\end{fulllineitems}

\index{beta\_trainable (kerch.kernel.Polynomial property)@\spxentry{beta\_trainable}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.beta_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{beta\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating if the beta is trainable.

\end{fulllineitems}

\index{c() (kerch.kernel.Polynomial method)@\spxentry{c()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Polynomial method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Polynomial property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Polynomial property)@\spxentry{centered}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Polynomial method)@\spxentry{corr()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Polynomial method)@\spxentry{cov()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Polynomial property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Polynomial property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Polynomial property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
Feature dimension. Provided the degree \(\alpha\) is a natural number, it is given by
\begin{equation*}
\begin{split}\texttt{dim_feature} = \left(\begin{array}{c}\texttt{dim_input} + \alpha \\ \alpha\end{array}\right).\end{split}
\end{equation*}
\sphinxAtStartPar
If the degree \(\alpha\) is not a natural number, the explicit feature does not exist and by consequence
the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Polynomial property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Polynomial property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Polynomial property)@\spxentry{explicit}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise. This is the case only if the degree
\(\alpha\) is a natural number.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Polynomial method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.Polynomial method)@\spxentry{forward()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Polynomial property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Polynomial property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Polynomial property)@\spxentry{idx}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Polynomial method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Polynomial method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Polynomial method)@\spxentry{k()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
For the specific case of the polynomial kernel, the optimal value for \sphinxcode{\sphinxupquote{explicit}} is automatically determined
based on the size of the inputs if \sphinxcode{\sphinxupquote{explicit=None}}. This does not take into account possible transforms.
\end{sphinxadmonition}

\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Polynomial property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Polynomial property)@\spxentry{normalized}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Polynomial property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Polynomial property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.Polynomial method)@\spxentry{phi()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Polynomial method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Polynomial method)@\spxentry{reset()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Polynomial property)@\spxentry{sample}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Polynomial property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Polynomial property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Polynomial property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Polynomial method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Polynomial method)@\spxentry{train()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Polynomial method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Polynomial method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Polynomial method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Polynomial method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/generic/polynomial:examples}}

\subparagraph{Sine}
\label{\detokenize{kernel/generic/polynomial:sine}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1.5}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{polynomial-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{polynomial-1_01}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/polynomial:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{polynomial}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\subparagraph{Influence of the parameters}
\label{\detokenize{kernel/generic/polynomial:influence-of-the-parameters}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{k3} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{k4} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k3}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k3}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k3}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k4}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k4}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k4}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{polynomial-2}.pdf}
\end{figure}

\sphinxAtStartPar
In the following example, the kernels are also to ease the readability of the effects.
The diagonal always becomes the unity when normalizing, hence the more pronounced difference with the non\sphinxhyphen{}normalized example above.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{k3} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{k4} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k3}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k3}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k3}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k4}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Alpha = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k4}\PYG{o}{.}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Beta = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k4}\PYG{o}{.}\PYG{n}{beta}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{polynomial-3}.pdf}
\end{figure}


\subparagraph{Explicit and Implicit}
\label{\detokenize{kernel/generic/polynomial:explicit-and-implicit}}
\sphinxAtStartPar
The polynomial kernel can have its kernel matrix computed through the explicit feature map as \(k(x,y) = \phi(x)^\top\phi(y)\)
and implicitly using \(k(x,y) = \left(x^\top y + \beta\right)^\alpha\). The following confirms that both are equivalent.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,}\PYG{l+m+mi}{3}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Polynomial}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{explicit}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Explicit (sample)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{explicit}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Implicit (sample)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{explicit}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Implicit (out\PYGZhy{}of\PYGZhy{}sample)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{explicit}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Explicit (out\PYGZhy{}of\PYGZhy{}sample)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{polynomial-4}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/polynomial:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-b6c616342c40f12841bf0867a5e9a0d7f1d0d779.pdf}

\sphinxstepscope


\subsubsection{Cosine Kernel}
\label{\detokenize{kernel/generic/cosine:cosine-kernel}}\label{\detokenize{kernel/generic/cosine::doc}}

\paragraph{Class}
\label{\detokenize{kernel/generic/cosine:class}}\index{Cosine (class in kerch.kernel)@\spxentry{Cosine}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Cosine}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/generic/linear:kerch.kernel.Linear}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.generic.linear.Linear}}}}}

\sphinxAtStartPar
Cosine kernel.
\begin{equation*}
\begin{split}k(x,y) = \cos\left(\angle(x,y)\right) = \frac{x^{\top} y}{\max\left(\lVert x \rVert_2 \cdot \lVert y \rVert_2, \epsilon\right)}.\end{split}
\end{equation*}
\sphinxAtStartPar
This corresponds to a normalized linear kernel, or equivalently a linear kernel on which the datapoints are first
projected onto a hypersphere.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Cosine property)@\spxentry{Corr}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Cosine property)@\spxentry{Cov}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Cosine property)@\spxentry{K}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Cosine property)@\spxentry{Phi}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Cosine method)@\spxentry{c()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Cosine method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Cosine property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Cosine property)@\spxentry{centered}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Cosine method)@\spxentry{corr()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Cosine method)@\spxentry{cov()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Cosine property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Cosine property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Cosine property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Cosine property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Cosine property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Cosine property)@\spxentry{explicit}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Cosine method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.Cosine method)@\spxentry{forward()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Cosine property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Cosine property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Cosine property)@\spxentry{idx}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Cosine method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Cosine method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Cosine method)@\spxentry{k()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Cosine property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Cosine property)@\spxentry{normalized}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Cosine property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Cosine property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Cosine method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Cosine method)@\spxentry{reset()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Cosine property)@\spxentry{sample}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Cosine property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Cosine property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Cosine property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Cosine method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Cosine method)@\spxentry{train()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Cosine method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Cosine method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Cosine method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Cosine method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/cosine:kerch.kernel.Cosine.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/generic/cosine:examples}}

\subparagraph{Connection with Linear}
\label{\detokenize{kernel/generic/cosine:connection-with-linear}}
\sphinxAtStartPar
Essentially, a cosine kernel is the same as a linear kernel with normalization (as first transformation if multiple
are applied).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{n}{k\PYGZus{}cos} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Cosine}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{k\PYGZus{}lin} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}cos}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Cosine}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}lin}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Normalized Linear}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{cosine-1}.pdf}
\end{figure}


\subparagraph{Multiple Dimensions}
\label{\detokenize{kernel/generic/cosine:multiple-dimensions}}
\sphinxAtStartPar
The checkboard pattern appearing is a consequence of the normalization of one\sphinxhyphen{}dimensional input.
This does not happen in higher dimensions.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{sin} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{log} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{x1} \PYG{o}{=} \PYG{n}{sin}
\PYG{n}{x2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{sin}\PYG{p}{,}\PYG{n}{log}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Cosine}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x1}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Cosine}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x2}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{One Dimension}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Two Dimensions}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{cosine-2}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/cosine:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Cosine}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cosine}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/cosine:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-2166a8b247f8d452c56ea2c63ba3b44c7ffe4b43.pdf}

\sphinxstepscope


\subsubsection{Sigmoid Kernel}
\label{\detokenize{kernel/generic/sigmoid:sigmoid-kernel}}\label{\detokenize{kernel/generic/sigmoid::doc}}

\paragraph{Class}
\label{\detokenize{kernel/generic/sigmoid:class}}\index{Sigmoid (class in kerch.kernel)@\spxentry{Sigmoid}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Sigmoid}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}

\sphinxAtStartPar
Sigmoid kernel.
\begin{equation*}
\begin{split}k(x,y) = \sigma\left( a (x^\top y) + b \right),\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\sigma(\cdot)\) is the sigmoid function.

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
This kernel is not positive semi\sphinxhyphen{}definite. Normalization after centering is not possible.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{a}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Value for \(a\)., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{b}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Value for \(b\)., defaults to 0

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{params}} \textendash{} \sphinxtitleref{True} if the gradient of \(a\) and \(b\) are to be computed. If so, a graph is computed
and the parameters can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Sigmoid property)@\spxentry{Corr}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Sigmoid property)@\spxentry{Cov}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Sigmoid property)@\spxentry{K}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Sigmoid property)@\spxentry{Phi}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Sigmoid method)@\spxentry{c()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Sigmoid method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Sigmoid property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Sigmoid property)@\spxentry{centered}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Sigmoid method)@\spxentry{corr()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Sigmoid method)@\spxentry{cov()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Sigmoid property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Sigmoid property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Sigmoid property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Sigmoid property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Sigmoid property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Sigmoid property)@\spxentry{explicit}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Sigmoid method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Sigmoid method)@\spxentry{forward()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Sigmoid property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Sigmoid property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Sigmoid property)@\spxentry{idx}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Sigmoid method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Sigmoid method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Sigmoid method)@\spxentry{k()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Sigmoid property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Sigmoid property)@\spxentry{normalized}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Sigmoid property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Sigmoid property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{params\_trainable (kerch.kernel.Sigmoid property)@\spxentry{params\_trainable}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.params_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{params\_trainable}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean returning if the parameters a trainable or not.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Sigmoid method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Sigmoid method)@\spxentry{reset()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Sigmoid property)@\spxentry{sample}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Sigmoid property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Sigmoid property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Sigmoid property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Sigmoid method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Sigmoid method)@\spxentry{train()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Sigmoid method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Sigmoid method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Sigmoid method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Sigmoid method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/sigmoid:kerch.kernel.Sigmoid.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/generic/sigmoid:example}}

\subparagraph{Sine}
\label{\detokenize{kernel/generic/sigmoid:sine}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1.5}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Sigmoid}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{sigmoid-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{sigmoid-1_01}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/sigmoid:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Sigmoid}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigmoid}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/sigmoid:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-375f152d1d47c53a5956580d33f5d5a99f4786f3.pdf}

\sphinxstepscope


\subsubsection{Random Fourier Features Kernel}
\label{\detokenize{kernel/generic/rff:random-fourier-features-kernel}}\label{\detokenize{kernel/generic/rff::doc}}

\paragraph{Class}
\label{\detokenize{kernel/generic/rff:class}}\index{RFF (class in kerch.kernel)@\spxentry{RFF}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{RFF}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.random\_features.random\_features.RandomFeatures}}

\sphinxAtStartPar
Random Fourier Features kernel of \(\texttt{num_weights}\) weights and (optional) bandwidth \(\sigma\).
This can be seen as an explicit feature map approximation of \sphinxcode{\sphinxupquote{RBF}}.
\begin{equation*}
\begin{split}\phi(x) = \frac{1}{\sqrt{\texttt{num_weights}}} \left(\begin{array}{cc}
    \cos(w_1^{\top}x / \sigma) \\
    \vdots \\
    \cos(w_{\texttt{num_weights}}^{\top}x / \sigma) \\
    \sin(w_1^{\top}x / \sigma) \\
    \vdots \\
    \sin(w_{\texttt{num_weights}}^{\top}x / \sigma) \\
\end{array}\right)^\top\end{split}
\end{equation*}
\sphinxAtStartPar
with \(w_1, \ldots, w_{\texttt{num_weights}} \sim \mathcal{N}(0,I_{\texttt{dim_input}})\) and \(\texttt{dim_feature} = 2 \times \texttt{num_weights}\).

\sphinxAtStartPar
In the limit of \(\texttt{num_weights} \rightarrow +\infty\), we recover the RBF kernel:
\begin{equation*}
\begin{split}k(x,y) = \phi(x)^{\top}\phi(y) = \exp\left( -\frac{\lVert x-y \rVert_2^2}{2\sigma^2} \right)\end{split}
\end{equation*}\begin{equation*}
\begin{split}\phi(x) = \frac{1}{\sqrt{d}} \left(\begin{array}{c}
    \sigma(w_1^{\top}x / \sigma) \\
    \sigma(w_2^{\top}x / \sigma) \\
    \vdots \\
    \sigma(w_d^{\top}x / \sigma) \\
\end{array}\right)\end{split}
\end{equation*}
\sphinxAtStartPar
with \(w_1, \ldots, w_d \sim \mathcal{N}(0,I_{\texttt{dim_input}})\) and \(\texttt{dim_feature} = d\).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Provided \(d \geq \texttt{dim_input}\), the map guarantees \(x = \phi^\dag \circ \phi \circ x\).
The opposite \(d \leq \texttt{dim_input}\) guarantees \(x = \phi \circ \phi^\dag \circ x\). The
bijection is guaranteed if \(d = \texttt{dim_input}\).
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_weights}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of weights \(d\) sampled for the Random Features kernel., defaults to 1.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{weights}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_weights}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \_Explicit values for the weights may be provided instead of automatically sampling them with the
provided \sphinxtitleref{num\_weights}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{weights\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Specifies if the weights are to be considered as trainable parameters during
backpropagation., default to \sphinxtitleref{False}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. Defaults to 1.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.RFF property)@\spxentry{C}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.RFF property)@\spxentry{Corr}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.RFF property)@\spxentry{Cov}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.RFF property)@\spxentry{K}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/rff:kerch.kernel.RFF.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.RFF property)@\spxentry{Phi}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/generic/rff:kerch.kernel.RFF.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{activation\_fn() (kerch.kernel.RFF method)@\spxentry{activation\_fn()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.activation_fn}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{activation\_fn}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{activation\_fn\_inv() (kerch.kernel.RFF method)@\spxentry{activation\_fn\_inv()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.activation_fn_inv}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{activation\_fn\_inv}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{after\_step() (kerch.kernel.RFF method)@\spxentry{after\_step()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Specific operations to be performed after a training step. We refer to the documentation of
\DUrole{xref,std,std-doc}{../features/module} for further information.

\end{fulllineitems}

\index{c() (kerch.kernel.RFF method)@\spxentry{c()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.RFF method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.RFF property)@\spxentry{cache\_level}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.RFF property)@\spxentry{centered}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{closed\_form\_kernel() (kerch.kernel.RFF method)@\spxentry{closed\_form\_kernel()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.closed_form_kernel}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{closed\_form\_kernel}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{y}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{corr() (kerch.kernel.RFF method)@\spxentry{corr()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.RFF method)@\spxentry{cov()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.RFF property)@\spxentry{current\_sample}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.RFF property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.RFF property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map \(\texttt{dim_feature} = 2 \times \texttt{num_weights}\).

\end{fulllineitems}

\index{dim\_input (kerch.kernel.RFF property)@\spxentry{dim\_input}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.RFF property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.RFF property)@\spxentry{explicit}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.RFF method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.RFF method)@\spxentry{forward()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.RFF property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.RFF property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.RFF property)@\spxentry{idx}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.RFF method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.RFF method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.RFF method)@\spxentry{k()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.RFF property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.RFF property)@\spxentry{normalized}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.RFF property)@\spxentry{num\_idx}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.RFF property)@\spxentry{num\_sample}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{num\_weights (kerch.kernel.RFF property)@\spxentry{num\_weights}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.num_weights}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_weights}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
Weight dimension \(d\).

\end{fulllineitems}

\index{phi() (kerch.kernel.RFF method)@\spxentry{phi()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.RFF method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.RFF method)@\spxentry{reset()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.RFF property)@\spxentry{sample}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.RFF property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.RFF property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.RFF property)@\spxentry{sigma}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.RFF property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.RFF method)@\spxentry{stochastic()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.RFF method)@\spxentry{train()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.RFF method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.RFF method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.RFF method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.RFF method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{weights (kerch.kernel.RFF property)@\spxentry{weights}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.weights}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{weights}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }Optional\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Tensor parameter containing the \(w_1, \ldots, w_d\). The first dimension is \(d\) and the second
\sphinxtitleref{dim\_input}.

\end{fulllineitems}

\index{weights\_trainable (kerch.kernel.RFF property)@\spxentry{weights\_trainable}\spxextra{kerch.kernel.RFF property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/rff:kerch.kernel.RFF.weights_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{weights\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the weights is considered for training during backpropagation, False if considered as a constant.

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/generic/rff:examples}}

\subparagraph{Sine}
\label{\detokenize{kernel/generic/rff:sine}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)}

\PYG{n}{k\PYGZus{}rbf} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{k\PYGZus{}rff} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RFF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{num\PYGZus{}weights}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}rbf}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RBF}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}rff}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RFF}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{rff-1}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/rff:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RFF}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rff}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/rff:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-286c8f8fa1e701d35e292cf4694a4bdd81881c21.pdf}

\sphinxstepscope


\subsubsection{Nyström Kernel}
\label{\detokenize{kernel/generic/nystrom:nystrom-kernel}}\label{\detokenize{kernel/generic/nystrom::doc}}
\sphinxAtStartPar
This kernel allows to approximate explicit feature maps for kernels whose explicit representation lives in an
infinite\sphinxhyphen{}dimensional Hilbert space. Through eigendecomposition, we are able to find
a base in the RKHS spanned by the provided sample. By looking at the RKHS coefficients for each datapoint, we are able to
recover an approximate explicit feature map.
\index{Nystrom (class in kerch.kernel)@\spxentry{Nystrom}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Nystrom}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.explicit.Explicit}}}}}

\sphinxAtStartPar
Nyström kernel. Constructs an explicit feature map based on the eigendecomposition of any kernel matrix based on
some sample.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of the explicit feature map to be constructed. This value cannot exceed the number of sample
points. During eigendecomposition, very small eigenvalues are also going to be pruned to avoid numerical
instability. If \sphinxtitleref{None}, the value will be assigned to \sphinxtitleref{num\_sample}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{base\_kernel\_type}} \textendash{} The name of kernel on which the explicit feature map is going to be constructed. Default to
kerch.DEFAULT\_KERNEL\_TYPE

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{base\_kernel\_transform}} \textendash{} Same as kernel\_transform but for the base kernel, when using the factory through
\sphinxtitleref{base\_type}. Defaults to {[}{]}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Other arguments for the \_Projected kernel (e.g. the bandwidth for an RBF kernel, the degree for a
polynomial kernel etc.). For the default values, please refer to the requested class in question.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{base\_kernel}} (\sphinxstyleliteralemphasis{\sphinxupquote{kerch.kernel.*}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of creating a new kernel on which to use the Nyström method, one can also perform it
on an existing kernel. In that case, the other \_Projected arguments are bypassed., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.Nystrom property)@\spxentry{C}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.Nystrom property)@\spxentry{Corr}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Nystrom property)@\spxentry{Cov}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Nystrom property)@\spxentry{K}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Nystrom property)@\spxentry{Phi}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{base\_kernel (kerch.kernel.Nystrom property)@\spxentry{base\_kernel}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.base_kernel}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{base\_kernel}}}
\pysigstopsignatures
\sphinxAtStartPar
Kernel on which Nystrom performs the decomposition.

\end{fulllineitems}

\index{c() (kerch.kernel.Nystrom method)@\spxentry{c()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Nystrom method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Nystrom property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Nystrom property)@\spxentry{centered}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Nystrom method)@\spxentry{corr()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Nystrom method)@\spxentry{cov()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Nystrom property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Nystrom property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim (kerch.kernel.Nystrom property)@\spxentry{dim}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.dim}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Nystrom property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Nystrom property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Nystrom property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Nystrom property)@\spxentry{explicit}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Nystrom method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.Nystrom method)@\spxentry{forward()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed() (kerch.kernel.Nystrom method)@\spxentry{hparams\_fixed()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.hparams_fixed}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Nystrom property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Nystrom property)@\spxentry{idx}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Nystrom method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Nystrom method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Nystrom method)@\spxentry{k()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Nystrom property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Nystrom property)@\spxentry{normalized}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Nystrom property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Nystrom property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.Nystrom method)@\spxentry{phi()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Nystrom method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Nystrom method)@\spxentry{reset()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Nystrom property)@\spxentry{sample}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Nystrom property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Nystrom property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Nystrom property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Nystrom method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Nystrom method)@\spxentry{train()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Nystrom method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Nystrom method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Nystrom method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Nystrom method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/generic/nystrom:kerch.kernel.Nystrom.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/generic/nystrom:example}}

\subparagraph{Explicit RBF}
\label{\detokenize{kernel/generic/nystrom:explicit-rbf}}
\sphinxAtStartPar
We first consider an RBF kernel and will approximate its explicit feature map both for the sample input and
an out\sphinxhyphen{}of\sphinxhyphen{}sample input.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}

\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Nystrom}\PYG{p}{(}\PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix}
\PYG{n}{fig1}\PYG{p}{,} \PYG{n}{axs1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig1}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrices of the Base Kernel (RBF)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im1} \PYG{o}{=} \PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs1}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig1}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs1}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} explicit feature map}
\PYG{n}{fig2}\PYG{p}{,} \PYG{n}{axs2} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig2}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Explicit Feature Maps (Nystrom)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{Phi}\PYG{p}{)}
\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im2} \PYG{o}{=} \PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs2}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig2}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs2}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix from the explicit feature map}
\PYG{n}{fig3}\PYG{p}{,} \PYG{n}{axs3} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig3}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrices from the Explicit Feature Map (Nystrom)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im3} \PYG{o}{=} \PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs3}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig3}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im3}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs3}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{nystrom-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{nystrom-1_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{nystrom-1_02}.pdf}
\end{figure}


\subparagraph{Effect of the dimension}
\label{\detokenize{kernel/generic/nystrom:effect-of-the-dimension}}
\sphinxAtStartPar
On the example hereabove, al lot of features seem to contain few information. One may decide that keeping all the bases of the RKHS is unnecessary, and only keeping the ones corresponding to the
most variance on the sample dataset. In the following example, only 6 dimensions are kept (few to accentuate the effect for demonstration purposes). This of course reduces the
quality of the approximation.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.1}

\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Nystrom}\PYG{p}{(}\PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix}
\PYG{n}{fig1}\PYG{p}{,} \PYG{n}{axs1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig1}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrices of the Base Kernel (RBF)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im1} \PYG{o}{=} \PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k\PYGZus{}base}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs1}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs1}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig1}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs1}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} explicit feature map}
\PYG{n}{fig2}\PYG{p}{,} \PYG{n}{axs2} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig2}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Explicit Feature Maps (Nystrom)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{Phi}\PYG{p}{)}
\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im2} \PYG{o}{=} \PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs2}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs2}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig2}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs2}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix from the explicit feature map}
\PYG{n}{fig3}\PYG{p}{,} \PYG{n}{axs3} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig3}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrices from the Explicit Feature Map (Nystrom)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im3} \PYG{o}{=} \PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs3}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs3}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig3}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im3}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs3}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{nystrom-2_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{nystrom-2_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{nystrom-2_02}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/generic/nystrom:factory}}
\sphinxAtStartPar
The following lines are equivalent. First, an example with the default arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Nystrom}\PYG{p}{(}\PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nystrom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Nystrom}\PYG{p}{(}\PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nystrom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{base\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
And now with some arguments:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Nystrom}\PYG{p}{(}\PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nystrom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k\PYGZus{}base} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Nystrom}\PYG{p}{(}\PYG{n}{base\PYGZus{}kernel}\PYG{o}{=}\PYG{n}{k\PYGZus{}base}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nystrom}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{base\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/generic/nystrom:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-fafd86910641219b807ff04636dc8e087d342b7d.pdf}


\subsection{Network\sphinxhyphen{}Based Kernels}
\label{\detokenize{kernel/index:network-based-kernels}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Explicit Feature Map
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Kernel
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameters
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Factory Type
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.ExplicitNN}}}}}
&
\sphinxAtStartPar
\(\phi(x) = NN\left(x\right)\)
&
\sphinxAtStartPar
\(k(x,y)=\phi(x)^\top\phi(y)\)
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’explicit\_nn’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.ImplicitNN}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y) = NN\left( [x, y] \right).\)
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’implicit\_nn’}}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\subsubsection{Explicit Network\sphinxhyphen{}based Kernel}
\label{\detokenize{kernel/network/explicit_nn:explicit-network-based-kernel}}\label{\detokenize{kernel/network/explicit_nn::doc}}\index{ExplicitNN (class in kerch.kernel)@\spxentry{ExplicitNN}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{ExplicitNN}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.explicit.Explicit}}}}}

\sphinxAtStartPar
Explicit feature map kernel, given by a neural network.
\begin{equation*}
\begin{split}k(x,y) = NN\left(x\right)^\top NN\left(y\right).\end{split}
\end{equation*}
\sphinxAtStartPar
In other words, we have
\begin{equation*}
\begin{split}\phi(x) = NN\left(x\right)\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{encoder}} (\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}}) \textendash{} Explicit feature map encoder network.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{decoder}} (\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit decoder network

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{networks\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxcode{\sphinxupquote{True}} if the encoder and decoders are trainable. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recon\_loss\_fun}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.modules.loss.\_Loss}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instance of the reconstruction loss function for the encoder/decoder pair.
Defaults to torch.nn.MSELoss(reduction=’mean’).

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.ExplicitNN property)@\spxentry{C}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.ExplicitNN property)@\spxentry{Corr}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.ExplicitNN property)@\spxentry{Cov}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.ExplicitNN property)@\spxentry{K}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.ExplicitNN property)@\spxentry{Phi}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.ExplicitNN method)@\spxentry{c()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.ExplicitNN method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.ExplicitNN property)@\spxentry{cache\_level}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.ExplicitNN property)@\spxentry{centered}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.ExplicitNN method)@\spxentry{corr()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.ExplicitNN method)@\spxentry{cov()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.ExplicitNN property)@\spxentry{current\_sample}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.ExplicitNN property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{decode() (kerch.kernel.ExplicitNN method)@\spxentry{decode()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.decode}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{decode}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{decoder (kerch.kernel.ExplicitNN property)@\spxentry{decoder}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.decoder}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{decoder}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{torch.nn.modules.module.Module}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{dim\_feature (kerch.kernel.ExplicitNN property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.ExplicitNN property)@\spxentry{dim\_input}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.ExplicitNN property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{encoder (kerch.kernel.ExplicitNN property)@\spxentry{encoder}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.encoder}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{encoder}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{torch.nn.modules.module.Module}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{explicit (kerch.kernel.ExplicitNN property)@\spxentry{explicit}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.ExplicitNN method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.ExplicitNN method)@\spxentry{forward()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed() (kerch.kernel.ExplicitNN method)@\spxentry{hparams\_fixed()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.hparams_fixed}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}{}{}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.ExplicitNN property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.ExplicitNN property)@\spxentry{idx}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.ExplicitNN method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.ExplicitNN method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.ExplicitNN method)@\spxentry{k()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.ExplicitNN property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{loss() (kerch.kernel.ExplicitNN method)@\spxentry{loss()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.loss}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}
\pysigstopsignatures
\end{fulllineitems}

\index{normalized (kerch.kernel.ExplicitNN property)@\spxentry{normalized}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.ExplicitNN property)@\spxentry{num\_idx}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.ExplicitNN property)@\spxentry{num\_sample}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.ExplicitNN method)@\spxentry{phi()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.ExplicitNN method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.ExplicitNN method)@\spxentry{reset()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.ExplicitNN property)@\spxentry{sample}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.ExplicitNN property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.ExplicitNN property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.ExplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.ExplicitNN method)@\spxentry{stochastic()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.ExplicitNN method)@\spxentry{train()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.ExplicitNN method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.ExplicitNN method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.ExplicitNN method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.ExplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/explicit_nn:kerch.kernel.ExplicitNN.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\subsubsection{Implicit Network\sphinxhyphen{}based Kernel}
\label{\detokenize{kernel/network/implicit_nn:implicit-network-based-kernel}}\label{\detokenize{kernel/network/implicit_nn::doc}}\index{ImplicitNN (class in kerch.kernel)@\spxentry{ImplicitNN}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{ImplicitNN}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}

\sphinxAtStartPar
Implicit kernel class, parametrized by a neural network.
\begin{equation*}
\begin{split}k(x,y) = NN\left( [x, y] \right).\end{split}
\end{equation*}
\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
This kernel is not positive semi\sphinxhyphen{}definite in the general case. This is only possible if a specific choice of
neural network is provided.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{network}} (\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn.Module}}}) \textendash{} Network to be used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.ImplicitNN property)@\spxentry{Corr}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.ImplicitNN property)@\spxentry{Cov}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.ImplicitNN property)@\spxentry{K}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.ImplicitNN property)@\spxentry{Phi}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.ImplicitNN method)@\spxentry{c()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.ImplicitNN method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.ImplicitNN property)@\spxentry{cache\_level}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.ImplicitNN property)@\spxentry{centered}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.ImplicitNN method)@\spxentry{corr()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.ImplicitNN method)@\spxentry{cov()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.ImplicitNN property)@\spxentry{current\_sample}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.ImplicitNN property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.ImplicitNN property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.ImplicitNN property)@\spxentry{dim\_input}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.ImplicitNN property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.ImplicitNN property)@\spxentry{explicit}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.ImplicitNN method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.ImplicitNN method)@\spxentry{forward()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.ImplicitNN property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.ImplicitNN property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.ImplicitNN property)@\spxentry{idx}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.ImplicitNN method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.ImplicitNN method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.ImplicitNN method)@\spxentry{k()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.ImplicitNN property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.ImplicitNN property)@\spxentry{normalized}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.ImplicitNN property)@\spxentry{num\_idx}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.ImplicitNN property)@\spxentry{num\_sample}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.ImplicitNN method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.ImplicitNN method)@\spxentry{reset()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.ImplicitNN property)@\spxentry{sample}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.ImplicitNN property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.ImplicitNN property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.ImplicitNN property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.ImplicitNN method)@\spxentry{stochastic()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.ImplicitNN method)@\spxentry{train()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.ImplicitNN method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.ImplicitNN method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.ImplicitNN method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.ImplicitNN method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/network/implicit_nn:kerch.kernel.ImplicitNN.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Time Kernels}
\label{\detokenize{kernel/index:time-kernels}}
\sphinxAtStartPar
The idea behind time kernels is that time has the same local effect at
all time, or in other words that the kernels are translational invariant. We typically consider the following kernels:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Explicit Feature Map
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Kernel
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameters
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Factory Type
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/time/indicator:kerch.kernel.Indicator}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Indicator}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
See documentation
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{lag}}, \sphinxcode{\sphinxupquote{gamma}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’indicator’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RBF}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)=\exp\left(-\frac{\left\lVert x-y\right\rVert^2_2}{2\sigma^2}\right)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’rbf’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/time/hat:kerch.kernel.Hat}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Hat}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
See documentation
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{lag}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’hat’}}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\subsubsection{Indicator Kernel}
\label{\detokenize{kernel/time/indicator:indicator-kernel}}\label{\detokenize{kernel/time/indicator::doc}}

\paragraph{Class}
\label{\detokenize{kernel/time/indicator:class}}\index{Indicator (class in kerch.kernel)@\spxentry{Indicator}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Indicator}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}

\sphinxAtStartPar
Indicator kernel.
\begin{equation*}
\begin{split}k(x,y) = \left\{
\begin{array}
g\gamma & \text{ if } |x-y|=0, \\
1 & \text{ if } 0 < |x-y| \leq p, \\
0 & \text{ otherwise.}
\end{array}
\right.\end{split}
\end{equation*}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
If the default value for \(\gamma\) is used and the \(p\) is to be trained, their two values will be
linked.
\end{sphinxadmonition}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Depending on the choice of \(\gamma\), the kernel may not be positive semi\sphinxhyphen{}definite. The default value
however ensures it, as long as the inputs are integers. If they are not, this may get more complicated.
\end{sphinxadmonition}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this name of kernel, the input dimension of the datapoints \sphinxtitleref{dim\_input} must be 1.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Lag parameter \(p\)., defaults to 1.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gamma}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Identity value \(\gamma\) of the kernel. If \sphinxtitleref{None}, the value will be \(\gamma = 2p+1\) to
ensure positive semi\sphinxhyphen{}definiteness., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lag\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the lag \(p\) is to be computed. If so, a graph is computed
and the lag can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gamma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the \(\gamma\) is to be computed. If so, a graph is computed
and the \(\gamma\) can be updated. \sphinxtitleref{False} just leads to a static computation., this value will be tied to the
evolution of the lag \(p\)., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Indicator property)@\spxentry{Corr}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Indicator property)@\spxentry{Cov}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Indicator property)@\spxentry{K}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/time/indicator:kerch.kernel.Indicator.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Indicator property)@\spxentry{Phi}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Indicator method)@\spxentry{c()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Indicator method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Indicator property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Indicator property)@\spxentry{centered}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Indicator method)@\spxentry{corr()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Indicator method)@\spxentry{cov()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Indicator property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Indicator property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Indicator property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Indicator property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Indicator property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Indicator property)@\spxentry{explicit}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Indicator method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Indicator method)@\spxentry{forward()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{gamma (kerch.kernel.Indicator property)@\spxentry{gamma}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.gamma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{gamma}}}
\pysigstopsignatures
\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Indicator property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Indicator property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Indicator property)@\spxentry{idx}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Indicator method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Indicator method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Indicator method)@\spxentry{k()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Indicator property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{lag (kerch.kernel.Indicator property)@\spxentry{lag}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.lag}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{lag}}}
\pysigstopsignatures
\sphinxAtStartPar
Lah \(p\) of the kernel.

\end{fulllineitems}

\index{lag\_trainable (kerch.kernel.Indicator property)@\spxentry{lag\_trainable}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.lag_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{lag\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating if the lag \(p\) is trainable.

\end{fulllineitems}

\index{normalized (kerch.kernel.Indicator property)@\spxentry{normalized}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Indicator property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Indicator property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Indicator method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Indicator method)@\spxentry{reset()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Indicator property)@\spxentry{sample}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Indicator property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Indicator property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Indicator property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Indicator method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Indicator method)@\spxentry{train()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Indicator method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Indicator method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Indicator method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Indicator method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/indicator:kerch.kernel.Indicator.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/time/indicator:examples}}

\subparagraph{Linear (Time)}
\label{\detokenize{kernel/time/indicator:linear-time}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Indicator}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lag}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Indicator with lag }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{lag}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{indicator-1}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/time/indicator:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Indicator}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{indicator}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/time/indicator:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-e728d7f74eef0226bdb468d98334a6d44d763f72.pdf}

\sphinxstepscope


\subsubsection{Hat Kernel}
\label{\detokenize{kernel/time/hat:hat-kernel}}\label{\detokenize{kernel/time/hat::doc}}

\paragraph{Class}
\label{\detokenize{kernel/time/hat:class}}\index{Hat (class in kerch.kernel)@\spxentry{Hat}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Hat}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}

\sphinxAtStartPar
Hat kernel.
\begin{equation*}
\begin{split}k(x,y) = \left\{
\begin{array}
lp + 1 - |x-y| & \text{ if } |x-y|\leq p, \\
0 & \text{ otherwise.}
\end{array}
\right.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lag}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Lag parameter., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lag\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the lag is to be computed. If so, a graph is computed
and the lag can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Hat property)@\spxentry{Corr}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Hat property)@\spxentry{Cov}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Hat property)@\spxentry{K}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/time/hat:kerch.kernel.Hat.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Hat property)@\spxentry{Phi}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Hat method)@\spxentry{c()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Hat method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Hat property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Hat property)@\spxentry{centered}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Hat method)@\spxentry{corr()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Hat method)@\spxentry{cov()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Hat property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Hat property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Hat property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Hat property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Hat property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Hat property)@\spxentry{explicit}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Hat method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Hat method)@\spxentry{forward()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Hat property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Hat property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Hat property)@\spxentry{idx}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Hat method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Hat method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Hat method)@\spxentry{k()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Hat property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{lag (kerch.kernel.Hat property)@\spxentry{lag}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.lag}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{lag}}}
\pysigstopsignatures
\sphinxAtStartPar
Lah \(p\) of the kernel.

\end{fulllineitems}

\index{lag\_trainable (kerch.kernel.Hat property)@\spxentry{lag\_trainable}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.lag_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{lag\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating if the lag \(p\) is trainable.

\end{fulllineitems}

\index{normalized (kerch.kernel.Hat property)@\spxentry{normalized}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Hat property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Hat property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Hat method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Hat method)@\spxentry{reset()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Hat property)@\spxentry{sample}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Hat property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Hat property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Hat property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Hat method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Hat method)@\spxentry{train()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Hat method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Hat method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Hat method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Hat method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/time/hat:kerch.kernel.Hat.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Examples}
\label{\detokenize{kernel/time/hat:examples}}

\subparagraph{Linear (Time)}
\label{\detokenize{kernel/time/hat:linear-time}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Hat}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lag}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Hat with lag }\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{lag}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{hat-1}.pdf}
\end{figure}


\subparagraph{Factory}
\label{\detokenize{kernel/time/hat:factory}}
\sphinxAtStartPar
The following lines are equivalent:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Hat}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{factory}\PYG{p}{(}\PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hat}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/time/hat:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-eadace7c5190b7e80164d2dbdcd9ebb5685babb7.pdf}


\subsection{Statistical Kernels}
\label{\detokenize{kernel/index:statistical-kernels}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Explicit Feature Map
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Kernel
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameters
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Factory Type
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Uniform}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)=1 \text{ for } \frac{d(x,y)}{\sigma} \leq 1\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’uniform’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Triangular}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)=1 - \frac{d(x,y)}{\sigma} \text{ for } \frac{d(x,y)}{\sigma} \leq 1\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’triangular’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Epanechnikov}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)=1 - \left(\frac{d(x,y)}{\sigma}\right)^2 \text{ for } \frac{d(x,y)}{\sigma} \leq 1\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’epanechnikov’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Quartic}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)= \left(1 - \left(\frac{d(x,y)}{\sigma}\right)^2\right)^2 \text{ for } \frac{d(x,y)}{\sigma} \leq 1\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’quartic’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Triweight}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)= \left(1 - \left(\frac{d(x,y)}{\sigma}\right)^2\right)^3 \text{ for } \frac{d(x,y)}{\sigma} \leq 1\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’triweight’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Tricube}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)= \left(1 - \left|\frac{d(x,y)}{\sigma}\right|^3\right)^3 \text{ for } \frac{d(x,y)}{\sigma} \leq 1\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’tricube’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Exponential}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)= \exp\left(-\frac{d(x,y)}{\sigma}\right)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}, \sphinxcode{\sphinxupquote{squared}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’exponential’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Logistic}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)= \frac{4}{\exp\left(d(x,y)/\sigma\right) + 2 + \exp\left(-d(x,y)/\sigma\right)}\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’logistic’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Silverman}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y)= \exp\left(-\frac{d(x,y)}{\sqrt{2}\sigma}\right)\sin\left(\frac{d(x,y)}{\sqrt{2}\sigma} + \frac{\pi}{4}\right)\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}, \sphinxcode{\sphinxupquote{distance}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’silverman’}}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\subsubsection{Uniform (window) Kernel}
\label{\detokenize{kernel/statistics/uniform:uniform-window-kernel}}\label{\detokenize{kernel/statistics/uniform::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/uniform:class}}\index{Uniform (class in kerch.kernel)@\spxentry{Uniform}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Uniform}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Uniform (window) kernel.
\begin{equation*}
\begin{split}k(x,y) = %
\begin{cases}
1 & \text{for } \frac{d(x,y)}{\sigma} \leq 1, \\
0 & \text{otherwise}.
\end{cases}\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Uniform property)@\spxentry{Corr}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Uniform property)@\spxentry{Cov}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Uniform property)@\spxentry{K}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Uniform property)@\spxentry{Phi}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Uniform method)@\spxentry{c()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Uniform method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Uniform property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Uniform property)@\spxentry{centered}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Uniform method)@\spxentry{corr()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Uniform method)@\spxentry{cov()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Uniform property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Uniform property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Uniform property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Uniform property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Uniform property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Uniform property)@\spxentry{explicit}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Uniform method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Uniform method)@\spxentry{forward()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Uniform property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Uniform property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Uniform property)@\spxentry{idx}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Uniform method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Uniform method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Uniform method)@\spxentry{k()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Uniform property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Uniform property)@\spxentry{normalized}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Uniform property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Uniform property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Uniform method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Uniform method)@\spxentry{reset()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Uniform property)@\spxentry{sample}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Uniform property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Uniform property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Uniform property)@\spxentry{sigma}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Uniform property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Uniform property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Uniform method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Uniform method)@\spxentry{train()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Uniform method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Uniform method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Uniform method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Uniform method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/uniform:kerch.kernel.Uniform.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/uniform:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/uniform:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Uniform}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Uniform Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{uniform-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/uniform:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Uniform}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Uniform}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Uniform (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Uniform (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{uniform-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/uniform:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-ebfd136f59f185019892b4530bb89e6b8eaa746a.pdf}

\sphinxstepscope


\subsubsection{Triangular Kernel}
\label{\detokenize{kernel/statistics/triangular:triangular-kernel}}\label{\detokenize{kernel/statistics/triangular::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/triangular:class}}\index{Triangular (class in kerch.kernel)@\spxentry{Triangular}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Triangular}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Uniform kernel.
\begin{equation*}
\begin{split}k(x,y) = %
\begin{cases}
1- \frac{d(x,y)}{\sigma} & \text{for } \frac{d(x,y)}{\sigma} \leq 1, \\
0 & \text{otherwise}.
\end{cases}\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Triangular property)@\spxentry{Corr}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Triangular property)@\spxentry{Cov}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Triangular property)@\spxentry{K}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Triangular property)@\spxentry{Phi}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Triangular method)@\spxentry{c()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Triangular method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Triangular property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Triangular property)@\spxentry{centered}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Triangular method)@\spxentry{corr()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Triangular method)@\spxentry{cov()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Triangular property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Triangular property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Triangular property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Triangular property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Triangular property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Triangular property)@\spxentry{explicit}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Triangular method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Triangular method)@\spxentry{forward()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Triangular property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Triangular property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Triangular property)@\spxentry{idx}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Triangular method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Triangular method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Triangular method)@\spxentry{k()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Triangular property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Triangular property)@\spxentry{normalized}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Triangular property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Triangular property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Triangular method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Triangular method)@\spxentry{reset()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Triangular property)@\spxentry{sample}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Triangular property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Triangular property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Triangular property)@\spxentry{sigma}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Triangular property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Triangular property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Triangular method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Triangular method)@\spxentry{train()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Triangular method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Triangular method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Triangular method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Triangular method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triangular:kerch.kernel.Triangular.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/triangular:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/triangular:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triangular}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Triangular Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{triangular-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/triangular:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triangular}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triangular}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Triangular (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Triangular (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{triangular-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/triangular:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-dd6fa43d56e4bdbee7a0d0baf34ef2a1e1e91908.pdf}

\sphinxstepscope


\subsubsection{Epanechnikov (parabolic) Kernel}
\label{\detokenize{kernel/statistics/epanechnikov:epanechnikov-parabolic-kernel}}\label{\detokenize{kernel/statistics/epanechnikov::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/epanechnikov:class}}\index{Epanechnikov (class in kerch.kernel)@\spxentry{Epanechnikov}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Epanechnikov}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Epanechnikov (parabolic) kernel.
\begin{equation*}
\begin{split}k(x,y) = %
\begin{cases}
1 - \left(\frac{d(x,y)}{\sigma}\right)^2 & \text{for } \frac{d(x,y)}{\sigma} \leq 1, \\
0 & \text{otherwise}.
\end{cases}\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Epanechnikov property)@\spxentry{Corr}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Epanechnikov property)@\spxentry{Cov}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Epanechnikov property)@\spxentry{K}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Epanechnikov property)@\spxentry{Phi}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Epanechnikov method)@\spxentry{c()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Epanechnikov method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Epanechnikov property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Epanechnikov property)@\spxentry{centered}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Epanechnikov method)@\spxentry{corr()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Epanechnikov method)@\spxentry{cov()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Epanechnikov property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Epanechnikov property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Epanechnikov property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Epanechnikov property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Epanechnikov property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Epanechnikov property)@\spxentry{explicit}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Epanechnikov method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Epanechnikov method)@\spxentry{forward()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Epanechnikov property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Epanechnikov property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Epanechnikov property)@\spxentry{idx}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Epanechnikov method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Epanechnikov method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Epanechnikov method)@\spxentry{k()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Epanechnikov property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Epanechnikov property)@\spxentry{normalized}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Epanechnikov property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Epanechnikov property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Epanechnikov method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Epanechnikov method)@\spxentry{reset()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Epanechnikov property)@\spxentry{sample}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Epanechnikov property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Epanechnikov property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Epanechnikov property)@\spxentry{sigma}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Epanechnikov property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Epanechnikov property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Epanechnikov method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Epanechnikov method)@\spxentry{train()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Epanechnikov method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Epanechnikov method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Epanechnikov method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Epanechnikov method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/epanechnikov:kerch.kernel.Epanechnikov.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/epanechnikov:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/epanechnikov:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Epanechnikov}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Epanechnikov Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{epanechnikov-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/epanechnikov:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Epanechnikov}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Epanechnikov}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Epanechnikov (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Epanechnikov (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{epanechnikov-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/epanechnikov:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-d2575c47a5890c7d23015dfebbe36560405118c5.pdf}

\sphinxstepscope


\subsubsection{Quartic (biweight) Kernel}
\label{\detokenize{kernel/statistics/quartic:quartic-biweight-kernel}}\label{\detokenize{kernel/statistics/quartic::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/quartic:class}}\index{Quartic (class in kerch.kernel)@\spxentry{Quartic}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Quartic}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Quartic (biweight) kernel.
\begin{equation*}
\begin{split}k(x,y) = %
\begin{cases}
\left(1 - \left(\frac{d(x,y)}{\sigma}\right)^2\right)^2 & \text{for } \frac{d(x,y)}{\sigma} \leq 1, \\
0 & \text{otherwise}.
\end{cases}\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Quartic property)@\spxentry{Corr}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Quartic property)@\spxentry{Cov}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Quartic property)@\spxentry{K}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Quartic property)@\spxentry{Phi}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Quartic method)@\spxentry{c()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Quartic method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Quartic property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Quartic property)@\spxentry{centered}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Quartic method)@\spxentry{corr()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Quartic method)@\spxentry{cov()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Quartic property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Quartic property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Quartic property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Quartic property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Quartic property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Quartic property)@\spxentry{explicit}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Quartic method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Quartic method)@\spxentry{forward()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Quartic property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Quartic property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Quartic property)@\spxentry{idx}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Quartic method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Quartic method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Quartic method)@\spxentry{k()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Quartic property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Quartic property)@\spxentry{normalized}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Quartic property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Quartic property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Quartic method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Quartic method)@\spxentry{reset()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Quartic property)@\spxentry{sample}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Quartic property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Quartic property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Quartic property)@\spxentry{sigma}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Quartic property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Quartic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Quartic method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Quartic method)@\spxentry{train()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Quartic method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Quartic method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Quartic method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Quartic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/quartic:kerch.kernel.Quartic.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/quartic:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/quartic:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Quartic}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quartic Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{quartic-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/quartic:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Quartic}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Quartic}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Quartic (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Quartic (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{quartic-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/quartic:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-5675492db098db3d60b421ab4338146600149f0a.pdf}

\sphinxstepscope


\subsubsection{Triweight Kernel}
\label{\detokenize{kernel/statistics/triweight:triweight-kernel}}\label{\detokenize{kernel/statistics/triweight::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/triweight:class}}\index{Triweight (class in kerch.kernel)@\spxentry{Triweight}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Triweight}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Triweight kernel.
\begin{equation*}
\begin{split}k(x,y) = %
\begin{cases}
\left(1 - \left(\frac{d(x,y)}{\sigma}\right)^2\right)^3 & \text{for } \frac{d(x,y)}{\sigma} \leq 1, \\
0 & \text{otherwise}.
\end{cases}\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Triweight property)@\spxentry{Corr}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Triweight property)@\spxentry{Cov}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Triweight property)@\spxentry{K}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Triweight property)@\spxentry{Phi}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Triweight method)@\spxentry{c()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Triweight method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Triweight property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Triweight property)@\spxentry{centered}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Triweight method)@\spxentry{corr()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Triweight method)@\spxentry{cov()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Triweight property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Triweight property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Triweight property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Triweight property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Triweight property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Triweight property)@\spxentry{explicit}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Triweight method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Triweight method)@\spxentry{forward()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Triweight property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Triweight property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Triweight property)@\spxentry{idx}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Triweight method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Triweight method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Triweight method)@\spxentry{k()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Triweight property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Triweight property)@\spxentry{normalized}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Triweight property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Triweight property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Triweight method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Triweight method)@\spxentry{reset()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Triweight property)@\spxentry{sample}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Triweight property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Triweight property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Triweight property)@\spxentry{sigma}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Triweight property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Triweight property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Triweight method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Triweight method)@\spxentry{train()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Triweight method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Triweight method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Triweight method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Triweight method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/triweight:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/triweight:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triweight}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Triweight Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{triweight-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/triweight:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triweight}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triweight}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Triweight (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Triweight (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{triweight-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/triweight:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-16cd0c8c4ca8fc19d874b22c543955a34df8569b.pdf}

\sphinxstepscope


\subsubsection{Tricube Kernel}
\label{\detokenize{kernel/statistics/tricube:tricube-kernel}}\label{\detokenize{kernel/statistics/tricube::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/tricube:class}}\index{Tricube (class in kerch.kernel)@\spxentry{Tricube}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Tricube}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Triweight kernel.
\begin{equation*}
\begin{split}k(x,y) = %
\begin{cases}
\left(1 - \left(|\frac{d(x,y)|}{\sigma}\right)^3\right)^3 & \text{for } \frac{d(x,y)}{\sigma} \leq 1, \\
0 & \text{otherwise}.
\end{cases}\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Tricube property)@\spxentry{Corr}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Tricube property)@\spxentry{Cov}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Tricube property)@\spxentry{K}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Tricube property)@\spxentry{Phi}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Tricube method)@\spxentry{c()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Tricube method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Tricube property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Tricube property)@\spxentry{centered}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Tricube method)@\spxentry{corr()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Tricube method)@\spxentry{cov()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Tricube property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Tricube property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Tricube property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Tricube property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Tricube property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Tricube property)@\spxentry{explicit}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Tricube method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Tricube method)@\spxentry{forward()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Tricube property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Tricube property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Tricube property)@\spxentry{idx}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Tricube method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Tricube method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Tricube method)@\spxentry{k()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Tricube property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Tricube property)@\spxentry{normalized}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Tricube property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Tricube property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Tricube method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Tricube method)@\spxentry{reset()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Tricube property)@\spxentry{sample}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Tricube property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Tricube property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Tricube property)@\spxentry{sigma}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Tricube property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Tricube property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Tricube method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Tricube method)@\spxentry{train()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Tricube method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Tricube method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Tricube method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Tricube method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/tricube:kerch.kernel.Tricube.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/tricube:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/tricube:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Tricube}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tricube Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{tricube-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/tricube:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Tricube}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Tricube}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Tricube (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Tricube (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{tricube-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/tricube:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-4a47dd3bbd7dbf6fb66f271fe32bd5d3f70b0d70.pdf}

\sphinxstepscope


\subsubsection{Exponential Kernel}
\label{\detokenize{kernel/statistics/exponential:exponential-kernel}}\label{\detokenize{kernel/statistics/exponential::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/exponential:class}}\index{Exponential (class in kerch.kernel)@\spxentry{Exponential}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Exponential}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Generic exponential kernels.
\begin{equation*}
\begin{split}k(x,y) = \exp\left(-\frac{d(x,y)^2}{2\sigma^2}\right),\end{split}
\end{equation*}
\sphinxAtStartPar
for the argument \sphinxcode{\sphinxupquote{squared=True}} (default) and
\begin{equation*}
\begin{split}k(x,y) = \exp\left(-\frac{d(x,y)}{\sqrt{2}\sigma}\right),\end{split}
\end{equation*}
\sphinxAtStartPar
for the argument \sphinxcode{\sphinxupquote{squared=False}}.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{squared\textasciigrave{}\textasciigrave{}In the case of \textasciigrave{}\textasciigrave{}distance=\textquotesingle{}euclidean\textquotesingle{}}} (default), these shapes correspond to the {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RBF}}}}} for
the \sphinxcode{\sphinxupquote{squared=True}} (default) and {\hyperref[\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Laplacian}}}}} for \sphinxcode{\sphinxupquote{squared=False}}. In other words is
{\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Exponential}}}}} equivalent to {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RBF}}}}} with the default set of parameters.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{squared}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}) \textendash{} Boolean indicating whether the norm in the exponential is squared. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Exponential property)@\spxentry{Corr}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Exponential property)@\spxentry{Cov}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Exponential property)@\spxentry{K}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Exponential property)@\spxentry{Phi}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Exponential method)@\spxentry{c()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Exponential method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Exponential property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Exponential property)@\spxentry{centered}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Exponential method)@\spxentry{corr()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Exponential method)@\spxentry{cov()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Exponential property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Exponential property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Exponential property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Exponential property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Exponential property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Exponential property)@\spxentry{explicit}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Exponential method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Exponential method)@\spxentry{forward()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Exponential property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Exponential property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Exponential property)@\spxentry{idx}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Exponential method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Exponential method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Exponential method)@\spxentry{k()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Exponential property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Exponential property)@\spxentry{normalized}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Exponential property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Exponential property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Exponential method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Exponential method)@\spxentry{reset()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Exponential property)@\spxentry{sample}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Exponential property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Exponential property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Exponential property)@\spxentry{sigma}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Exponential property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{squared (kerch.kernel.Exponential property)@\spxentry{squared}\spxextra{kerch.kernel.Exponential property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.squared}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{squared}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the norm in the exponential is squared.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Exponential method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Exponential method)@\spxentry{train()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Exponential method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Exponential method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Exponential method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Exponential method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/exponential:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/exponential:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{k\PYGZus{}squared} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}                     \PYG{c+c1}{\PYGZsh{} same as RBF kernel}
\PYG{n}{k\PYGZus{}non\PYGZus{}squared} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{squared}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} same as Laplacian kernel}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{(}\PYG{n}{k\PYGZus{}squared}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{,} \PYG{n}{k\PYGZus{}non\PYGZus{}squared}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Exponential Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Squared (default)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Non\PYGZhy{}Squared}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{exponential-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/exponential:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exponential (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exponential (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{exponential-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/exponential:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-55073abcc3edd2a9463f157445a31910e8315722.pdf}

\sphinxstepscope


\subsubsection{Logistic Kernel}
\label{\detokenize{kernel/statistics/logistic:logistic-kernel}}\label{\detokenize{kernel/statistics/logistic::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/logistic:class}}\index{Logistic (class in kerch.kernel)@\spxentry{Logistic}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Logistic}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Logistic kernel.
\begin{equation*}
\begin{split}k(x,y) = \frac{4}{\exp\left(d(x,y) \sigma \right) + 2 + \exp\left(-d(x,y) / \sigma \right)}.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Logistic property)@\spxentry{Corr}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Logistic property)@\spxentry{Cov}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Logistic property)@\spxentry{K}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Logistic property)@\spxentry{Phi}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Logistic method)@\spxentry{c()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Logistic method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Logistic property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Logistic property)@\spxentry{centered}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Logistic method)@\spxentry{corr()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Logistic method)@\spxentry{cov()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Logistic property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Logistic property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Logistic property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Logistic property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Logistic property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Logistic property)@\spxentry{explicit}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Logistic method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Logistic method)@\spxentry{forward()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Logistic property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Logistic property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Logistic property)@\spxentry{idx}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Logistic method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Logistic method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Logistic method)@\spxentry{k()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Logistic property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Logistic property)@\spxentry{normalized}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Logistic property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Logistic property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Logistic method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Logistic method)@\spxentry{reset()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Logistic property)@\spxentry{sample}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Logistic property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Logistic property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Logistic property)@\spxentry{sigma}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Logistic property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Logistic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Logistic method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Logistic method)@\spxentry{train()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Logistic method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Logistic method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Logistic method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Logistic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/logistic:kerch.kernel.Logistic.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/logistic:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/logistic:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Logistic}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Logistic Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{logistic-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/logistic:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Logistic}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Logistic}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Logistic (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Logistic (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{logistic-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/logistic:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-2f10c5b029249e93dcc4ae9caf0182d575c733d9.pdf}

\sphinxstepscope


\subsubsection{Silverman Kernel}
\label{\detokenize{kernel/statistics/silverman:silverman-kernel}}\label{\detokenize{kernel/statistics/silverman::doc}}

\paragraph{Class}
\label{\detokenize{kernel/statistics/silverman:class}}\index{Silverman (class in kerch.kernel)@\spxentry{Silverman}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Silverman}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.select\_distance.SelectDistance}}

\sphinxAtStartPar
Logistic kernel.
\begin{equation*}
\begin{split}k(x,y) = \exp\left( - \frac{d(x,y)}{\sqrt{2}\sigma}\right) \sin\left(\frac{d(x,y)}{\sqrt{2}\sigma} + \frac{\pi}{4}\right).\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{distance}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} The normed used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}\textasciigrave{}}}. Other options are \sphinxcode{\sphinxupquote{\textquotesingle{}manhattan\textquotesingle{}}}, \sphinxcode{\sphinxupquote{\textquotesingle{}chebyshev\textquotesingle{}}},
\sphinxcode{\sphinxupquote{\textquotesingle{}minkowski\textquotesingle{}}} and \sphinxcode{\sphinxupquote{\textquotesingle{}wasserstein\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Silverman property)@\spxentry{Corr}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Silverman property)@\spxentry{Cov}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Silverman property)@\spxentry{K}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Silverman property)@\spxentry{Phi}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Silverman method)@\spxentry{c()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Silverman method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Silverman property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Silverman property)@\spxentry{centered}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Silverman method)@\spxentry{corr()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Silverman method)@\spxentry{cov()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Silverman property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Silverman property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Silverman property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Silverman property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Silverman property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Silverman property)@\spxentry{explicit}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Silverman method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Silverman method)@\spxentry{forward()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Silverman property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Silverman property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Silverman property)@\spxentry{idx}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Silverman method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Silverman method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Silverman method)@\spxentry{k()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Silverman property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Silverman property)@\spxentry{normalized}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Silverman property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Silverman property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Silverman method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Silverman method)@\spxentry{reset()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Silverman property)@\spxentry{sample}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Silverman property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Silverman property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Silverman property)@\spxentry{sigma}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Silverman property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Silverman property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Silverman method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Silverman method)@\spxentry{train()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Silverman method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Silverman method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Silverman method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Silverman method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/statistics/silverman:kerch.kernel.Silverman.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Example}
\label{\detokenize{kernel/statistics/silverman:example}}

\subparagraph{Shape}
\label{\detokenize{kernel/statistics/silverman:shape}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Silverman}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{shape} \PYG{o}{=} \PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{shape}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Silverman Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{silverman-1}.pdf}
\end{figure}


\subparagraph{Random}
\label{\detokenize{kernel/statistics/silverman:random}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input} \PYG{o}{=} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{num\PYGZus{}input}\PYG{p}{,} \PYG{n}{dim\PYGZus{}input}\PYG{p}{)}

\PYG{n}{k1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Silverman}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Silverman}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chebyshev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Silverman (Euclidean)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Silverman (Chebyshev)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{silverman-2}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/statistics/silverman:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-90f9774909afd4cd6de8b23796d9357052858594.pdf}


\subsection{Vision Kernels}
\label{\detokenize{kernel/index:vision-kernels}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Explicit Feature Map
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Kernel
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameters
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Factory Type
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.AdditiveChi2}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y) = \sum_i \frac{2x_i y_i}{x_i + y_i}\)
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’additive\_chi\_2’}}
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.SkewedChi2}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\(k(x,y) = \prod_i \frac{2\sqrt{x_i+p} \sqrt{y_i+p}}{x_i + y_i + 2}\)
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{p}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{’skewed\_chi\_2’}}
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\subsubsection{Additive Chi Squared Kernel}
\label{\detokenize{kernel/vision/additive_chi2:additive-chi-squared-kernel}}\label{\detokenize{kernel/vision/additive_chi2::doc}}\index{AdditiveChi2 (class in kerch.kernel)@\spxentry{AdditiveChi2}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{AdditiveChi2}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}

\sphinxAtStartPar
Additive Chi Squared kernel. Often used in computer vision.
\begin{equation*}
\begin{split}k(x,y) = \sum_i \frac{2x_i y_i}{x_i + y_i}.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.AdditiveChi2 property)@\spxentry{Corr}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.AdditiveChi2 property)@\spxentry{Cov}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.AdditiveChi2 property)@\spxentry{K}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.AdditiveChi2 property)@\spxentry{Phi}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.AdditiveChi2 method)@\spxentry{c()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.AdditiveChi2 method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.AdditiveChi2 property)@\spxentry{cache\_level}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.AdditiveChi2 property)@\spxentry{centered}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.AdditiveChi2 method)@\spxentry{corr()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.AdditiveChi2 method)@\spxentry{cov()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.AdditiveChi2 property)@\spxentry{current\_sample}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.AdditiveChi2 property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.AdditiveChi2 property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.AdditiveChi2 property)@\spxentry{dim\_input}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.AdditiveChi2 property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.AdditiveChi2 property)@\spxentry{explicit}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.AdditiveChi2 method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.AdditiveChi2 method)@\spxentry{forward()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.AdditiveChi2 property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.AdditiveChi2 property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.AdditiveChi2 property)@\spxentry{idx}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.AdditiveChi2 method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.AdditiveChi2 method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.AdditiveChi2 method)@\spxentry{k()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.AdditiveChi2 property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.AdditiveChi2 property)@\spxentry{normalized}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.AdditiveChi2 property)@\spxentry{num\_idx}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.AdditiveChi2 property)@\spxentry{num\_sample}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.AdditiveChi2 method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.AdditiveChi2 method)@\spxentry{reset()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.AdditiveChi2 property)@\spxentry{sample}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.AdditiveChi2 property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.AdditiveChi2 property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.AdditiveChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.AdditiveChi2 method)@\spxentry{stochastic()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.AdditiveChi2 method)@\spxentry{train()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.AdditiveChi2 method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.AdditiveChi2 method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.AdditiveChi2 method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.AdditiveChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/additive_chi2:kerch.kernel.AdditiveChi2.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/vision/additive_chi2:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-32337a1dee1be70642a3e364c1c7cbadb668904d.pdf}

\sphinxstepscope


\subsubsection{Skewed Chi Squared Kernel}
\label{\detokenize{kernel/vision/skewed_chi2:skewed-chi-squared-kernel}}\label{\detokenize{kernel/vision/skewed_chi2::doc}}\index{SkewedChi2 (class in kerch.kernel)@\spxentry{SkewedChi2}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{SkewedChi2}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.implicit.Implicit}}}}}

\sphinxAtStartPar
Skewed Chi Squared kernel. Often used in computer vision.
\begin{equation*}
\begin{split}k(x,y) = \prod_i \frac{2\sqrt{x_i+p} \sqrt{y_i+p}}{x_i + y_i + 2}.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{p}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Free parameter \(p\)., defaults to 0.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{p\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of \(p\) is to be computed. If so, a graph is computed
and \(p\) can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.SkewedChi2 property)@\spxentry{Corr}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.SkewedChi2 property)@\spxentry{Cov}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.SkewedChi2 property)@\spxentry{K}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.SkewedChi2 property)@\spxentry{Phi}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.SkewedChi2 method)@\spxentry{c()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.SkewedChi2 method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.SkewedChi2 property)@\spxentry{cache\_level}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.SkewedChi2 property)@\spxentry{centered}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.SkewedChi2 method)@\spxentry{corr()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.SkewedChi2 method)@\spxentry{cov()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.SkewedChi2 property)@\spxentry{current\_sample}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.SkewedChi2 property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.SkewedChi2 property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.SkewedChi2 property)@\spxentry{dim\_input}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.SkewedChi2 property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.SkewedChi2 property)@\spxentry{explicit}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.SkewedChi2 method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.SkewedChi2 method)@\spxentry{forward()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.SkewedChi2 property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.SkewedChi2 property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.SkewedChi2 property)@\spxentry{idx}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.SkewedChi2 method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.SkewedChi2 method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.SkewedChi2 method)@\spxentry{k()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.SkewedChi2 property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.SkewedChi2 property)@\spxentry{normalized}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.SkewedChi2 property)@\spxentry{num\_idx}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.SkewedChi2 property)@\spxentry{num\_sample}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{p (kerch.kernel.SkewedChi2 property)@\spxentry{p}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.p}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{p}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Parameter \(p\) of the kernel.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.SkewedChi2 method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.SkewedChi2 method)@\spxentry{reset()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.SkewedChi2 property)@\spxentry{sample}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.SkewedChi2 property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.SkewedChi2 property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.SkewedChi2 property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.SkewedChi2 method)@\spxentry{stochastic()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.SkewedChi2 method)@\spxentry{train()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.SkewedChi2 method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.SkewedChi2 method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.SkewedChi2 method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.SkewedChi2 method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/vision/skewed_chi2:kerch.kernel.SkewedChi2.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/vision/skewed_chi2:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-e9146085f8ad0a99f919cceca347036510bf3abd.pdf}


\subsection{Abstract Kernels}
\label{\detokenize{kernel/index:abstract-kernels}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Class
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Explicit Feature Map
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Kernel
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Parameters
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Factory Type
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Exponential}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\_dist()}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{sigma}}
&
\sphinxAtStartPar
N/A
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Explicit}}}}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\_explicit()}}
&
\sphinxAtStartPar
\(k(x,y)=\phi(x)^\top\phi(y)\)
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
N/A
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Implicit}}}}}
&
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\_implicit()}}
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
N/A
\\
\hline
\sphinxAtStartPar
{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Kernel}}}}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\_explicit()}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\_implicit()}}
&
\sphinxAtStartPar
None
&
\sphinxAtStartPar
N/A
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\subsubsection{Distance\sphinxhyphen{}Based Kernel}
\label{\detokenize{kernel/abstract/distance:distance-based-kernel}}\label{\detokenize{kernel/abstract/distance::doc}}
\sphinxAtStartPar
This class is meant to be inherited for to create kernels that are of the form
\begin{equation*}
\begin{split}k(x,y) = f\left(\frac{d(x,y)}{\sigma}\right),\end{split}
\end{equation*}
\sphinxAtStartPar
in particular the {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Exponential}}}}} class and its subclasses such as {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RBF}}}}}.


\paragraph{Abstract Classes}
\label{\detokenize{kernel/abstract/distance:abstract-classes}}\index{Distance (class in kerch.kernel)@\spxentry{Distance}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Distance}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.\_distance.\_Distance}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Distance property)@\spxentry{Corr}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Distance property)@\spxentry{Cov}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Distance property)@\spxentry{K}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/abstract/distance:kerch.kernel.Distance.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Distance property)@\spxentry{Phi}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Distance method)@\spxentry{c()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Distance method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Distance property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Distance property)@\spxentry{centered}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Distance method)@\spxentry{corr()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Distance method)@\spxentry{cov()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Distance property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Distance property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Distance property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Distance property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Distance property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Distance property)@\spxentry{explicit}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Distance method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Distance method)@\spxentry{forward()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Distance property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Distance property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Distance property)@\spxentry{idx}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Distance method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Distance method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Distance method)@\spxentry{k()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Distance property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Distance property)@\spxentry{normalized}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Distance property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Distance property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Distance method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Distance method)@\spxentry{reset()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Distance property)@\spxentry{sample}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Distance property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Distance property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.Distance property)@\spxentry{sigma}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.Distance property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.Distance property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Distance method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Distance method)@\spxentry{train()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Distance method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Distance method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Distance method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Distance method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.Distance.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{DistanceSquared (class in kerch.kernel)@\spxentry{DistanceSquared}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{DistanceSquared}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.kernel.distance.\_distance.\_Distance}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Bandwidth \(\sigma\) of the kernel. If \sphinxtitleref{None}, the value is filled by a heuristic on
the sample data: half of the square root of the median of the pairwise distances. Computing the heuristic on
the full sample data can be expensive and \sphinxtitleref{idx\_sample} or \sphinxtitleref{prop\_sample} could be specified to only compute
it on a subset only., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sigma\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradient of the bandwidth is to be computed. If so, a graph is computed
and the bandwidth can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.DistanceSquared property)@\spxentry{Corr}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.DistanceSquared property)@\spxentry{Cov}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.DistanceSquared property)@\spxentry{K}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.DistanceSquared property)@\spxentry{Phi}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.DistanceSquared method)@\spxentry{c()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.DistanceSquared method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.DistanceSquared property)@\spxentry{cache\_level}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.DistanceSquared property)@\spxentry{centered}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.DistanceSquared method)@\spxentry{corr()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.DistanceSquared method)@\spxentry{cov()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.DistanceSquared property)@\spxentry{current\_sample}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.DistanceSquared property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.DistanceSquared property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.DistanceSquared property)@\spxentry{dim\_input}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.DistanceSquared property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.DistanceSquared property)@\spxentry{explicit}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.DistanceSquared method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.DistanceSquared method)@\spxentry{forward()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.DistanceSquared property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.DistanceSquared property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.DistanceSquared property)@\spxentry{idx}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.DistanceSquared method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.DistanceSquared method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.DistanceSquared method)@\spxentry{k()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.DistanceSquared property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.DistanceSquared property)@\spxentry{normalized}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.DistanceSquared property)@\spxentry{num\_idx}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.DistanceSquared property)@\spxentry{num\_sample}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.DistanceSquared method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.DistanceSquared method)@\spxentry{reset()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.DistanceSquared property)@\spxentry{sample}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.DistanceSquared property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.DistanceSquared property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{sigma (kerch.kernel.DistanceSquared property)@\spxentry{sigma}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.sigma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma}}}
\pysigstopsignatures
\sphinxAtStartPar
Bandwidth \(\sigma\) of the kernel.

\end{fulllineitems}

\index{sigma\_trainable (kerch.kernel.DistanceSquared property)@\spxentry{sigma\_trainable}\spxextra{kerch.kernel.DistanceSquared property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.sigma_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sigma\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating of the bandwidth \(\sigma\) is trainable.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.DistanceSquared method)@\spxentry{stochastic()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.DistanceSquared method)@\spxentry{train()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.DistanceSquared method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.DistanceSquared method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.DistanceSquared method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.DistanceSquared method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/distance:kerch.kernel.DistanceSquared.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Defining a New Kernel}
\label{\detokenize{kernel/abstract/distance:defining-a-new-kernel}}
\sphinxAtStartPar
To create a new kernel, it suffices to provide an implementation of the private method \sphinxcode{\sphinxupquote{\_dist(self, x, y)}}. The
automatic definition of the bandwidth and the other functionalities of all exponential kernel will be automatically
provided. The following examples show how to implement a kernel based on the \(\ell^1\)\sphinxhyphen{}distance. In particular
\begin{equation*}
\begin{split}k(x,y) = \exp\left( -\frac{\lVert x-y \rVert_1}{2\sigma^2} \right).\end{split}
\end{equation*}

\subparagraph{Minimal Example}
\label{\detokenize{kernel/abstract/distance:minimal-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{c+c1}{\PYGZsh{} we define our l1 kernel}
\PYG{k}{class} \PYG{n+nc}{MyExponential}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}dist}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num\PYGZus{}x, dim]}
        \PYG{c+c1}{\PYGZsh{} y: torch.Tensor of size [num\PYGZus{}y, dim]}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{]}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}

        \PYG{n}{diff} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{y}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num\PYGZus{}x, num\PYGZus{}y]}
        \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{diff}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{keepdim}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we define our sample}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} now we can just use the kernel}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{MyExponential}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sigma = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mi}{1}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sigma = }\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{+}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{sigma}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-1_01}.pdf}
\end{figure}

\sphinxAtStartPar
This also works with the other properties of the kernels. In the following example, we have project all inputs linearly
in the interval \([0,1]\) (\sphinxcode{\sphinxupquote{minmax\_rescaling}}) and the feature map/kernel is centered.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we define the sample}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we also define an out\PYGZus{}of\PYGZus{}sample}
\PYG{n}{t\PYGZus{}oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t\PYGZus{}oos} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t\PYGZus{}oos} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we initialize our new kernel}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{MyExponential}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sample\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{minmax\PYGZus{}rescaling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} sample}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{current\PYGZus{}sample\PYGZus{}projected}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} out\PYGZhy{}of\PYGZhy{}sample}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{transform\PYGZus{}input}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Out\PYGZhy{}of\PYGZhy{}Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-2_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-2_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-2_02}.pdf}
\end{figure}


\subparagraph{Extensive Example}
\label{\detokenize{kernel/abstract/distance:extensive-example}}
\sphinxAtStartPar
This is more extensive example where an extra parameter is added to control the degree of the distance. We also provide
a name and the \sphinxcode{\sphinxupquote{hparams}} property.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we define our l1 kernel}
\PYG{k}{class} \PYG{n+nc}{MyExponential}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyExponential}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} all parameters are typically passed through the keyword arguments kwargs}
        \PYG{n}{degree} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{degree}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we ensure that is a torch value of the correct data type used by kerch (modifiable)}
        \PYG{n}{degree} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{castf}\PYG{p}{(}\PYG{n}{degree}\PYG{p}{,} \PYG{n}{tensor}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we now store it as a parameter, tu ensure that the values are ported over when changing the device}
        \PYG{c+c1}{\PYGZsh{} ! due to the nature of PyTorch, parameters can only be added after the call to super.}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{degree} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Parameter}\PYG{p}{(}\PYG{n}{degree}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}str\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} it is always nicer to add a name to the kernel (it must begin with small letters, the capitalization is automatic)}
        \PYG{c+c1}{\PYGZsh{} this also has an influence on the \PYGZus{}\PYGZus{}repr\PYGZus{}\PYGZus{} attribute}
        \PYG{k}{return} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{l1 exponential kernel}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{hparams}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} this returns a dictionary containing the properties}
        \PYG{c+c1}{\PYGZsh{} it is important to call the super to also pass the other parameters like the sigma etc.}
        \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L1 Exponential}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Degree}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{degree}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
                \PYG{o}{*}\PYG{o}{*}\PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyExponential}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{\PYGZcb{}}


    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}dist}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num\PYGZus{}x, self.dim\PYGZus{}input]}
        \PYG{c+c1}{\PYGZsh{} y: torch.Tensor of size [num\PYGZus{}y, self.dim\PYGZus{}input]}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{]}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}

        \PYG{n}{diff} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{y}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num\PYGZus{}x, num\PYGZus{}y]}
        \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{diff}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{degree}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{keepdim}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}


\subparagraph{Example without an Exponential}
\label{\detokenize{kernel/abstract/distance:example-without-an-exponential}}
\sphinxAtStartPar
We can also directly use the {\hyperref[\detokenize{kernel/abstract/distance:kerch.kernel.Distance}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Distance}}}}} class to create a kernel that is not an exponential.
For example, let us consider the implementation of an \(\ell^1\)\sphinxhyphen{}based and the use of the pre\sphinxhyphen{}defined
\(\ell^2\)\sphinxhyphen{}distance based kernel.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we define our l1 kernel}
\PYG{k}{class} \PYG{n+nc}{MyL1Distance}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Distance}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Distance}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}dist}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num\PYGZus{}x, dim]}
        \PYG{c+c1}{\PYGZsh{} y: torch.Tensor of size [num\PYGZus{}y, dim]}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{]}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{T}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}

        \PYG{n}{diff} \PYG{o}{=} \PYG{n}{x} \PYG{o}{\PYGZhy{}} \PYG{n}{y}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num\PYGZus{}x, num\PYGZus{}y]}
        \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{diff}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{keepdim}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} we define out kernel that is going to use the distances}
\PYG{k}{class} \PYG{n+nc}{MyKernel}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{SelectDistance}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{SelectDistance}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}implicit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num\PYGZus{}x, dim]}
        \PYG{c+c1}{\PYGZsh{} y: torch.Tensor of size [num\PYGZus{}y, dim]}
        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num\PYGZus{}x, num\PYGZus{}y]}
        \PYG{k}{return} \PYG{o}{\PYGZhy{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}dist}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we define our sample}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we define our two kernels, the first one with our defined distance, the second one with a pre\PYGZhy{}defined distance}
\PYG{n}{k1} \PYG{o}{=} \PYG{n}{MyKernel}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{n}{MyL1Distance}\PYG{p}{)}
\PYG{n}{k2} \PYG{o}{=} \PYG{n}{MyKernel}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{distance}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{euclidean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k1}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{L1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k2}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{L2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-3_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{distance-3_01}.pdf}
\end{figure}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/abstract/distance:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-4033d6c3aa1bf9241cbf7b6878115d8c37b056f6.pdf}

\sphinxstepscope


\subsubsection{Explicit Kernel}
\label{\detokenize{kernel/abstract/explicit:explicit-kernel}}\label{\detokenize{kernel/abstract/explicit::doc}}
\sphinxAtStartPar
This class is meant to be inherited to create kernels that are defined by an explicit feature map \(\phi(x)\). The
kernel will be automatically computed as the inner product of the feature maps \(k(x,y)=\phi(x)^\top\phi(y)\)
without it having to be defined in the inherited implementation.


\paragraph{Abstract Class}
\label{\detokenize{kernel/abstract/explicit:abstract-class}}\index{Explicit (class in kerch.kernel)@\spxentry{Explicit}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Explicit}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.kernel.Kernel}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.Explicit property)@\spxentry{C}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.Explicit property)@\spxentry{Corr}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Explicit property)@\spxentry{Cov}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Explicit property)@\spxentry{K}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Explicit property)@\spxentry{Phi}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Explicit method)@\spxentry{c()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Explicit method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Explicit property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Explicit property)@\spxentry{centered}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Explicit method)@\spxentry{corr()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Explicit method)@\spxentry{cov()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Explicit property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Explicit property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Explicit property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Explicit property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Explicit property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Explicit property)@\spxentry{explicit}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Explicit method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.Explicit method)@\spxentry{forward()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Explicit property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Explicit property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Explicit property)@\spxentry{idx}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Explicit method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Explicit method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Explicit method)@\spxentry{k()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Explicit property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Explicit property)@\spxentry{normalized}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Explicit property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Explicit property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.Explicit method)@\spxentry{phi()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Explicit method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Explicit method)@\spxentry{reset()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Explicit property)@\spxentry{sample}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Explicit property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Explicit property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Explicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Explicit method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Explicit method)@\spxentry{train()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Explicit method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Explicit method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Explicit method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Explicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Defining a New Kernel}
\label{\detokenize{kernel/abstract/explicit:defining-a-new-kernel}}
\sphinxAtStartPar
To create a new kernel, it suffices to provide an implementation of the private method \sphinxcode{\sphinxupquote{\_explicit(self, x)}}.
By inheriting from \sphinxcode{\sphinxupquote{kerch.kernel.Explicit}}, all the functionalities of explicit kernels will be inherited. As an
example, let us implement the following kernel:
\begin{equation*}
\begin{split}\phi(x) = \left[x_1, \ldots, x_{\texttt{num}}, x_1^2, \ldots, x_{\texttt{num}}^2, \log(x^\top x + 1)\right].\end{split}
\end{equation*}

\subparagraph{Minimal Example}
\label{\detokenize{kernel/abstract/explicit:minimal-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{c+c1}{\PYGZsh{} we define our new kernel}
\PYG{k}{class} \PYG{n+nc}{MyExplicit}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Explicit}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}explicit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num, self.dim\PYGZus{}input]}
        \PYG{n}{phi1} \PYG{o}{=} \PYG{n}{x}                                                    \PYG{c+c1}{\PYGZsh{} [num, self.dim\PYGZus{}input]}
        \PYG{n}{phi2} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}                                               \PYG{c+c1}{\PYGZsh{} [num, self.dim\PYGZus{}input]}
        \PYG{n}{phi3} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{x} \PYG{o}{*} \PYG{n}{x}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{keepdim}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} [num, 1]}

        \PYG{n}{phi} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{(}\PYG{n}{phi1}\PYG{p}{,} \PYG{n}{phi2}\PYG{p}{,} \PYG{n}{phi3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} [num, 2*self.dim\PYGZus{}input + 1]}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num, self.dim\PYGZus{}feature]}
        \PYG{c+c1}{\PYGZsh{} if not specified (see further), self.dim\PYGZus{}feature will be determined automatically}
        \PYG{k}{return} \PYG{n}{phi}

\PYG{c+c1}{\PYGZsh{} now we can just use the kernel}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{MyExplicit}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}


\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{Phi}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Feature Map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Kernel Matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{explicit-1_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{explicit-1_01}.pdf}
\end{figure}

\sphinxAtStartPar
This also works with the other properties of the kernels. In the following example, we have project all inputs linearly
in the interval \([0,1]\) (\sphinxcode{\sphinxupquote{\textquotesingle{}minmax\_rescaling\textquotesingle{}}}) and the feature map is standardized (\sphinxcode{\sphinxupquote{\textquotesingle{}standard\textquotesingle{}}}).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we define the sample}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we also define an out\PYGZus{}of\PYGZus{}sample}
\PYG{n}{t\PYGZus{}oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t\PYGZus{}oos} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t\PYGZus{}oos} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we initialize our new kernel}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{MyExplicit}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sample\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{minmax\PYGZus{}rescaling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{standard}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} sample}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{current\PYGZus{}sample\PYGZus{}projected}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{Phi}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Feature Map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} out\PYGZhy{}of\PYGZhy{}sample}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{transform\PYGZus{}input}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{phi}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Feature Map}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Out\PYGZhy{}of\PYGZhy{}Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{explicit-2_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{explicit-2_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{explicit-2_02}.pdf}
\end{figure}


\subparagraph{Extensive Example}
\label{\detokenize{kernel/abstract/explicit:extensive-example}}
\sphinxAtStartPar
This is more extensive example where an extra parameter is added to control the degree of the exponentiation. We also provide
a name and the \sphinxcode{\sphinxupquote{hparams}} property. We also define \sphinxcode{\sphinxupquote{dim\_feature}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} we define our new kernel}
\PYG{k}{class} \PYG{n+nc}{MyExplicit}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} all parameters are typically passed through the keyword arguments kwargs}
        \PYG{n}{degree} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{degree}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} call to super}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyExplicit}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we ensure that is a torch value of the correct data type used by kerch (modifiable)}
        \PYG{n}{degree} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{castf}\PYG{p}{(}\PYG{n}{degree}\PYG{p}{,} \PYG{n}{tensor}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we now store it as a parameter, tu ensure that the values are ported over when changing the device}
        \PYG{c+c1}{\PYGZsh{} ! due to the nature of PyTorch, parameters can only be added after the call to super.}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{degree} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Parameter}\PYG{p}{(}\PYG{n}{degree}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}str\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} it is always nicer to add a name to the kernel (it must begin with small letters, the capitalization is automatic)}
        \PYG{c+c1}{\PYGZsh{} this also has an influence on the \PYGZus{}\PYGZus{}repr\PYGZus{}\PYGZus{} attribute}
        \PYG{k}{return} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my explicit kernel}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{dim\PYGZus{}feature}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{int}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} it is more efficient to provide the feature dimension explicitly to avoid the model determining it on the}
        \PYG{c+c1}{\PYGZsh{} running the explicit method on the sample, which is a rather useless computation}
        \PYG{k}{return} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dim\PYGZus{}input} \PYG{o}{+} \PYG{l+m+mi}{1}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{hparams}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} this returns a dictionary containing the properties}
        \PYG{c+c1}{\PYGZsh{} it is important to call the super to also pass the other parameters like the sigma etc.}
        \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MyExplicit}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Degree}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{degree}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
                \PYG{o}{*}\PYG{o}{*}\PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyExplicit}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{\PYGZcb{}}


    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}explicit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num, self.dim\PYGZus{}input]}
        \PYG{n}{phi1} \PYG{o}{=} \PYG{n}{x}                                                    \PYG{c+c1}{\PYGZsh{} [num, self.dim\PYGZus{}input]}
        \PYG{n}{phi2} \PYG{o}{=} \PYG{n}{x} \PYG{o}{*}\PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{degree}                                     \PYG{c+c1}{\PYGZsh{} [num, self.dim\PYGZus{}input]}
        \PYG{n}{phi3} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{x} \PYG{o}{*} \PYG{n}{x}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{keepdim}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} [num, 1]}

        \PYG{n}{phi} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{cat}\PYG{p}{(}\PYG{p}{(}\PYG{n}{phi1}\PYG{p}{,} \PYG{n}{phi2}\PYG{p}{,} \PYG{n}{phi3}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}                  \PYG{c+c1}{\PYGZsh{} [num, 2 * self.dim\PYGZus{}input + 1]}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num, self.dim\PYGZus{}feature]}
        \PYG{c+c1}{\PYGZsh{} self.dim\PYGZus{}feature is provided so it will be deduced from the provided definition and not computed on the go}
        \PYG{k}{return} \PYG{n}{phi}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/abstract/explicit:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-e9810b589caf9cec1854a455ec70d4f47ac12b14.pdf}

\sphinxstepscope


\subsubsection{Implicit Kernel}
\label{\detokenize{kernel/abstract/implicit:implicit-kernel}}\label{\detokenize{kernel/abstract/implicit::doc}}
\sphinxAtStartPar
This class is meant to be inherited to create kernel that are defined implicitly with a \(k(x,y)\). These classes
do not have an explicit representation as it is supposed to live in a infinite\sphinxhyphen{}dimensional Hilbert space.


\paragraph{Abstract Class}
\label{\detokenize{kernel/abstract/implicit:abstract-class}}\index{Implicit (class in kerch.kernel)@\spxentry{Implicit}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Implicit}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.kernel.Kernel}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{Corr (kerch.kernel.Implicit property)@\spxentry{Corr}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Implicit property)@\spxentry{Cov}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Implicit property)@\spxentry{K}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Implicit property)@\spxentry{Phi}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
\sphinxcode{\sphinxupquote{phi()}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Implicit method)@\spxentry{c()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Implicit method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Implicit property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Implicit property)@\spxentry{centered}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Implicit method)@\spxentry{corr()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Implicit method)@\spxentry{cov()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Implicit property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Implicit property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Implicit property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
For implicit kernels, the feature dimension is infinite.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Implicit property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Implicit property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Implicit property)@\spxentry{explicit}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Implicit method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Not available for kernels that have no explicit feature map representation.
\end{sphinxadmonition}

\end{fulllineitems}

\index{forward() (kerch.kernel.Implicit method)@\spxentry{forward()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Implicit property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Implicit property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Implicit property)@\spxentry{idx}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Implicit method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Implicit method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Implicit method)@\spxentry{k()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Implicit property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Implicit property)@\spxentry{normalized}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Implicit property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Implicit property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Implicit method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Implicit method)@\spxentry{reset()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Implicit property)@\spxentry{sample}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Implicit property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Implicit property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Implicit property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Implicit method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Implicit method)@\spxentry{train()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Implicit method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Implicit method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Implicit method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Implicit method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Defining a New Kernel}
\label{\detokenize{kernel/abstract/implicit:defining-a-new-kernel}}
\sphinxAtStartPar
To create a new kernel, it suffices to provide an implementation of the private method \sphinxcode{\sphinxupquote{\_implicit(self, x, y)}}.
By inheriting from \sphinxcode{\sphinxupquote{kerch.kernel.Implicit}}, all the functionalities of explicit kernels will be inherited. As an
example, let us implement the following kernel:
\begin{equation*}
\begin{split}k(x,y) = \log(x^\top y + 1).\end{split}
\end{equation*}

\subparagraph{Minimal Example}
\label{\detokenize{kernel/abstract/implicit:minimal-example}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{c+c1}{\PYGZsh{} we define our new kernel}
\PYG{k}{class} \PYG{n+nc}{MyImplicit}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Implicit}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}implicit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num\PYGZus{}x, self.dim\PYGZus{}input]}
        \PYG{c+c1}{\PYGZsh{} y: torch.Tensor of size [num\PYGZus{}y, self.dim\PYGZus{}input]}

        \PYG{n}{k} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{x} \PYG{o}{@} \PYG{n}{y}\PYG{o}{.}\PYG{n}{T} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num\PYGZus{}x, num\PYGZus{}y]}
        \PYG{k}{return} \PYG{n}{k}

\PYG{c+c1}{\PYGZsh{} now we can just use the kernel}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{MyImplicit}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Kernel Matrix}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{implicit-1}.pdf}
\end{figure}

\sphinxAtStartPar
This also works with the other properties of the kernels. In the following example, we have project all inputs linearly
in the interval \([0,1]\) (\sphinxcode{\sphinxupquote{minmax\_rescaling}}) and the feature map/kernel is centered and then normalized.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we define the sample}
\PYG{n}{t} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{sample} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we also define an out\PYGZus{}of\PYGZus{}sample}
\PYG{n}{t\PYGZus{}oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,}\PYG{l+m+mi}{30}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{t\PYGZus{}oos} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{t\PYGZus{}oos} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we initialize our new kernel}
\PYG{n}{k} \PYG{o}{=} \PYG{n}{MyImplicit}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,} \PYG{n}{sample\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{minmax\PYGZus{}rescaling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{normalize}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} sample}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{current\PYGZus{}sample\PYGZus{}projected}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} out\PYGZhy{}of\PYGZhy{}sample}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{transform\PYGZus{}input}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Transformed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{orientation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{horizontal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Out\PYGZhy{}of\PYGZhy{}Sample}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel matrix}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Sample \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} Sample}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{im} \PYG{o}{=} \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{oos}\PYG{p}{)}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OOS \PYGZhy{} OOS}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flat}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{im}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{axs}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{tolist}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{implicit-2_00}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{implicit-2_01}.pdf}
\end{figure}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{implicit-2_02}.pdf}
\end{figure}


\subparagraph{Extensive Example}
\label{\detokenize{kernel/abstract/implicit:extensive-example}}
\sphinxAtStartPar
This is more extensive example where an extra parameter \(a\) is added. We also provide
a name and the \sphinxcode{\sphinxupquote{hparams}} property. The extended kernel definition is
\begin{equation*}
\begin{split}k(x,y) = \log(x^\top y + a).\end{split}
\end{equation*}
\sphinxAtStartPar
Because of the logarithm, we must also verify that \(a\) is strictly positive (inner product are guaranteed positive).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} we define our new implicit kernel}
\PYG{k}{class} \PYG{n+nc}{MyImplicit}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Exponential}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} all parameters are typically passed through the keyword arguments kwargs}
        \PYG{n}{a} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we assert that the value is positive because of the logarithm.}
        \PYG{k}{assert} \PYG{n}{a} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The value for the parameter a must be strictly positive.}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{c+c1}{\PYGZsh{} we ensure that is a torch value of the correct data type used by kerch (modifiable)}
        \PYG{n}{a} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{utils}\PYG{o}{.}\PYG{n}{castf}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{tensor}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} call to super}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyImplicit}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we now store it as a parameter, tu ensure that the values are ported over when changing the device}
        \PYG{c+c1}{\PYGZsh{} ! due to the nature of PyTorch, parameters can only be added after the call to super.}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{a} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Parameter}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}str\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} it is always nicer to add a name to the kernel (it must begin with small letters, the capitalization is automatic)}
        \PYG{c+c1}{\PYGZsh{} this also has an influence on the \PYGZus{}\PYGZus{}repr\PYGZus{}\PYGZus{} attribute}
        \PYG{k}{return} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{my implicit kernel}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{hparams}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} this returns a dictionary containing the properties}
        \PYG{c+c1}{\PYGZsh{} it is important to call the super to also pass the other parameters like the sigma etc.}
        \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MyImplicit}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel parameter a}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{a}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
                \PYG{o}{*}\PYG{o}{*}\PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyExplicit}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{\PYGZcb{}}


    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}implicit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} x: torch.Tensor of size [num\PYGZus{}x, self.dim\PYGZus{}input]}
        \PYG{c+c1}{\PYGZsh{} y: torch.Tensor of size [num\PYGZus{}y, self.dim\PYGZus{}input]}

        \PYG{n}{k} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{x} \PYG{o}{@} \PYG{n}{y}\PYG{o}{.}\PYG{n}{T} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{a}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} return torch.Tensor of size [num\PYGZus{}x, num\PYGZus{}y]}
        \PYG{k}{return} \PYG{n}{k}
\end{sphinxVerbatim}


\paragraph{Inheritance Diagram}
\label{\detokenize{kernel/abstract/implicit:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-9ae4b5227d68e4303de115ce51e30bafc2d5e0a8.pdf}

\sphinxstepscope


\subsubsection{Kernel Class}
\label{\detokenize{kernel/abstract/kernel:kernel-class}}\label{\detokenize{kernel/abstract/kernel::doc}}
\sphinxAtStartPar
This class is meant to create kernels that have an explicit feature map \(\phi(x)\), but also another way of computing the kernel
\(k(x,y)\) than through the inner product of the feature maps. This is for example the case of the {\hyperref[\detokenize{kernel/generic/polynomial:kerch.kernel.Polynomial}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Polynomial}}}}} kernel.
When inheriting from this class, both methods \sphinxcode{\sphinxupquote{\_explicit(self, x)}}, \sphinxcode{\sphinxupquote{\_implicit(self, x, y)}} and the property \sphinxcode{\sphinxupquote{\_dim\_feature}}
need to be defined. Beware however of inconsistencies as depending on the representation, the kernel will be computed either
implicitly with the provided method or explicitly as the inner product of the explicit feature map defined by the other method.
If you only want to define one of both method, it is preferable directly use {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Implicit}}}}} or {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Explicit}}}}}
to avoid possible inconsistencies. We also refer to these classes for the correct implementation of the necessary methods.


\paragraph{Abstract Class}
\label{\detokenize{kernel/abstract/kernel:abstract-class}}\index{Kernel (class in kerch.kernel)@\spxentry{Kernel}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{Kernel}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.\_base\_kernel.\_BaseKernel}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} A list composed of the elements \sphinxtitleref{‘normalize’} or \sphinxtitleref{‘center’}. For example a centered
cosine kernel which is centered and normalized in order to get a covariance matrix for example can be obtained
by invoking a linear kernel with \sphinxtitleref{default\_transform = {[}‘normalize’, ‘center’, ‘normalize’{]}} or just a cosine
kernel with \sphinxtitleref{default\_transform = {[}‘center’, ‘normalize’{]}}. Redundancy is automatically handled., defaults
to \sphinxtitleref{{[}{]}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.Kernel property)@\spxentry{C}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.Kernel property)@\spxentry{Corr}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.Kernel property)@\spxentry{Cov}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.Kernel property)@\spxentry{K}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.Kernel property)@\spxentry{Phi}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.Kernel method)@\spxentry{c()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.Kernel method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.Kernel property)@\spxentry{cache\_level}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.kernel.Kernel property)@\spxentry{centered}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.kernel.Kernel method)@\spxentry{corr()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.kernel.Kernel method)@\spxentry{cov()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.kernel.Kernel property)@\spxentry{current\_sample}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.Kernel property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.Kernel property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.Kernel property)@\spxentry{dim\_input}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.Kernel property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.Kernel property)@\spxentry{explicit}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.kernel.Kernel method)@\spxentry{explicit\_preimage()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.kernel.Kernel method)@\spxentry{forward()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}dual\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.Kernel property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.Kernel property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.Kernel property)@\spxentry{idx}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.Kernel method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.Kernel method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.Kernel method)@\spxentry{k()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_transform (kerch.kernel.Kernel property)@\spxentry{kernel\_transform}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{normalized (kerch.kernel.Kernel property)@\spxentry{normalized}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.kernel.Kernel property)@\spxentry{num\_idx}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.Kernel property)@\spxentry{num\_sample}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.Kernel method)@\spxentry{phi()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.Kernel method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.Kernel method)@\spxentry{reset()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.Kernel property)@\spxentry{sample}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.Kernel property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.Kernel property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.Kernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.Kernel method)@\spxentry{stochastic()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.Kernel method)@\spxentry{train()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.Kernel method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.Kernel method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.Kernel method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.Kernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\paragraph{Mother Class}
\label{\detokenize{kernel/abstract/kernel:mother-class}}
\sphinxAtStartPar
This abstract class is itself built onto an even more abstract class that does not support kernel transforms. One can
inherit from that one if no transforms are required. The cache management is also minimal.
\index{\_BaseKernel (class in kerch.kernel)@\spxentry{\_BaseKernel}\spxextra{class in kerch.kernel}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.kernel.}}\sphinxbfcode{\sphinxupquote{\_BaseKernel}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{features/sample:kerch.feature.Sample}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.sample.Sample}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{C (kerch.kernel.\_BaseKernel property)@\spxentry{C}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.kernel.\_BaseKernel property)@\spxentry{Corr}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.kernel.\_BaseKernel property)@\spxentry{Cov}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{K (kerch.kernel.\_BaseKernel property)@\spxentry{K}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.kernel.\_BaseKernel property)@\spxentry{Phi}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{c() (kerch.kernel.\_BaseKernel method)@\spxentry{c()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac1M\sum_{i}^{M} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor(dim\_feature,dim\_feature)

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.kernel.\_BaseKernel method)@\spxentry{cache\_keys()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.kernel.\_BaseKernel property)@\spxentry{cache\_level}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{current\_sample (kerch.kernel.\_BaseKernel property)@\spxentry{current\_sample}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.kernel.\_BaseKernel property)@\spxentry{current\_sample\_projected}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_feature (kerch.kernel.\_BaseKernel property)@\spxentry{dim\_feature}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }inf}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the dimension of the explicit feature map if it exists.

\end{fulllineitems}

\index{dim\_input (kerch.kernel.\_BaseKernel property)@\spxentry{dim\_input}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.kernel.\_BaseKernel property)@\spxentry{empty\_sample}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{explicit (kerch.kernel.\_BaseKernel property)@\spxentry{explicit}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{forward() (kerch.kernel.\_BaseKernel method)@\spxentry{forward()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}implicit\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.kernel.\_BaseKernel property)@\spxentry{hparams\_fixed}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.kernel.\_BaseKernel property)@\spxentry{hparams\_variable}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.kernel.\_BaseKernel property)@\spxentry{idx}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.kernel.\_BaseKernel method)@\spxentry{implicit\_preimage()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.kernel.\_BaseKernel method)@\spxentry{init\_sample()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.kernel.\_BaseKernel method)@\spxentry{k()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{N,M},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^N\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^N\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels, this computation is more expensive as it requires to \_center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{M}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor(N,M)

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_idx (kerch.kernel.\_BaseKernel property)@\spxentry{num\_idx}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.kernel.\_BaseKernel property)@\spxentry{num\_sample}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{phi() (kerch.kernel.\_BaseKernel method)@\spxentry{phi()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.,
defaults to \sphinxtitleref{None}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{print\_cache() (kerch.kernel.\_BaseKernel method)@\spxentry{print\_cache()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.kernel.\_BaseKernel method)@\spxentry{reset()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.kernel.\_BaseKernel property)@\spxentry{sample}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.kernel.\_BaseKernel property)@\spxentry{sample\_trainable}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.kernel.\_BaseKernel property)@\spxentry{sample\_transform}\spxextra{kerch.kernel.\_BaseKernel property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.kernel.\_BaseKernel method)@\spxentry{stochastic()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.kernel.\_BaseKernel method)@\spxentry{train()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.kernel.\_BaseKernel method)@\spxentry{transform\_input()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.kernel.\_BaseKernel method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.kernel.\_BaseKernel method)@\spxentry{update\_sample()}\spxextra{kerch.kernel.\_BaseKernel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subparagraph{Inheritance Diagram}
\label{\detokenize{kernel/abstract/kernel:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-a2846e9ca3495f8d51a9ea28344f0e95d9f63355.pdf}


\section{Creating Kernels}
\label{\detokenize{kernel/index:creating-kernels}}
\sphinxAtStartPar
It is also possible to create other kernels than the ones already implemented and still taking benefit of all features
common to all kernel such as automatic transformation of the input of the kernel (centering, normalization…), all
attributes (\sphinxcode{\sphinxupquote{K}}, \sphinxcode{\sphinxupquote{Phi}}…), cache management, stochasicity etc. We refer to the examples in the documentation of the abstract
classes {\hyperref[\detokenize{kernel/statistics/exponential:kerch.kernel.Exponential}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Exponential}}}}}, {\hyperref[\detokenize{kernel/abstract/explicit:kerch.kernel.Explicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Explicit}}}}}, {\hyperref[\detokenize{kernel/abstract/implicit:kerch.kernel.Implicit}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Implicit}}}}} and
{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Kernel}}}}}.

\sphinxstepscope


\chapter{Level Module}
\label{\detokenize{level/index:level-module}}\label{\detokenize{level/index::doc}}

\section{Single\sphinxhyphen{}View}
\label{\detokenize{level/index:single-view}}
\sphinxstepscope


\subsection{Kernel Principal Component Analysis}
\label{\detokenize{level/kpca:kernel-principal-component-analysis}}\label{\detokenize{level/kpca::doc}}

\subsubsection{Class}
\label{\detokenize{level/kpca:class}}\index{KPCA (class in kerch.level)@\spxentry{KPCA}\spxextra{class in kerch.level}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.level.}}\sphinxbfcode{\sphinxupquote{KPCA}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.level.\_KPCA.\_KPCA}}, \sphinxcode{\sphinxupquote{kerch.level.single\_view.Level.Level}}

\sphinxAtStartPar
Kernel Principal Component Analysis.
\index{C (kerch.level.KPCA property)@\spxentry{C}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{C\_reconstructed (kerch.level.KPCA property)@\spxentry{C\_reconstructed}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.C_reconstructed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C\_reconstructed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{Corr (kerch.level.KPCA property)@\spxentry{Corr}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.level.KPCA property)@\spxentry{Cov}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{H (kerch.level.KPCA property)@\spxentry{H}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.H}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{H}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{K (kerch.level.KPCA property)@\spxentry{K}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{level/kpca:kerch.level.KPCA.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{K\_reconstructed (kerch.level.KPCA property)@\spxentry{K\_reconstructed}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.K_reconstructed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K\_reconstructed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{Phi (kerch.level.KPCA property)@\spxentry{Phi}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{level/kpca:kerch.level.KPCA.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{PhiW (kerch.level.KPCA property)@\spxentry{PhiW}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.PhiW}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{PhiW}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{W (kerch.level.KPCA property)@\spxentry{W}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.W}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{W}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{after\_step() (kerch.level.KPCA method)@\spxentry{after\_step()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs after\sphinxhyphen{}step operations, for example a transform of the parameters onto some manifold.

\end{fulllineitems}

\index{attach\_to() (kerch.level.KPCA method)@\spxentry{attach\_to()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.attach_to}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attach\_to}}}{\emph{\DUrole{n}{weight\_fn}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{attached (kerch.level.KPCA property)@\spxentry{attached}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.attached}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{attached}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the view is attached to another multi\_view.

\end{fulllineitems}

\index{before\_step() (kerch.level.KPCA method)@\spxentry{before\_step()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.before_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{before\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs steps required before each training step.

\end{fulllineitems}

\index{bias (kerch.level.KPCA property)@\spxentry{bias}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{bias\_trainable (kerch.level.KPCA property)@\spxentry{bias\_trainable}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.bias_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{c() (kerch.level.KPCA method)@\spxentry{c()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.level.KPCA method)@\spxentry{cache\_keys()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.level.KPCA property)@\spxentry{cache\_level}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.level.KPCA property)@\spxentry{centered}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.level.KPCA method)@\spxentry{corr()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.level.KPCA method)@\spxentry{cov()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.level.KPCA property)@\spxentry{current\_sample}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.level.KPCA property)@\spxentry{current\_sample\_projected}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{current\_target (kerch.level.KPCA property)@\spxentry{current\_target}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.current_target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the target that are currently used in the computations, taking the stochastic aspect into account
if relevant.

\end{fulllineitems}

\index{detach() (kerch.level.KPCA method)@\spxentry{detach()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.detach}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{detach}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{dim\_feature (kerch.level.KPCA property)@\spxentry{dim\_feature}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map if relevant.

\end{fulllineitems}

\index{dim\_input (kerch.level.KPCA property)@\spxentry{dim\_input}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{dim\_output (kerch.level.KPCA property)@\spxentry{dim\_output}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dim_output}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_output}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Output dimension.

\end{fulllineitems}

\index{draw\_h() (kerch.level.KPCA method)@\spxentry{draw\_h()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.draw_h}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_h}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a \(h^\star\) normally.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of \(h^\star\) to be sampled, defaults to 1.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Latent representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_k() (kerch.level.KPCA method)@\spxentry{draw\_k()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.draw_k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_k}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{posterior}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a dual representation k given its posterior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{posterior}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether phi has to be drawn from its posterior distribution or its conditional
given the prior of h. Defaults to True.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of k to be sampled, defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Dual representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, num\_idx{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_phi() (kerch.level.KPCA method)@\spxentry{draw\_phi()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.draw_phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_phi}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{posterior}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a primal representation phi given its posterior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{posterior}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether phi has to be drawn from its posterior distribution or its conditional
given the prior of h. Defaults to True.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of phi to be sampled, defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Primal representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{dual\_correlation (kerch.level.KPCA property)@\spxentry{dual\_correlation}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dual_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the hidden variables \(\mathbf{h}^\top \mathbf{h}\). This should be the identity provided
that the hidden variables lie on the Stiefel manifold.

\end{fulllineitems}

\index{dual\_param (kerch.level.KPCA property)@\spxentry{dual\_param}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dual_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dual parameter of size {[}num\_idx, dim\_output{]}.

\end{fulllineitems}

\index{dual\_projector (kerch.level.KPCA property)@\spxentry{dual\_projector}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dual_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the hidden variables \(\mathbf{h}\mathbf{h}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the hidden variables lie on the
Stiefel manifold.

\end{fulllineitems}

\index{dual\_trainable (kerch.level.KPCA property)@\spxentry{dual\_trainable}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.dual_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns whether the hidden variables are trainable (a gradient can be computed on it).

\end{fulllineitems}

\index{empty\_sample (kerch.level.KPCA property)@\spxentry{empty\_sample}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{eta (kerch.level.KPCA property)@\spxentry{eta}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.eta}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{eta}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Level weight \(\eta\), e.g. for the weight of the loss.

\end{fulllineitems}

\index{explicit (kerch.level.KPCA property)@\spxentry{explicit}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.level.KPCA method)@\spxentry{explicit\_preimage()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.level.KPCA method)@\spxentry{forward()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{h() (kerch.level.KPCA method)@\spxentry{h()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.h}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{h}}}{\emph{\DUrole{n}{phi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{k}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a \sphinxtitleref{h} given the maximum a posteriori of the distribution. By choosing the input, you either
choose a primal or dual representation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Primal representation.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dual representation.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
MAP of h given phi or k.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.level.KPCA property)@\spxentry{hparams\_fixed}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.level.KPCA property)@\spxentry{hparams\_variable}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.level.KPCA property)@\spxentry{idx}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.level.KPCA method)@\spxentry{implicit\_preimage()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_parameters() (kerch.level.KPCA method)@\spxentry{init\_parameters()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.init_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_parameters}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{overwrite}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the model parameters: the weight in primal and the hidden values in dual.
This is suitable for gradient\sphinxhyphen{}based training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overwrite}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Does not initialize already initialized parameters if False., defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.level.KPCA method)@\spxentry{init\_sample()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.level.KPCA method)@\spxentry{k()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{k\_map() (kerch.level.KPCA method)@\spxentry{k\_map()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.k_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k\_map}}}{\emph{\DUrole{n}{h}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
RKHS representation \(k(x^\star,\mathtt{sample})\) given a latent representation \(h^\star\).
\begin{equation*}
\begin{split}k(x^\star, x_j) = KH^\toph^\star,\end{split}
\end{equation*}
\sphinxAtStartPar
with \(K\) the kernel matrix on the sample \sphinxcode{\sphinxupquote{self.K}} and \(H\) the hidden vectors \sphinxcode{\sphinxupquote{self.hidden}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{h}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_output}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Latent representation \(h^\star\).

\item[{Returns}] \leavevmode
\sphinxAtStartPar
RKHS representation \(k(x^\star,\mathtt{sample})\).

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, num\_idx{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{kappa (kerch.level.KPCA property)@\spxentry{kappa}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.kappa}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kappa}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel (kerch.level.KPCA property)@\spxentry{kernel}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.kernel}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{kerch.kernel.kernel.Kernel}}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel\_transform (kerch.level.KPCA property)@\spxentry{kernel\_transform}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{level\_trainable (kerch.level.KPCA property)@\spxentry{level\_trainable}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.level_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{level\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{loss() (kerch.level.KPCA method)@\spxentry{loss()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.loss}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Reconstruction error on the sample.

\end{fulllineitems}

\index{losses() (kerch.level.KPCA method)@\spxentry{losses()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.losses}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{losses}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}
\pysigstopsignatures
\sphinxAtStartPar
Different components of the losses.

\end{fulllineitems}

\index{model\_variance() (kerch.level.KPCA method)@\spxentry{model\_variance()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.model_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{model\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Total variance learnt by the model given by the sum of the eigenvalues.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this value to strictly be interpreted as a variance, the corresponding kernel (or feature map)
has to be normalized. We refer to the remark of \sphinxcode{\sphinxupquote{total\_variance}}.
\end{sphinxadmonition}

\end{fulllineitems}

\index{normalized (kerch.level.KPCA property)@\spxentry{normalized}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.level.KPCA property)@\spxentry{num\_idx}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.level.KPCA property)@\spxentry{num\_sample}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{param\_trainable (kerch.level.KPCA property)@\spxentry{param\_trainable}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.param_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{param\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{phi() (kerch.level.KPCA method)@\spxentry{phi()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{phi\_map() (kerch.level.KPCA method)@\spxentry{phi\_map()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.phi_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi\_map}}}{\emph{\DUrole{n}{h}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Feature representation \(\phi(x^\star)\) given a latent representation \(h^\star\).
\begin{equation*}
\begin{split}\phi(x^\star) = = W h^\star.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{h}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_output}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Latent representation \(h^\star\).

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Feature representation \(\phi(x^\star)\).

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{phiw() (kerch.level.KPCA method)@\spxentry{phiw()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.phiw}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phiw}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Input, defaults to the sample (None).

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
\(\phi(x)^\top W\) or \(k(x)^top H\)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{primal\_correlation (kerch.level.KPCA property)@\spxentry{primal\_correlation}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.primal_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the weights \(\mathbf{w}^\top \mathbf{w}\). This should be the identity provided
that the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{primal\_param (kerch.level.KPCA property)@\spxentry{primal\_param}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.primal_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Primal parameters of size {[}dim\_feature, dim\_output{]}.

\end{fulllineitems}

\index{primal\_projector (kerch.level.KPCA property)@\spxentry{primal\_projector}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.primal_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the weights \(\mathbf{w}\mathbf{w}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{print\_cache() (kerch.level.KPCA method)@\spxentry{print\_cache()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reconstruct() (kerch.level.KPCA method)@\spxentry{reconstruct()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.reconstruct}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reconstruct}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{relative\_variance() (kerch.level.KPCA method)@\spxentry{relative\_variance()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.relative_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{relative\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Relative variance learnt by the model given by \sphinxcode{\sphinxupquote{\textasciigrave{}model\_variance}}/\sphinxcode{\sphinxupquote{total\_variance}}.
This number is always comprised between 0 and 1 and avoids any considerations on normalization.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\end{fulllineitems}

\index{representation (kerch.level.KPCA property)@\spxentry{representation}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.representation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{representation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{requires\_bias (kerch.level.KPCA property)@\spxentry{requires\_bias}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.requires_bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{requires\_bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{reset() (kerch.level.KPCA method)@\spxentry{reset()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.level.KPCA property)@\spxentry{sample}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.level.KPCA property)@\spxentry{sample\_trainable}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.level.KPCA property)@\spxentry{sample\_transform}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{solve() (kerch.level.KPCA method)@\spxentry{solve()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.solve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{target}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Solves the model by decomposing the kernel matrix or the covariance matrix in principal components
(eigendecomposition).

\sphinxAtStartPar
Fits the model according to the input \sphinxcode{\sphinxupquote{sample}} and output \sphinxcode{\sphinxupquote{target}}. Many models have both a primal and
a dual formulation to be fitted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Representation of the model (\sphinxcode{\sphinxupquote{"primal"}} or \sphinxcode{\sphinxupquote{"dual"}})., defaults to \sphinxcode{\sphinxupquote{"dual"}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{sqrt\_vals (kerch.level.KPCA property)@\spxentry{sqrt\_vals}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.sqrt_vals}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sqrt\_vals}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{stochastic() (kerch.level.KPCA method)@\spxentry{stochastic()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{target (kerch.level.KPCA property)@\spxentry{target}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
target to be matched to.

\end{fulllineitems}

\index{total\_variance() (kerch.level.KPCA method)@\spxentry{total\_variance()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.total_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{total\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Total variance contained in the feature map. In primal formulation,
this is given by \(\DeclareMathOperator{\tr}{tr}\tr(C)\), where \(C = \sum\phi(x)\phi(x)^\top\) is
the covariance matrix on the sample. In dual, this is given by \(\DeclareMathOperator{\tr}{tr}\tr(K)\),
where \(K_{ij} = k(x_i,x_j)\) is the kernel matrix on the sample.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this value to strictly be interpreted as a variance, the corresponding kernel (or feature map)
has to be normalized. In that case however, the total variance will amount to the dimension of the feature
map in primal and the number of datapoints in dual.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.level.KPCA method)@\spxentry{train()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.level.KPCA method)@\spxentry{transform\_input()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.level.KPCA method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_dual() (kerch.level.KPCA method)@\spxentry{update\_dual()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.update_dual}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_dual}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{update\_sample() (kerch.level.KPCA method)@\spxentry{update\_sample()}\spxextra{kerch.level.KPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{vals (kerch.level.KPCA property)@\spxentry{vals}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.vals}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{vals}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Eigenvalues of the model. The model has to be fitted for these values to exist.

\end{fulllineitems}

\index{watch\_properties (kerch.level.KPCA property)@\spxentry{watch\_properties}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.watch_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watch\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Properties to be watched (monitored). Their values can be accessed through the property \sphinxtitleref{watched\_properties}.
This is relevant e.g. in case of training.

\end{fulllineitems}

\index{watched\_properties (kerch.level.KPCA property)@\spxentry{watched\_properties}\spxextra{kerch.level.KPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/kpca:kerch.level.KPCA.watched_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watched\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
A dictionary containing the values of the properties that are specified in \sphinxtitleref{watch\_properties}.

\end{fulllineitems}


\end{fulllineitems}



\subsubsection{Inheritance Diagram}
\label{\detokenize{level/kpca:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-f3680bb99a08de13e084b033edcf1651cdfd5dc4.pdf}

\sphinxstepscope


\subsection{Probabilistic Principal Component Analysis}
\label{\detokenize{level/ppca:probabilistic-principal-component-analysis}}\label{\detokenize{level/ppca::doc}}\index{PPCA (class in kerch.level)@\spxentry{PPCA}\spxextra{class in kerch.level}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.level.}}\sphinxbfcode{\sphinxupquote{PPCA}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.level.\_PPCA.\_PPCA}}, \sphinxcode{\sphinxupquote{kerch.level.single\_view.Level.Level}}
\index{C (kerch.level.PPCA property)@\spxentry{C}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.level.PPCA property)@\spxentry{Corr}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.level.PPCA property)@\spxentry{Cov}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{H (kerch.level.PPCA property)@\spxentry{H}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.H}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{H}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{K (kerch.level.PPCA property)@\spxentry{K}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{level/ppca:kerch.level.PPCA.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.level.PPCA property)@\spxentry{Phi}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{level/ppca:kerch.level.PPCA.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{PhiW (kerch.level.PPCA property)@\spxentry{PhiW}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.PhiW}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{PhiW}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{W (kerch.level.PPCA property)@\spxentry{W}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.W}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{W}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{after\_step() (kerch.level.PPCA method)@\spxentry{after\_step()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs after\sphinxhyphen{}step operations, for example a transform of the parameters onto some manifold.

\end{fulllineitems}

\index{attach\_to() (kerch.level.PPCA method)@\spxentry{attach\_to()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.attach_to}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attach\_to}}}{\emph{\DUrole{n}{weight\_fn}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{attached (kerch.level.PPCA property)@\spxentry{attached}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.attached}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{attached}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the view is attached to another multi\_view.

\end{fulllineitems}

\index{before\_step() (kerch.level.PPCA method)@\spxentry{before\_step()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.before_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{before\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs steps required before each training step.

\end{fulllineitems}

\index{bias (kerch.level.PPCA property)@\spxentry{bias}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{bias\_trainable (kerch.level.PPCA property)@\spxentry{bias\_trainable}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.bias_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{c() (kerch.level.PPCA method)@\spxentry{c()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.level.PPCA method)@\spxentry{cache\_keys()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.level.PPCA property)@\spxentry{cache\_level}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.level.PPCA property)@\spxentry{centered}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.level.PPCA method)@\spxentry{corr()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.level.PPCA method)@\spxentry{cov()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.level.PPCA property)@\spxentry{current\_sample}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.level.PPCA property)@\spxentry{current\_sample\_projected}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{current\_target (kerch.level.PPCA property)@\spxentry{current\_target}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.current_target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the target that are currently used in the computations, taking the stochastic aspect into account
if relevant.

\end{fulllineitems}

\index{detach() (kerch.level.PPCA method)@\spxentry{detach()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.detach}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{detach}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{dim\_feature (kerch.level.PPCA property)@\spxentry{dim\_feature}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map if relevant.

\end{fulllineitems}

\index{dim\_input (kerch.level.PPCA property)@\spxentry{dim\_input}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{dim\_output (kerch.level.PPCA property)@\spxentry{dim\_output}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dim_output}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_output}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Output dimension.

\end{fulllineitems}

\index{draw\_h() (kerch.level.PPCA method)@\spxentry{draw\_h()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.draw_h}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_h}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a h given its prior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of h to be sampled, defaults to 1.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Latent representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_k() (kerch.level.PPCA method)@\spxentry{draw\_k()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.draw_k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_k}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{posterior}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a dual representation k given its posterior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{posterior}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether phi has to be drawn from its posterior distribution or its conditional
given the prior of h. Defaults to True.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of k to be sampled, defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Dual representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, num\_idx{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_phi() (kerch.level.PPCA method)@\spxentry{draw\_phi()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.draw_phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_phi}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{posterior}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a primal representation phi given its posterior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{posterior}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether phi has to be drawn from its posterior distribution or its conditional
given the prior of h. Defaults to True.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of phi to be sampled, defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Primal representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{dual\_correlation (kerch.level.PPCA property)@\spxentry{dual\_correlation}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dual_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the hidden variables \(\mathbf{h}^\top \mathbf{h}\). This should be the identity provided
that the hidden variables lie on the Stiefel manifold.

\end{fulllineitems}

\index{dual\_param (kerch.level.PPCA property)@\spxentry{dual\_param}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dual_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dual parameter of size {[}num\_idx, dim\_output{]}.

\end{fulllineitems}

\index{dual\_projector (kerch.level.PPCA property)@\spxentry{dual\_projector}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dual_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the hidden variables \(\mathbf{h}\mathbf{h}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the hidden variables lie on the
Stiefel manifold.

\end{fulllineitems}

\index{dual\_trainable (kerch.level.PPCA property)@\spxentry{dual\_trainable}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.dual_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns whether the hidden variables are trainable (a gradient can be computed on it).

\end{fulllineitems}

\index{empty\_sample (kerch.level.PPCA property)@\spxentry{empty\_sample}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{eta (kerch.level.PPCA property)@\spxentry{eta}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.eta}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{eta}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Level weight \(\eta\), e.g. for the weight of the loss.

\end{fulllineitems}

\index{explicit (kerch.level.PPCA property)@\spxentry{explicit}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.level.PPCA method)@\spxentry{explicit\_preimage()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{feature\_noise (kerch.level.PPCA property)@\spxentry{feature\_noise}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.feature_noise}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{feature\_noise}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Sigma value, acts as a regularization parameter.

\end{fulllineitems}

\index{forward() (kerch.level.PPCA method)@\spxentry{forward()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{h\_map() (kerch.level.PPCA method)@\spxentry{h\_map()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.h_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{h\_map}}}{\emph{\DUrole{n}{phi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{k}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a \sphinxtitleref{h} given the maximum a posteriori of the distribution. By choosing the input, you either
choose a primal or dual representation.
:param phi: Primal representation.
:param k: Dual representation.
:type phi: Tensor{[}N, dim\_input{]}, optional
:type k: Tensor{[}N, num\_idx{]}, optional
:return: MAP of h given phi or k.
:rtype: Tensor{[}N, dim\_output{]}

\end{fulllineitems}

\index{hparams\_fixed (kerch.level.PPCA property)@\spxentry{hparams\_fixed}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.level.PPCA property)@\spxentry{hparams\_variable}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.level.PPCA property)@\spxentry{idx}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.level.PPCA method)@\spxentry{implicit\_preimage()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_parameters() (kerch.level.PPCA method)@\spxentry{init\_parameters()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.init_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_parameters}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{overwrite}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the model parameters: the weight in primal and the hidden values in dual.
This is suitable for gradient\sphinxhyphen{}based training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overwrite}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Does not initialize already initialized parameters if False., defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.level.PPCA method)@\spxentry{init\_sample()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.level.PPCA method)@\spxentry{k()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{k\_map() (kerch.level.PPCA method)@\spxentry{k\_map()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.k_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k\_map}}}{\emph{\DUrole{n}{h}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Maximum a posteriori of k given h.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{h}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_output}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Latent representation.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
MAP of k given h.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, num\_idx{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{kappa (kerch.level.PPCA property)@\spxentry{kappa}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.kappa}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kappa}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel (kerch.level.PPCA property)@\spxentry{kernel}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.kernel}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{kerch.kernel.kernel.Kernel}}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel\_transform (kerch.level.PPCA property)@\spxentry{kernel\_transform}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{level\_trainable (kerch.level.PPCA property)@\spxentry{level\_trainable}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.level_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{level\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{loss() (kerch.level.PPCA method)@\spxentry{loss()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.loss}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{losses() (kerch.level.PPCA method)@\spxentry{losses()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.losses}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{losses}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}
\pysigstopsignatures
\sphinxAtStartPar
Different components of the losses.

\end{fulllineitems}

\index{model\_variance() (kerch.level.PPCA method)@\spxentry{model\_variance()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.model_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{model\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Total variance learnt by the model given by the sum of the eigenvalues.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this value to strictly be interpreted as a variance, the corresponding kernel (or feature map)
has to be normalized. We refer to the remark of \sphinxcode{\sphinxupquote{total\_variance}}.
\end{sphinxadmonition}

\end{fulllineitems}

\index{mu (kerch.level.PPCA property)@\spxentry{mu}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.mu}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{mu}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{normalized (kerch.level.PPCA property)@\spxentry{normalized}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.level.PPCA property)@\spxentry{num\_idx}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.level.PPCA property)@\spxentry{num\_sample}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{param\_trainable (kerch.level.PPCA property)@\spxentry{param\_trainable}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.param_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{param\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{phi() (kerch.level.PPCA method)@\spxentry{phi()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{phi\_map() (kerch.level.PPCA method)@\spxentry{phi\_map()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.phi_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi\_map}}}{\emph{\DUrole{n}{h}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Maximum a posteriori of phi given h.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{h}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_output}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Latent representation.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
MAP of phi given h.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{phiw() (kerch.level.PPCA method)@\spxentry{phiw()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.phiw}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phiw}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Input, defaults to the sample (None).

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
\(\phi(x)^\top W\) or \(k(x)^top H\)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{primal\_correlation (kerch.level.PPCA property)@\spxentry{primal\_correlation}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.primal_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the weights \(\mathbf{w}^\top \mathbf{w}\). This should be the identity provided
that the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{primal\_param (kerch.level.PPCA property)@\spxentry{primal\_param}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.primal_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Primal parameters of size {[}dim\_feature, dim\_output{]}.

\end{fulllineitems}

\index{primal\_projector (kerch.level.PPCA property)@\spxentry{primal\_projector}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.primal_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the weights \(\mathbf{w}\mathbf{w}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{print\_cache() (kerch.level.PPCA method)@\spxentry{print\_cache()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{relative\_variance() (kerch.level.PPCA method)@\spxentry{relative\_variance()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.relative_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{relative\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Relative variance learnt by the model given by \sphinxcode{\sphinxupquote{\textasciigrave{}model\_variance}}/\sphinxcode{\sphinxupquote{total\_variance}}.
This number is always comprised between 0 and 1 and avoids any considerations on normalization.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\end{fulllineitems}

\index{representation (kerch.level.PPCA property)@\spxentry{representation}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.representation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{representation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{requires\_bias (kerch.level.PPCA property)@\spxentry{requires\_bias}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.requires_bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{requires\_bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{reset() (kerch.level.PPCA method)@\spxentry{reset()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.level.PPCA property)@\spxentry{sample}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.level.PPCA property)@\spxentry{sample\_trainable}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.level.PPCA property)@\spxentry{sample\_transform}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{solve() (kerch.level.PPCA method)@\spxentry{solve()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.solve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{target}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Fits the model according to the input \sphinxcode{\sphinxupquote{sample}} and output \sphinxcode{\sphinxupquote{target}}. Many models have both a primal and
a dual formulation to be fitted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Matrix}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Input sample of the model., defaults to the sample provided by the model.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{target}} (\sphinxstyleliteralemphasis{\sphinxupquote{Matrix}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{vector}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Target sample of the model, defaults to \sphinxcode{\sphinxupquote{\textasciigrave{}None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Representation of the model (\sphinxcode{\sphinxupquote{"primal"}} or \sphinxcode{\sphinxupquote{"dual"}})., defaults to \sphinxcode{\sphinxupquote{"dual"}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sqrt\_vals (kerch.level.PPCA property)@\spxentry{sqrt\_vals}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.sqrt_vals}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sqrt\_vals}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{stochastic() (kerch.level.PPCA method)@\spxentry{stochastic()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{target (kerch.level.PPCA property)@\spxentry{target}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
target to be matched to.

\end{fulllineitems}

\index{total\_variance() (kerch.level.PPCA method)@\spxentry{total\_variance()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.total_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{total\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Total variance contained in the feature map. In primal formulation,
this is given by \(\DeclareMathOperator{\tr}{tr}\tr(C)\), where \(C = \sum\phi(x)\phi(x)^\top\) is
the covariance matrix on the sample. In dual, this is given by \(\DeclareMathOperator{\tr}{tr}\tr(K)\),
where \(K_{ij} = k(x_i,x_j)\) is the kernel matrix on the sample.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this value to strictly be interpreted as a variance, the corresponding kernel (or feature map)
has to be normalized. In that case however, the total variance will amount to the dimension of the feature
map in primal and the number of datapoints in dual.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.level.PPCA method)@\spxentry{train()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.level.PPCA method)@\spxentry{transform\_input()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.level.PPCA method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_dual() (kerch.level.PPCA method)@\spxentry{update\_dual()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.update_dual}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_dual}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{update\_sample() (kerch.level.PPCA method)@\spxentry{update\_sample()}\spxextra{kerch.level.PPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{use\_mean (kerch.level.PPCA property)@\spxentry{use\_mean}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.use_mean}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{use\_mean}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{vals (kerch.level.PPCA property)@\spxentry{vals}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.vals}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{vals}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Eigenvalues of the model. The model has to be fitted for these values to exist.

\end{fulllineitems}

\index{watch\_properties (kerch.level.PPCA property)@\spxentry{watch\_properties}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.watch_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watch\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Properties to be watched (monitored). Their values can be accessed through the property \sphinxtitleref{watched\_properties}.
This is relevant e.g. in case of training.

\end{fulllineitems}

\index{watched\_properties (kerch.level.PPCA property)@\spxentry{watched\_properties}\spxextra{kerch.level.PPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ppca:kerch.level.PPCA.watched_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watched\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
A dictionary containing the values of the properties that are specified in \sphinxtitleref{watch\_properties}.

\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\subsection{Least\sphinxhyphen{}Squares Support Vector Machine}
\label{\detokenize{level/lssvm:least-squares-support-vector-machine}}\label{\detokenize{level/lssvm::doc}}\index{LSSVM (class in kerch.level)@\spxentry{LSSVM}\spxextra{class in kerch.level}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.level.}}\sphinxbfcode{\sphinxupquote{LSSVM}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.level.single\_view.Level.Level}}

\sphinxAtStartPar
Least squares support vector machine.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{gamma}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Regularization parameter of the LSSVM. Defaults to 1.

\end{description}\end{quote}
\index{C (kerch.level.LSSVM property)@\spxentry{C}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.level.LSSVM property)@\spxentry{Corr}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.level.LSSVM property)@\spxentry{Cov}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{H (kerch.level.LSSVM property)@\spxentry{H}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.H}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{H}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{K (kerch.level.LSSVM property)@\spxentry{K}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{level/lssvm:kerch.level.LSSVM.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.level.LSSVM property)@\spxentry{Phi}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{level/lssvm:kerch.level.LSSVM.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{PhiW (kerch.level.LSSVM property)@\spxentry{PhiW}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.PhiW}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{PhiW}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{W (kerch.level.LSSVM property)@\spxentry{W}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.W}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{W}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{after\_step() (kerch.level.LSSVM method)@\spxentry{after\_step()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs after\sphinxhyphen{}step operations, for example a transform of the parameters onto some manifold.

\end{fulllineitems}

\index{attach\_to() (kerch.level.LSSVM method)@\spxentry{attach\_to()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.attach_to}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attach\_to}}}{\emph{\DUrole{n}{weight\_fn}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{attached (kerch.level.LSSVM property)@\spxentry{attached}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.attached}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{attached}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the view is attached to another multi\_view.

\end{fulllineitems}

\index{before\_step() (kerch.level.LSSVM method)@\spxentry{before\_step()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.before_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{before\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs steps required before each training step.

\end{fulllineitems}

\index{bias (kerch.level.LSSVM property)@\spxentry{bias}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{bias\_trainable (kerch.level.LSSVM property)@\spxentry{bias\_trainable}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.bias_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{c() (kerch.level.LSSVM method)@\spxentry{c()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.level.LSSVM method)@\spxentry{cache\_keys()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.level.LSSVM property)@\spxentry{cache\_level}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.level.LSSVM property)@\spxentry{centered}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.level.LSSVM method)@\spxentry{corr()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.level.LSSVM method)@\spxentry{cov()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.level.LSSVM property)@\spxentry{current\_sample}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.level.LSSVM property)@\spxentry{current\_sample\_projected}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{current\_target (kerch.level.LSSVM property)@\spxentry{current\_target}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.current_target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the target that are currently used in the computations, taking the stochastic aspect into account
if relevant.

\end{fulllineitems}

\index{detach() (kerch.level.LSSVM method)@\spxentry{detach()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.detach}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{detach}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{dim\_feature (kerch.level.LSSVM property)@\spxentry{dim\_feature}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map if relevant.

\end{fulllineitems}

\index{dim\_input (kerch.level.LSSVM property)@\spxentry{dim\_input}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{dim\_output (kerch.level.LSSVM property)@\spxentry{dim\_output}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dim_output}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_output}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Output dimension.

\end{fulllineitems}

\index{dual\_correlation (kerch.level.LSSVM property)@\spxentry{dual\_correlation}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dual_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the hidden variables \(\mathbf{h}^\top \mathbf{h}\). This should be the identity provided
that the hidden variables lie on the Stiefel manifold.

\end{fulllineitems}

\index{dual\_param (kerch.level.LSSVM property)@\spxentry{dual\_param}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dual_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dual parameter of size {[}num\_idx, dim\_output{]}.

\end{fulllineitems}

\index{dual\_projector (kerch.level.LSSVM property)@\spxentry{dual\_projector}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dual_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the hidden variables \(\mathbf{h}\mathbf{h}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the hidden variables lie on the
Stiefel manifold.

\end{fulllineitems}

\index{dual\_trainable (kerch.level.LSSVM property)@\spxentry{dual\_trainable}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.dual_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns whether the hidden variables are trainable (a gradient can be computed on it).

\end{fulllineitems}

\index{empty\_sample (kerch.level.LSSVM property)@\spxentry{empty\_sample}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{eta (kerch.level.LSSVM property)@\spxentry{eta}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.eta}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{eta}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Level weight \(\eta\), e.g. for the weight of the loss.

\end{fulllineitems}

\index{explicit (kerch.level.LSSVM property)@\spxentry{explicit}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.level.LSSVM method)@\spxentry{explicit\_preimage()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.level.LSSVM method)@\spxentry{forward()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{gamma (kerch.level.LSSVM property)@\spxentry{gamma}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.gamma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{gamma}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{hparams\_fixed (kerch.level.LSSVM property)@\spxentry{hparams\_fixed}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.level.LSSVM property)@\spxentry{hparams\_variable}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.level.LSSVM property)@\spxentry{idx}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.level.LSSVM method)@\spxentry{implicit\_preimage()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_parameters() (kerch.level.LSSVM method)@\spxentry{init\_parameters()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.init_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_parameters}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{overwrite}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the model parameters: the weight in primal and the hidden values in dual.
This is suitable for gradient\sphinxhyphen{}based training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overwrite}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Does not initialize already initialized parameters if False., defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.level.LSSVM method)@\spxentry{init\_sample()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.level.LSSVM method)@\spxentry{k()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kappa (kerch.level.LSSVM property)@\spxentry{kappa}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.kappa}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kappa}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel (kerch.level.LSSVM property)@\spxentry{kernel}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.kernel}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{kerch.kernel.kernel.Kernel}}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel\_transform (kerch.level.LSSVM property)@\spxentry{kernel\_transform}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{level\_trainable (kerch.level.LSSVM property)@\spxentry{level\_trainable}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.level_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{level\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{loss() (kerch.level.LSSVM method)@\spxentry{loss()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.loss}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{losses() (kerch.level.LSSVM method)@\spxentry{losses()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.losses}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{losses}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}
\pysigstopsignatures
\sphinxAtStartPar
Different components of the losses.

\end{fulllineitems}

\index{normalized (kerch.level.LSSVM property)@\spxentry{normalized}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.level.LSSVM property)@\spxentry{num\_idx}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.level.LSSVM property)@\spxentry{num\_sample}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{param\_trainable (kerch.level.LSSVM property)@\spxentry{param\_trainable}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.param_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{param\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{phi() (kerch.level.LSSVM method)@\spxentry{phi()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{phiw() (kerch.level.LSSVM method)@\spxentry{phiw()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.phiw}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phiw}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Input, defaults to the sample (None).

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
\(\phi(x)^\top W\) or \(k(x)^top H\)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{primal\_correlation (kerch.level.LSSVM property)@\spxentry{primal\_correlation}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.primal_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the weights \(\mathbf{w}^\top \mathbf{w}\). This should be the identity provided
that the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{primal\_param (kerch.level.LSSVM property)@\spxentry{primal\_param}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.primal_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Primal parameters of size {[}dim\_feature, dim\_output{]}.

\end{fulllineitems}

\index{primal\_projector (kerch.level.LSSVM property)@\spxentry{primal\_projector}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.primal_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the weights \(\mathbf{w}\mathbf{w}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{print\_cache() (kerch.level.LSSVM method)@\spxentry{print\_cache()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{representation (kerch.level.LSSVM property)@\spxentry{representation}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.representation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{representation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{requires\_bias (kerch.level.LSSVM property)@\spxentry{requires\_bias}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.requires_bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{requires\_bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{reset() (kerch.level.LSSVM method)@\spxentry{reset()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.level.LSSVM property)@\spxentry{sample}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.level.LSSVM property)@\spxentry{sample\_trainable}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.level.LSSVM property)@\spxentry{sample\_transform}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{solve() (kerch.level.LSSVM method)@\spxentry{solve()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.solve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{target}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Fits the model according to the input \sphinxcode{\sphinxupquote{sample}} and output \sphinxcode{\sphinxupquote{target}}. Many models have both a primal and
a dual formulation to be fitted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Matrix}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Input sample of the model., defaults to the sample provided by the model.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{target}} (\sphinxstyleliteralemphasis{\sphinxupquote{Matrix}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{vector}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Target sample of the model, defaults to \sphinxcode{\sphinxupquote{\textasciigrave{}None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Representation of the model (\sphinxcode{\sphinxupquote{"primal"}} or \sphinxcode{\sphinxupquote{"dual"}})., defaults to \sphinxcode{\sphinxupquote{"dual"}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{stochastic() (kerch.level.LSSVM method)@\spxentry{stochastic()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{target (kerch.level.LSSVM property)@\spxentry{target}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
target to be matched to.

\end{fulllineitems}

\index{train() (kerch.level.LSSVM method)@\spxentry{train()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.level.LSSVM method)@\spxentry{transform\_input()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.level.LSSVM method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_dual() (kerch.level.LSSVM method)@\spxentry{update\_dual()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.update_dual}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_dual}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{update\_sample() (kerch.level.LSSVM method)@\spxentry{update\_sample()}\spxextra{kerch.level.LSSVM method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{watch\_properties (kerch.level.LSSVM property)@\spxentry{watch\_properties}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.watch_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watch\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Properties to be watched (monitored). Their values can be accessed through the property \sphinxtitleref{watched\_properties}.
This is relevant e.g. in case of training.

\end{fulllineitems}

\index{watched\_properties (kerch.level.LSSVM property)@\spxentry{watched\_properties}\spxextra{kerch.level.LSSVM property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/lssvm:kerch.level.LSSVM.watched_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watched\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
A dictionary containing the values of the properties that are specified in \sphinxtitleref{watch\_properties}.

\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\subsection{Ridge Regression}
\label{\detokenize{level/ridge:ridge-regression}}\label{\detokenize{level/ridge::doc}}\index{Ridge (class in kerch.level)@\spxentry{Ridge}\spxextra{class in kerch.level}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.level.}}\sphinxbfcode{\sphinxupquote{Ridge}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{level/lssvm:kerch.level.LSSVM}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.level.single\_view.LSSVM.LSSVM}}}}}
\index{C (kerch.level.Ridge property)@\spxentry{C}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit matrix on the sample datapoints.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_idx}}\sum_i^\texttt{num_idx} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}
\end{fulllineitems}

\index{Corr (kerch.level.Ridge property)@\spxentry{Corr}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.Corr}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Corr}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix of the sample. Same as calling self.corr().

\end{fulllineitems}

\index{Cov (kerch.level.Ridge property)@\spxentry{Cov}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.Cov}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Cov}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix of the sample. Same as calling self.cov().

\end{fulllineitems}

\index{H (kerch.level.Ridge property)@\spxentry{H}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.H}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{H}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{K (kerch.level.Ridge property)@\spxentry{K}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the kernel matrix on the sample data. Same result as calling {\hyperref[\detokenize{level/ridge:kerch.level.Ridge.k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{k()}}}}}, but faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.
\begin{equation*}
\begin{split}K_{ij} = k(x_i,x_j).\end{split}
\end{equation*}
\end{fulllineitems}

\index{Phi (kerch.level.Ridge property)@\spxentry{Phi}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(\cdot)\) of the sample datapoints. Same as calling
{\hyperref[\detokenize{level/ridge:kerch.level.Ridge.phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{phi()}}}}}, but slightly faster.
It is loaded from memory if already computed and unchanged since then, to avoid re\sphinxhyphen{}computation when recurrently
called.

\end{fulllineitems}

\index{PhiW (kerch.level.Ridge property)@\spxentry{PhiW}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.PhiW}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{PhiW}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{W (kerch.level.Ridge property)@\spxentry{W}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.W}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{W}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{after\_step() (kerch.level.Ridge method)@\spxentry{after\_step()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs after\sphinxhyphen{}step operations, for example a transform of the parameters onto some manifold.

\end{fulllineitems}

\index{attach\_to() (kerch.level.Ridge method)@\spxentry{attach\_to()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.attach_to}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attach\_to}}}{\emph{\DUrole{n}{weight\_fn}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{attached (kerch.level.Ridge property)@\spxentry{attached}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.attached}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{attached}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the view is attached to another multi\_view.

\end{fulllineitems}

\index{before\_step() (kerch.level.Ridge method)@\spxentry{before\_step()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.before_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{before\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs steps required before each training step.

\end{fulllineitems}

\index{bias (kerch.level.Ridge property)@\spxentry{bias}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{bias\_trainable (kerch.level.Ridge property)@\spxentry{bias\_trainable}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.bias_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{bias\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{c() (kerch.level.Ridge method)@\spxentry{c()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample explicit matrix.
\begin{equation*}
\begin{split}C = \frac{1}{\texttt{num_x}}\sum_{i}^{\texttt{num_x}} \phi(x_i)\phi(x_i)^\top.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.level.Ridge method)@\spxentry{cache\_keys()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.level.Ridge property)@\spxentry{cache\_level}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{centered (kerch.level.Ridge property)@\spxentry{centered}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.centered}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{centered}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{corr() (kerch.level.Ridge method)@\spxentry{corr()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.corr}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{corr}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the correlation matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used., defaults to \sphinxtitleref{None}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Correlation matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{cov() (kerch.level.Ridge method)@\spxentry{cov()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.cov}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cov}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the covariance matrix fo the provided input.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used.
Defaults to \sphinxtitleref{None}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Covariance matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}dim\_feature, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{current\_sample (kerch.level.Ridge property)@\spxentry{current\_sample}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.level.Ridge property)@\spxentry{current\_sample\_projected}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{current\_target (kerch.level.Ridge property)@\spxentry{current\_target}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.current_target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the target that are currently used in the computations, taking the stochastic aspect into account
if relevant.

\end{fulllineitems}

\index{detach() (kerch.level.Ridge method)@\spxentry{detach()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.detach}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{detach}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{dim\_feature (kerch.level.Ridge property)@\spxentry{dim\_feature}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map if relevant.

\end{fulllineitems}

\index{dim\_input (kerch.level.Ridge property)@\spxentry{dim\_input}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{dim\_output (kerch.level.Ridge property)@\spxentry{dim\_output}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dim_output}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_output}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Output dimension.

\end{fulllineitems}

\index{dual\_correlation (kerch.level.Ridge property)@\spxentry{dual\_correlation}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dual_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the hidden variables \(\mathbf{h}^\top \mathbf{h}\). This should be the identity provided
that the hidden variables lie on the Stiefel manifold.

\end{fulllineitems}

\index{dual\_param (kerch.level.Ridge property)@\spxentry{dual\_param}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dual_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dual parameter of size {[}num\_idx, dim\_output{]}.

\end{fulllineitems}

\index{dual\_projector (kerch.level.Ridge property)@\spxentry{dual\_projector}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dual_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the hidden variables \(\mathbf{h}\mathbf{h}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the hidden variables lie on the
Stiefel manifold.

\end{fulllineitems}

\index{dual\_trainable (kerch.level.Ridge property)@\spxentry{dual\_trainable}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.dual_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns whether the hidden variables are trainable (a gradient can be computed on it).

\end{fulllineitems}

\index{empty\_sample (kerch.level.Ridge property)@\spxentry{empty\_sample}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{eta (kerch.level.Ridge property)@\spxentry{eta}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.eta}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{eta}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Level weight \(\eta\), e.g. for the weight of the loss.

\end{fulllineitems}

\index{explicit (kerch.level.Ridge property)@\spxentry{explicit}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
True if the method has an explicit formulation, False otherwise.

\end{fulllineitems}

\index{explicit\_preimage() (kerch.level.Ridge method)@\spxentry{explicit\_preimage()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.explicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{explicit\_preimage}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}explicit\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of an explicit feature map of the kernel, given by \sphinxcode{\sphinxupquote{phi\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}: Uses an explicit implementation specific to the kernel (if available). This is always preferable if available.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_phi()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Explicit feature map image to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the explicit feature map on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}explicit\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{forward() (kerch.level.Ridge method)@\spxentry{forward()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Passes datapoints through the kernel.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{,}}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}) \textendash{} Datapoints to be passed through the kernel.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Chosen representation. If \sphinxtitleref{dual}, an out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix is returned. If
\sphinxtitleref{primal} is specified, it returns the explicit feature map., defaults to \sphinxtitleref{dual}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix or explicit feature map depending on \sphinxtitleref{representation}.

\item[{Raises}] \leavevmode
\sphinxAtStartPar
RepresentationError

\end{description}\end{quote}

\end{fulllineitems}

\index{gamma (kerch.level.Ridge property)@\spxentry{gamma}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.gamma}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{gamma}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{hparams\_fixed (kerch.level.Ridge property)@\spxentry{hparams\_fixed}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the hyper\sphinxhyphen{}parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{hparams\_variable (kerch.level.Ridge property)@\spxentry{hparams\_variable}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dictionnary containing the parameters and their values. This can be relevant for monitoring.

\end{fulllineitems}

\index{idx (kerch.level.Ridge property)@\spxentry{idx}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{implicit\_preimage() (kerch.level.Ridge method)@\spxentry{implicit\_preimage()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.implicit_preimage}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{implicit\_preimage}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{method}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}knn\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Computes a pre\sphinxhyphen{}image of coefficients in the RKHS of the kernel, given by \sphinxcode{\sphinxupquote{k\_image}}.
Different methods are available:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}: Nearest neighbors. We refer to {\hyperref[\detokenize{methods/knn:kerch.method.knn}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.knn()}}}}} for more details.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}smoother\textquotesingle{}}}: Kernel smoothing. We refer to {\hyperref[\detokenize{methods/smoother:kerch.method.smoother}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.smoother()}}}}} for more details

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}iterative\textquotesingle{}}}: Iterative optimization. We refer to {\hyperref[\detokenize{methods/iterative:kerch.method.iterative_preimage_k}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.method.iterative\_preimage\_k()}}}}} for more details

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} RKHS coefficients to be inverted. If not specified (\sphinxcode{\sphinxupquote{None}}), the kernel matrix on the sample is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{method}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Pre\sphinxhyphen{}image method to be used. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}knn\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Additional parameters of the pre\sphinxhyphen{}image method used. Please refer to its documentation for
further details.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_parameters() (kerch.level.Ridge method)@\spxentry{init\_parameters()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.init_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_parameters}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{overwrite}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the model parameters: the weight in primal and the hidden values in dual.
This is suitable for gradient\sphinxhyphen{}based training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overwrite}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Does not initialize already initialized parameters if False., defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{init\_sample() (kerch.level.Ridge method)@\spxentry{init\_sample()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.level.Ridge method)@\spxentry{k()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{explicit}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a kernel matrix, either of the sample, either out\sphinxhyphen{}of\sphinxhyphen{}sample, either fully out\sphinxhyphen{}of\sphinxhyphen{}sample.
\begin{equation*}
\begin{split}K = [k(x_i,y_j)]_{i,j=1}^{\texttt{num_x}, \texttt{num_y}},\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\{x_i\}_{i=1}^\texttt{num_x}\) the out\sphinxhyphen{}of\sphinxhyphen{}sample points (\sphinxtitleref{x}) and \(\{y_i\}_{j=1}^\texttt{num_y}\) the sample points
(\sphinxtitleref{y}).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In the case of centered kernels on an out\sphinxhyphen{}of\sphinxhyphen{}sample, this computation is more expensive as it requires to center according to
the sample data, which implies computing a statistic on the out\sphinxhyphen{}of\sphinxhyphen{}sample kernel matrix and thus
also computing it.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (first dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_y}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample points (second dimension). If \sphinxtitleref{None}, the default sample will be used. Defaults to \sphinxcode{\sphinxupquote{None}}

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Kernel matrix

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, num\_y{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{kappa (kerch.level.Ridge property)@\spxentry{kappa}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.kappa}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kappa}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel (kerch.level.Ridge property)@\spxentry{kernel}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.kernel}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{kerch.kernel.kernel.Kernel}}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{kernel\_transform (kerch.level.Ridge property)@\spxentry{kernel\_transform}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.kernel_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{kernel\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform performed on the kernel

\end{fulllineitems}

\index{level\_trainable (kerch.level.Ridge property)@\spxentry{level\_trainable}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.level_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{level\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{loss() (kerch.level.Ridge method)@\spxentry{loss()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.loss}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{losses() (kerch.level.Ridge method)@\spxentry{losses()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.losses}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{losses}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}
\pysigstopsignatures
\sphinxAtStartPar
Different components of the losses.

\end{fulllineitems}

\index{normalized (kerch.level.Ridge property)@\spxentry{normalized}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.normalized}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{normalized}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indicates whether the feature map is centered relative to its sample or equivalently is the kernel in centered
in its RKHS space, spanned by the sample.

\end{fulllineitems}

\index{num\_idx (kerch.level.Ridge property)@\spxentry{num\_idx}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.level.Ridge property)@\spxentry{num\_sample}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{param\_trainable (kerch.level.Ridge property)@\spxentry{param\_trainable}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.param_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{param\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{phi() (kerch.level.Ridge method)@\spxentry{phi()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the explicit feature map \(\phi(x)\) of the specified points.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_x}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} The datapoints serving as input of the explicit feature map. If \sphinxtitleref{None}, the sample will be used.
Defaults to \sphinxcode{\sphinxupquote{None}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Explicit feature map \(\phi(x)\) of the specified points.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num\_x, dim\_feature{]}

\item[{Raises}] \leavevmode
\sphinxAtStartPar
ExplicitError

\end{description}\end{quote}

\end{fulllineitems}

\index{phiw() (kerch.level.Ridge method)@\spxentry{phiw()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.phiw}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phiw}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Input, defaults to the sample (None).

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
\(\phi(x)^\top W\) or \(k(x)^top H\)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{primal\_correlation (kerch.level.Ridge property)@\spxentry{primal\_correlation}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.primal_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the weights \(\mathbf{w}^\top \mathbf{w}\). This should be the identity provided
that the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{primal\_param (kerch.level.Ridge property)@\spxentry{primal\_param}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.primal_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Primal parameters of size {[}dim\_feature, dim\_output{]}.

\end{fulllineitems}

\index{primal\_projector (kerch.level.Ridge property)@\spxentry{primal\_projector}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.primal_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the weights \(\mathbf{w}\mathbf{w}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{print\_cache() (kerch.level.Ridge method)@\spxentry{print\_cache()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{representation (kerch.level.Ridge property)@\spxentry{representation}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.representation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{representation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{requires\_bias (kerch.level.Ridge property)@\spxentry{requires\_bias}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.requires_bias}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{requires\_bias}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{reset() (kerch.level.Ridge method)@\spxentry{reset()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.level.Ridge property)@\spxentry{sample}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.level.Ridge property)@\spxentry{sample\_trainable}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.level.Ridge property)@\spxentry{sample\_transform}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{solve() (kerch.level.Ridge method)@\spxentry{solve()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.solve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{target}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Fits the model according to the input \sphinxcode{\sphinxupquote{sample}} and output \sphinxcode{\sphinxupquote{target}}. Many models have both a primal and
a dual formulation to be fitted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Matrix}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Input sample of the model., defaults to the sample provided by the model.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{target}} (\sphinxstyleliteralemphasis{\sphinxupquote{Matrix}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{vector}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Target sample of the model, defaults to \sphinxcode{\sphinxupquote{\textasciigrave{}None}}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Representation of the model (\sphinxcode{\sphinxupquote{"primal"}} or \sphinxcode{\sphinxupquote{"dual"}})., defaults to \sphinxcode{\sphinxupquote{"dual"}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{stochastic() (kerch.level.Ridge method)@\spxentry{stochastic()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{target (kerch.level.Ridge property)@\spxentry{target}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.target}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{target}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
target to be matched to.

\end{fulllineitems}

\index{train() (kerch.level.Ridge method)@\spxentry{train()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.level.Ridge method)@\spxentry{transform\_input()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.level.Ridge method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_dual() (kerch.level.Ridge method)@\spxentry{update\_dual()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.update_dual}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_dual}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{update\_sample() (kerch.level.Ridge method)@\spxentry{update\_sample()}\spxextra{kerch.level.Ridge method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{watch\_properties (kerch.level.Ridge property)@\spxentry{watch\_properties}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.watch_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watch\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Properties to be watched (monitored). Their values can be accessed through the property \sphinxtitleref{watched\_properties}.
This is relevant e.g. in case of training.

\end{fulllineitems}

\index{watched\_properties (kerch.level.Ridge property)@\spxentry{watched\_properties}\spxextra{kerch.level.Ridge property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/ridge:kerch.level.Ridge.watched_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watched\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
A dictionary containing the values of the properties that are specified in \sphinxtitleref{watch\_properties}.

\end{fulllineitems}


\end{fulllineitems}



\section{Multi\sphinxhyphen{}View}
\label{\detokenize{level/index:multi-view}}
\sphinxstepscope


\subsection{Multi\sphinxhyphen{}View Kernel Principal Component Analysis}
\label{\detokenize{level/mvkpca:multi-view-kernel-principal-component-analysis}}\label{\detokenize{level/mvkpca::doc}}\index{MVKPCA (class in kerch.level)@\spxentry{MVKPCA}\spxextra{class in kerch.level}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.level.}}\sphinxbfcode{\sphinxupquote{MVKPCA}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.level.\_KPCA.\_KPCA}}, \sphinxcode{\sphinxupquote{kerch.level.multi\_view.MVLevel.MVLevel}}

\sphinxAtStartPar
Multi\sphinxhyphen{}View Kernel Principal Component Analysis.
\index{C (kerch.level.MVKPCA property)@\spxentry{C}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.C}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{C}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{H (kerch.level.MVKPCA property)@\spxentry{H}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.H}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{H}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{K (kerch.level.MVKPCA property)@\spxentry{K}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.K}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{K}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{Ks (kerch.level.MVKPCA property)@\spxentry{Ks}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.Ks}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Ks}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{Phi (kerch.level.MVKPCA property)@\spxentry{Phi}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.Phi}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{Phi}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{PhiW (kerch.level.MVKPCA property)@\spxentry{PhiW}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.PhiW}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{PhiW}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{W (kerch.level.MVKPCA property)@\spxentry{W}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.W}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{W}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{after\_step() (kerch.level.MVKPCA method)@\spxentry{after\_step()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs after\sphinxhyphen{}step operations, for example a transform of the parameters onto some manifold.

\end{fulllineitems}

\index{attach\_to() (kerch.level.MVKPCA method)@\spxentry{attach\_to()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.attach_to}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attach\_to}}}{\emph{\DUrole{n}{weight\_fn}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{attach\_view() (kerch.level.MVKPCA method)@\spxentry{attach\_view()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.attach_view}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{attach\_view}}}{\emph{\DUrole{n}{view}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Adds a view

\end{fulllineitems}

\index{attached (kerch.level.MVKPCA property)@\spxentry{attached}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.attached}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{attached}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean indicating whether the view is attached to another multi\_view.

\end{fulllineitems}

\index{before\_step() (kerch.level.MVKPCA method)@\spxentry{before\_step()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.before_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{before\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Performs steps required before each training step.

\end{fulllineitems}

\index{c() (kerch.level.MVKPCA method)@\spxentry{c()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.c}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{c}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{cache\_keys() (kerch.level.MVKPCA method)@\spxentry{cache\_keys()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.level.MVKPCA property)@\spxentry{cache\_level}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{detach() (kerch.level.MVKPCA method)@\spxentry{detach()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.detach}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{detach}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{detach\_all() (kerch.level.MVKPCA method)@\spxentry{detach\_all()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.detach_all}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{detach\_all}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Detaches all the known.

\end{fulllineitems}

\index{dim\_feature (kerch.level.MVKPCA property)@\spxentry{dim\_feature}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dim_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of the explicit feature map if relevant.

\end{fulllineitems}

\index{dim\_output (kerch.level.MVKPCA property)@\spxentry{dim\_output}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dim_output}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_output}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Output dimension.

\end{fulllineitems}

\index{dims\_feature (kerch.level.MVKPCA property)@\spxentry{dims\_feature}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dims_feature}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dims\_feature}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }List\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
List containing the feature dimension of each view.

\end{fulllineitems}

\index{dims\_feature\_cumulative (kerch.level.MVKPCA property)@\spxentry{dims\_feature\_cumulative}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dims_feature_cumulative}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dims\_feature\_cumulative}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }List\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
List containing the cumulative feature dimensions of each view with the previus ones.

\end{fulllineitems}

\index{draw\_h() (kerch.level.MVKPCA method)@\spxentry{draw\_h()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.draw_h}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_h}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a \(h^\star\) normally.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of \(h^\star\) to be sampled, defaults to 1.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Latent representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_k() (kerch.level.MVKPCA method)@\spxentry{draw\_k()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.draw_k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_k}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{posterior}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a dual representation k given its posterior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{posterior}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether phi has to be drawn from its posterior distribution or its conditional
given the prior of h. Defaults to True.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of k to be sampled, defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Dual representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, num\_idx{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{draw\_phi() (kerch.level.MVKPCA method)@\spxentry{draw\_phi()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.draw_phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{draw\_phi}}}{\emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{posterior}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a primal representation phi given its posterior distribution.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{posterior}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicates whether phi has to be drawn from its posterior distribution or its conditional
given the prior of h. Defaults to True.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of phi to be sampled, defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Primal representation.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}num, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{dual\_correlation (kerch.level.MVKPCA property)@\spxentry{dual\_correlation}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dual_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the hidden variables \(\mathbf{h}^\top \mathbf{h}\). This should be the identity provided
that the hidden variables lie on the Stiefel manifold.

\end{fulllineitems}

\index{dual\_param (kerch.level.MVKPCA property)@\spxentry{dual\_param}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dual_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dual parameter of size {[}num\_idx, dim\_output{]}.

\end{fulllineitems}

\index{dual\_projector (kerch.level.MVKPCA property)@\spxentry{dual\_projector}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dual_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the hidden variables \(\mathbf{h}\mathbf{h}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the hidden variables lie on the
Stiefel manifold.

\end{fulllineitems}

\index{dual\_trainable (kerch.level.MVKPCA property)@\spxentry{dual\_trainable}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.dual_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dual\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns whether the hidden variables are trainable (a gradient can be computed on it).

\end{fulllineitems}

\index{eta (kerch.level.MVKPCA property)@\spxentry{eta}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.eta}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{eta}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}
\pysigstopsignatures
\sphinxAtStartPar
Level weight \(\eta\), e.g. for the weight of the loss.

\end{fulllineitems}

\index{forward() (kerch.level.MVKPCA method)@\spxentry{forward()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Define the computation performed at every call.

\sphinxAtStartPar
Should be overridden by all subclasses.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}

\index{h() (kerch.level.MVKPCA method)@\spxentry{h()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.h}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{h}}}{\emph{\DUrole{n}{phi}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{k}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Draws a \sphinxtitleref{h} given the maximum a posteriori of the distribution. By choosing the input, you either
choose a primal or dual representation.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Primal representation.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dual representation.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
MAP of h given phi or k.

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{hparams\_fixed (kerch.level.MVKPCA property)@\spxentry{hparams\_fixed}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Fixed hyperparameters of the module. By contrast with {\hyperref[\detokenize{level/mvkpca:kerch.level.MVKPCA.hparams_variable}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_variable}}}}}, these are the values that are fixed and
cannot possibly change during the training. If applicable, these can be specific architecture values for example.
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{hparams\_variable (kerch.level.MVKPCA property)@\spxentry{hparams\_variable}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Variable hyperparameters of the module. By contrast with {\hyperref[\detokenize{level/mvkpca:kerch.level.MVKPCA.hparams_fixed}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_fixed}}}}}, these are the values that are may change during
the training and may be monitored at various stages during the training.
If applicable, these can be kernel bandwidth parameters for example.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
All parameters that are potentially trainable, like a kernel bandwidth \(\sigma\) for example, are
included in this dictionary, even if the corresponding trainable argument is set to \sphinxcode{\sphinxupquote{False}}. In the
latter case, they will be not evolve during training iterations, but will still be included in this
dictionary.
\end{sphinxadmonition}

\sphinxAtStartPar
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{idx (kerch.level.MVKPCA property)@\spxentry{idx}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{init\_parameters() (kerch.level.MVKPCA method)@\spxentry{init\_parameters()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.init_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_parameters}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{overwrite}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the model parameters: the weight in primal and the hidden values in dual.
This is suitable for gradient\sphinxhyphen{}based training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{overwrite}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Does not initialize already initialized parameters if False., defaults to True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{k() (kerch.level.MVKPCA method)@\spxentry{k()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{k\_map() (kerch.level.MVKPCA method)@\spxentry{k\_map()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.k_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{k\_map}}}{\emph{\DUrole{n}{h}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
RKHS representation \(k(x^\star,\mathtt{sample})\) given a latent representation \(h^\star\).
\begin{equation*}
\begin{split}k(x^\star, x_j) = KH^\toph^\star,\end{split}
\end{equation*}
\sphinxAtStartPar
with \(K\) the kernel matrix on the sample \sphinxcode{\sphinxupquote{self.K}} and \(H\) the hidden vectors \sphinxcode{\sphinxupquote{self.hidden}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{h}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_output}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Latent representation \(h^\star\).

\item[{Returns}] \leavevmode
\sphinxAtStartPar
RKHS representation \(k(x^\star,\mathtt{sample})\).

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, num\_idx{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{ks() (kerch.level.MVKPCA method)@\spxentry{ks()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.ks}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{ks}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{level\_trainable (kerch.level.MVKPCA property)@\spxentry{level\_trainable}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.level_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{level\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Specifies whether the parameters weight and hidden are trainable or not.

\end{fulllineitems}

\index{loss() (kerch.level.MVKPCA method)@\spxentry{loss()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.loss}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{loss}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Reconstruction error on the sample.

\end{fulllineitems}

\index{losses() (kerch.level.MVKPCA method)@\spxentry{losses()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.losses}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{losses}}}{\emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}
\pysigstopsignatures
\sphinxAtStartPar
Different components of the losses.

\end{fulllineitems}

\index{model\_variance() (kerch.level.MVKPCA method)@\spxentry{model\_variance()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.model_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{model\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Total variance learnt by the model given by the sum of the eigenvalues.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this value to strictly be interpreted as a variance, the corresponding kernel (or feature map)
has to be normalized. We refer to the remark of \sphinxcode{\sphinxupquote{total\_variance}}.
\end{sphinxadmonition}

\end{fulllineitems}

\index{named\_views (kerch.level.MVKPCA property)@\spxentry{named\_views}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.named_views}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{named\_views}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }Iterator\DUrole{p}{{[}}Tuple\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{,}\DUrole{w}{  }kerch.level.single\_view.View.View\DUrole{p}{{]}}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{num\_idx (kerch.level.MVKPCA property)@\spxentry{num\_idx}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_views (kerch.level.MVKPCA property)@\spxentry{num\_views}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.num_views}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_views}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{phi() (kerch.level.MVKPCA method)@\spxentry{phi()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{phi\_map() (kerch.level.MVKPCA method)@\spxentry{phi\_map()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.phi_map}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phi\_map}}}{\emph{\DUrole{n}{h}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Feature representation \(\phi(x^\star)\) given a latent representation \(h^\star\).
\begin{equation*}
\begin{split}\phi(x^\star) = = W h^\star.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{h}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{N}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_output}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Latent representation \(h^\star\).

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Feature representation \(\phi(x^\star)\).

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Tensor{[}N, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{phis() (kerch.level.MVKPCA method)@\spxentry{phis()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.phis}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phis}}}{\emph{\DUrole{n}{x}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{phiw() (kerch.level.MVKPCA method)@\spxentry{phiw()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.phiw}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{phiw}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Input, defaults to the sample (None).

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} ‘primal’ or ‘dual’.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
\(\phi(x)^\top W\) or \(k(x)^top H\)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num, dim\_output{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (kerch.level.MVKPCA method)@\spxentry{predict()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.predict}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{known}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{primal\_correlation (kerch.level.MVKPCA property)@\spxentry{primal\_correlation}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.primal_correlation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_correlation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Correlation of the weights \(\mathbf{w}^\top \mathbf{w}\). This should be the identity provided
that the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{primal\_param (kerch.level.MVKPCA property)@\spxentry{primal\_param}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.primal_param}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_param}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Primal parameters of size {[}dim\_feature, dim\_output{]}.

\end{fulllineitems}

\index{primal\_projector (kerch.level.MVKPCA property)@\spxentry{primal\_projector}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.primal_projector}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{primal\_projector}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Projector on the subspace spanned by the weights \(\mathbf{w}\mathbf{w}^\top\).
This is a rigorous projector provided its determinant is unity, e.g. when the weights lie on the Stiefel manifold.

\end{fulllineitems}

\index{print\_cache() (kerch.level.MVKPCA method)@\spxentry{print\_cache()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{project() (kerch.level.MVKPCA method)@\spxentry{project()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.project}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{project}}}{\emph{\DUrole{n}{known}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}, \emph{\DUrole{n}{representation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{relative\_variance() (kerch.level.MVKPCA method)@\spxentry{relative\_variance()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.relative_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{relative\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Relative variance learnt by the model given by \sphinxcode{\sphinxupquote{\textasciigrave{}model\_variance}}/\sphinxcode{\sphinxupquote{total\_variance}}.
This number is always comprised between 0 and 1 and avoids any considerations on normalization.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\end{fulllineitems}

\index{representation (kerch.level.MVKPCA property)@\spxentry{representation}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.representation}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{representation}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{reset() (kerch.level.MVKPCA method)@\spxentry{reset()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{solve() (kerch.level.MVKPCA method)@\spxentry{solve()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.solve}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{solve}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{target}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Solves the model by decomposing the kernel matrix or the covariance matrix in principal components
(eigendecomposition).

\sphinxAtStartPar
Fits the model according to the input \sphinxcode{\sphinxupquote{sample}} and output \sphinxcode{\sphinxupquote{target}}. Many models have both a primal and
a dual formulation to be fitted.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{representation}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Representation of the model (\sphinxcode{\sphinxupquote{"primal"}} or \sphinxcode{\sphinxupquote{"dual"}})., defaults to \sphinxcode{\sphinxupquote{"dual"}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{sqrt\_vals (kerch.level.MVKPCA property)@\spxentry{sqrt\_vals}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.sqrt_vals}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sqrt\_vals}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{stochastic() (kerch.level.MVKPCA method)@\spxentry{stochastic()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{total\_variance() (kerch.level.MVKPCA method)@\spxentry{total\_variance()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.total_variance}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{total\_variance}}}{\emph{\DUrole{n}{as\_tensor}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{normalize}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{representation}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Total variance contained in the feature map. In primal formulation,
this is given by \(\DeclareMathOperator{\tr}{tr}\tr(C)\), where \(C = \sum\phi(x)\phi(x)^\top\) is
the covariance matrix on the sample. In dual, this is given by \(\DeclareMathOperator{\tr}{tr}\tr(K)\),
where \(K_{ij} = k(x_i,x_j)\) is the kernel matrix on the sample.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{as\_tensor}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indicated whether the variance has to be returned as a float or a torch.Tensor., defaults
to \sphinxcode{\sphinxupquote{False}}

\end{description}\end{quote}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
For this value to strictly be interpreted as a variance, the corresponding kernel (or feature map)
has to be normalized. In that case however, the total variance will amount to the dimension of the feature
map in primal and the number of datapoints in dual.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.level.MVKPCA method)@\spxentry{train()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{update\_dual() (kerch.level.MVKPCA method)@\spxentry{update\_dual()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.update_dual}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_dual}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{vals (kerch.level.MVKPCA property)@\spxentry{vals}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.vals}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{vals}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Eigenvalues of the model. The model has to be fitted for these values to exist.

\end{fulllineitems}

\index{view() (kerch.level.MVKPCA method)@\spxentry{view()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.view}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{view}}}{\emph{\DUrole{n}{id}}}{{ $\rightarrow$ kerch.level.single\_view.View.View}}
\pysigstopsignatures
\end{fulllineitems}

\index{views (kerch.level.MVKPCA property)@\spxentry{views}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.views}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{views}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }Iterator\DUrole{p}{{[}}kerch.level.single\_view.View.View\DUrole{p}{{]}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{views\_by\_name() (kerch.level.MVKPCA method)@\spxentry{views\_by\_name()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.views_by_name}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{views\_by\_name}}}{\emph{\DUrole{n}{names}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.List}{List}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}kerch.level.single\_view.View.View\DUrole{p}{{]}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{watch\_properties (kerch.level.MVKPCA property)@\spxentry{watch\_properties}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.watch_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watch\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Properties to be watched (monitored). Their values can be accessed through the property \sphinxtitleref{watched\_properties}.
This is relevant e.g. in case of training.

\end{fulllineitems}

\index{watched\_properties (kerch.level.MVKPCA property)@\spxentry{watched\_properties}\spxextra{kerch.level.MVKPCA property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.watched_properties}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{watched\_properties}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
A dictionary containing the values of the properties that are specified in \sphinxtitleref{watch\_properties}.

\end{fulllineitems}

\index{weights\_by\_name() (kerch.level.MVKPCA method)@\spxentry{weights\_by\_name()}\spxextra{kerch.level.MVKPCA method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{level/mvkpca:kerch.level.MVKPCA.weights_by_name}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{weights\_by\_name}}}{\emph{\DUrole{n}{names}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.List}{List}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}{}
\pysigstopsignatures
\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\chapter{Model Module}
\label{\detokenize{model/index:model-module}}\label{\detokenize{model/index::doc}}
\sphinxstepscope


\chapter{Methods Module}
\label{\detokenize{methods/index:methods-module}}\label{\detokenize{methods/index::doc}}
\sphinxstepscope


\section{Kernel Smoother}
\label{\detokenize{methods/smoother:kernel-smoother}}\label{\detokenize{methods/smoother::doc}}

\subsection{Functions}
\label{\detokenize{methods/smoother:functions}}

\subsubsection{Coefficients\sphinxhyphen{}Based Smoother}
\label{\detokenize{methods/smoother:coefficients-based-smoother}}\index{smoother() (in module kerch.method)@\spxentry{smoother()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/smoother:kerch.method.smoother}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{smoother}}}{\emph{\DUrole{n}{coefficients}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{observations}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}all\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns a weighted sum of the observations by the coefficients.
\begin{equation*}
\begin{split}\mathtt{out}_{i,j} = \frac{\sum_l\mathtt{weights}_{i,l} * \mathtt{observations}_{l,j}}{\sum_l\mathtt{weights}_{i,l}}.\end{split}
\end{equation*}\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{coefficients}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} coefficients used in the smoother.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observations}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} observation corresponding to each weight dimension.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of closest points to be used. Either an integer representing the number or
the string \sphinxcode{\sphinxupquote{\textquotesingle{}all\textquotesingle{}}}. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}all\textquotesingle{}}}.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Weighted observations

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_observations{]}

\end{description}\end{quote}

\end{fulllineitems}



\subsubsection{Kernel Smoother}
\label{\detokenize{methods/smoother:id1}}\index{kernel\_smoother() (in module kerch.method)@\spxentry{kernel\_smoother()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/smoother:kerch.method.kernel_smoother}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{kernel\_smoother}}}{\emph{\DUrole{n}{domain}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{observations}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}all\textquotesingle{}}}, \emph{\DUrole{n}{kernel\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}rbf\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Smoothens the function \(f(\mathtt{domain}_{i,:}) = \mathtt{observation}_{i,:}\) with weights defined
as a kernel on the domain.
\begin{equation*}
\begin{split}\mathtt{out}_{i,j} = \frac{\sum_l k\left(\mathtt{domain}_{i,:},\mathtt{domain}_{l,:}\right) * \mathtt{observations}_{l,j}}{\sum_l k\left(\mathtt{domain}_{i,:},\mathtt{domain}_{l,:}\right)}.\end{split}
\end{equation*}
\sphinxAtStartPar
The kernel is defined as in {\hyperref[\detokenize{kernel/index:kerch.kernel.factory}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.factory()}}}}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{domain}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_domain}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} domain corresponding to each observation.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observations}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} observation corresponding to each domain entry.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of closest points to be used. Either an integer representing the number or
the string \sphinxcode{\sphinxupquote{\textquotesingle{}all\textquotesingle{}}}. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}all\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_type}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Type of kernel chosen. For the possible choices, please refer to the \sphinxtitleref{Factory Type} column of the
{\hyperref[\detokenize{kernel/index::doc}]{\sphinxcrossref{\DUrole{doc}{Kernel Module}}}} documentation. Defaults to \sphinxcode{\sphinxupquote{kerch.DEFAULT\_KERNEL\_TYPE}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Arguments to be passed to the kernel constructor, such as \sphinxtitleref{sample} or \sphinxtitleref{sigma}. If an argument is
passed that does not exist (e.g. \sphinxtitleref{sigma} to a \sphinxtitleref{linear} kernel), it will just be neglected. For the default
values, please refer to the default values of the requested kernel.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Smoothed function \(f\) according to kernel \(k\).

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_observations, dim\_observations{]}

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Example}
\label{\detokenize{methods/smoother:example}}

\subsubsection{Different Kernels, Same Bandwidth}
\label{\detokenize{methods/smoother:different-kernels-same-bandwidth}}
\sphinxAtStartPar
As we can see in the following example, different kernels have different behaviors. They all use the same bandwidth as
without specification, the bandwidth is based on the distance matrix which is here the same for all as they are all
based on the same domain. The same bandwidth is not always appropriate for different kernels.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{c+c1}{\PYGZsh{} data}
\PYG{n}{fun} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{x\PYGZus{}equal} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{x\PYGZus{}nonequal} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}

\PYG{n}{y\PYGZus{}original} \PYG{o}{=} \PYG{n}{fun}\PYG{p}{(}\PYG{n}{x\PYGZus{}equal}\PYG{p}{)}
\PYG{n}{y\PYGZus{}noisy} \PYG{o}{=} \PYG{n}{fun}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.2} \PYG{o}{*} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn\PYGZus{}like}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}equal}\PYG{p}{,} \PYG{n}{y\PYGZus{}original}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original Data}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dotted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{y\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Noisy Data}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernels}
\PYG{n}{kernels} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RBF}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Laplacian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Logistic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{olive}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Epanechnikov}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Quartic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{chartreuse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Silverman}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Triangular}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{teal}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tricube}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cyan}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Triweight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{royalblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
           \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{purple}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} kernel smoother}
\PYG{k}{for} \PYG{n}{name}\PYG{p}{,} \PYG{n}{c} \PYG{o+ow}{in} \PYG{n}{kernels}\PYG{p}{:}
    \PYG{n}{y\PYGZus{}reconstructed} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{method}\PYG{o}{.}\PYG{n}{kernel\PYGZus{}smoother}\PYG{p}{(}\PYG{n}{domain}\PYG{o}{=}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{observations}\PYG{o}{=}\PYG{n}{y\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{n}{name}\PYG{o}{.}\PYG{n}{lower}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{y\PYGZus{}reconstructed}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{name}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{c}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Smoothing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ncol}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{smoother-1}.pdf}
\end{figure}


\subsubsection{Same Kernels, Different Bandwidths}
\label{\detokenize{methods/smoother:same-kernels-different-bandwidths}}
\sphinxAtStartPar
In this example, we show how two different kernels react differently based on the prescribed bandwidth. We will
consider two kernels, the {\hyperref[\detokenize{kernel/generic/laplacian:kerch.kernel.Laplacian}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Laplacian}}}}} with a very heavy tail and the very restricted
{\hyperref[\detokenize{kernel/statistics/triweight:kerch.kernel.Triweight}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Triweight}}}}}. We can first have a view at their respective shapes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{c+c1}{\PYGZsh{} domain}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{500}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} define the kernels}
\PYG{n}{k\PYGZus{}l1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{k\PYGZus{}l2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Laplacian}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{k\PYGZus{}t1} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triweight}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{k\PYGZus{}t2} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{Triweight}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot the shapes}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{k\PYGZus{}l1}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplacian with \PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}l1}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{k\PYGZus{}l2}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Laplacian with \PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}l2}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{k\PYGZus{}t1}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Triweight with \PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}t1}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{k\PYGZus{}t2}\PYG{o}{.}\PYG{n}{k}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Triweight with \PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{k\PYGZus{}t2}\PYG{o}{.}\PYG{n}{sigma}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} annotate the plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Shape}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k(x,y=0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{.25}\PYG{p}{,} \PYG{l+m+mf}{1.1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ncol}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{smoother-2}.pdf}
\end{figure}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{pyplot} \PYG{k}{as} \PYG{n}{plt}

\PYG{c+c1}{\PYGZsh{} data}
\PYG{n}{fun} \PYG{o}{=} \PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x} \PYG{o}{*}\PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{x\PYGZus{}equal} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{x\PYGZus{}nonequal} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{values}

\PYG{n}{y\PYGZus{}original} \PYG{o}{=} \PYG{n}{fun}\PYG{p}{(}\PYG{n}{x\PYGZus{}equal}\PYG{p}{)}
\PYG{n}{y\PYGZus{}noisy} \PYG{o}{=} \PYG{n}{fun}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{.2} \PYG{o}{*} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn\PYGZus{}like}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axs} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{ax} \PYG{o+ow}{in} \PYG{n}{axs}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}equal}\PYG{p}{,} \PYG{n}{y\PYGZus{}original}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original Data}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dotted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{y\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Noisy Data}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Smoothing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} kernel smoother}
\PYG{n}{sigmas} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
          \PYG{p}{(}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
          \PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cyan}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
          \PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{purple}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{s}\PYG{p}{,} \PYG{n}{c} \PYG{o+ow}{in} \PYG{n}{sigmas}\PYG{p}{:}
    \PYG{n}{y\PYGZus{}laplacian} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{method}\PYG{o}{.}\PYG{n}{kernel\PYGZus{}smoother}\PYG{p}{(}\PYG{n}{domain}\PYG{o}{=}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{observations}\PYG{o}{=}\PYG{n}{y\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{laplacian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{n}{s}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}triweight} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{method}\PYG{o}{.}\PYG{n}{kernel\PYGZus{}smoother}\PYG{p}{(}\PYG{n}{domain}\PYG{o}{=}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{observations}\PYG{o}{=}\PYG{n}{y\PYGZus{}noisy}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{triweight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{n}{s}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{y\PYGZus{}laplacian}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{c}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Bandwidth \PYGZdl{}}\PYG{l+s+s2}{\PYGZbs{}}\PYG{l+s+s2}{sigma\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{s}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}nonequal}\PYG{p}{,} \PYG{n}{y\PYGZus{}triweight}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{c}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} plot}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{suptitle}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kernel Smoothing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Laplacian}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Triweight}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{o}{*}\PYG{n}{axs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{get\PYGZus{}legend\PYGZus{}handles\PYGZus{}labels}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ncol}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{smoother-3}.pdf}
\end{figure}

\sphinxstepscope


\section{Nearest Neighbors}
\label{\detokenize{methods/knn:nearest-neighbors}}\label{\detokenize{methods/knn::doc}}\index{knn() (in module kerch.method)@\spxentry{knn()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/knn:kerch.method.knn}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{knn}}}{\emph{\DUrole{n}{dists}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{observations}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
For each distance \sphinxcode{\sphinxupquote{dists}}, returns the average of the \sphinxcode{\sphinxupquote{num}} smallest corresponding observations.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dists}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} coefficients used in the knn.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observations}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} observation corresponding to each weight dimension.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} number of nearest neighbors. Defaults to 1.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
KNN

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_observations{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{kernel\_knn() (in module kerch.method)@\spxentry{kernel\_knn()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/knn:kerch.method.kernel_knn}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{kernel\_knn}}}{\emph{\DUrole{n}{domain}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{observations}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{num}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{1}}, \emph{\DUrole{n}{kernel\_type}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}rbf\textquotesingle{}}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
For each coefficient, returns the average of the \sphinxcode{\sphinxupquote{num}} greatest corresponding kernel values on the domain.
The kernel is defined as in {\hyperref[\detokenize{kernel/index:kerch.kernel.factory}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.factory()}}}}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{domain}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_domain}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} domain corresponding to each observation.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{observations}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_observations}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} observation corresponding to each domain entry.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} number of nearest neighbors. Defaults to 1.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel\_type}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Type of kernel chosen. For the possible choices, please refer to the \sphinxtitleref{Factory Type} column of the
{\hyperref[\detokenize{kernel/index::doc}]{\sphinxcrossref{\DUrole{doc}{Kernel Module}}}} documentation. Defaults to \sphinxcode{\sphinxupquote{kerch.DEFAULT\_KERNEL\_TYPE}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{**kwargs}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxstyleliteralemphasis{\sphinxupquote{dict}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Arguments to be passed to the kernel constructor, such as \sphinxtitleref{sample} or \sphinxtitleref{sigma}. If an argument is
passed that does not exist (e.g. \sphinxtitleref{sigma} to a \sphinxtitleref{linear} kernel), it will just be neglected. For the default
values, please refer to the default values of the requested kernel.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
KNN

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_observations{]}

\end{description}\end{quote}

\end{fulllineitems}


\sphinxstepscope


\section{Iterative Solver}
\label{\detokenize{methods/iterative:iterative-solver}}\label{\detokenize{methods/iterative::doc}}\index{iterative() (in module kerch.method)@\spxentry{iterative()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/iterative:kerch.method.iterative}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{iterative}}}{\emph{\DUrole{n}{obj}}, \emph{\DUrole{n}{x0}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{num\_iter}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{50}}, \emph{\DUrole{n}{lr}\DUrole{o}{=}\DUrole{default_value}{0.001}}, \emph{\DUrole{n}{verbose}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Minimizes to following problem for each point in order to find the preimage:
\begin{equation*}
\begin{split}\tilde{\mathbf{x}} = \mathrm{argmin}_{\mathbf{x}} \mathtt{obj}(\mathbf{x}).\end{split}
\end{equation*}
\sphinxAtStartPar
The method optimizes with an SGD algorithm.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Shows the training loop. Defaults to \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{obj}} \textendash{} Objective to minimize.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x0}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Starting value for the optimization.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_iter}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of iterations for the optimization process. Defaults to 50.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lr}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Learning rate of the optimizer. Defaults to 0.001.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Solution \(\tilde{\mathbf{x}}\)

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{iterative\_preimage\_k() (in module kerch.method)@\spxentry{iterative\_preimage\_k()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/iterative:kerch.method.iterative_preimage_k}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{iterative\_preimage\_k}}}{\emph{\DUrole{n}{k\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{kernel}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel}]{\sphinxcrossref{kerch.kernel.\_base\_kernel.\_BaseKernel}}}}}, \emph{\DUrole{n}{num\_iter}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{50}}, \emph{\DUrole{n}{lr}\DUrole{o}{=}\DUrole{default_value}{0.001}}, \emph{\DUrole{n}{light\_cache}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{verbose}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Minimizes to following problem for each point in order to find the preimage:
\begin{equation*}
\begin{split}\tilde{\mathbf{x}} = \mathrm{argmin}_{\mathbf{x}} \big\lVert \mathtt{k\_image} - \mathtt{kernel.k(x)} \big\rVert_2^2\end{split}
\end{equation*}
\sphinxAtStartPar
The method optimizes with an SGD algorithm.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Shows the training loop. Defaults to \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{k\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{num\_idx}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} coefficients in the RKHS to be inverted.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel}} ({\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Kernel}}}}} instance.) \textendash{} kernel on which this RKHS is based.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_iter}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of iterations for the optimization process. Defaults to 50.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lr}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Learning rate of the optimizer. Defaults to 0.001.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{light\_cache}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Specifies whether the cache has to made lighter during the pre\sphinxhyphen{}image to avoid keeping the
statistics of each iteration. This results in a speedup. Defaults to \sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_input{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{iterative\_preimage\_phi() (in module kerch.method)@\spxentry{iterative\_preimage\_phi()}\spxextra{in module kerch.method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{methods/iterative:kerch.method.iterative_preimage_phi}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.method.}}\sphinxbfcode{\sphinxupquote{iterative\_preimage\_phi}}}{\emph{\DUrole{n}{phi\_image}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}, \emph{\DUrole{n}{kernel}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel._BaseKernel}]{\sphinxcrossref{kerch.kernel.\_base\_kernel.\_BaseKernel}}}}}, \emph{\DUrole{n}{num\_iter}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{50}}, \emph{\DUrole{n}{lr}\DUrole{o}{=}\DUrole{default_value}{0.001}}, \emph{\DUrole{n}{light\_cache}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{verbose}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Minimizes to following problem for each point in order to find the preimage:
\begin{equation*}
\begin{split}\tilde{\mathbf{x}} = \mathrm{argmin}_{\mathbf{x}} \big\lVert \mathtt{phi\_image} - \mathtt{kernel.phi(x)} \big\rVert_2^2\end{split}
\end{equation*}
\sphinxAtStartPar
The method optimizes with an SGD algorithm.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{verbose}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Shows the training loop. Defaults to \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{phi\_image}} (\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}}\sphinxstyleliteralemphasis{\sphinxupquote{ {[}}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_points}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_feature}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} feature map image to be inverted.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{kernel}} ({\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Kernel}}}}} instance.) \textendash{} kernel on which this RKHS is based.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_iter}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of iterations for the optimization process. Defaults to 50.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{lr}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Learning rate of the optimizer. Defaults to 0.001.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{light\_cache}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Specifies whether the cache has to made lighter during the pre\sphinxhyphen{}image to avoid keeping the
statistics of each iteration. This results in a speedup. Defaults to \sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Pre\sphinxhyphen{}image

\item[{Return type}] \leavevmode
\sphinxAtStartPar
\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor} {[}num\_points, dim\_feature{]}

\end{description}\end{quote}

\end{fulllineitems}


\sphinxstepscope


\chapter{Features}
\label{\detokenize{features/index:module-feature}}\label{\detokenize{features/index:features}}\label{\detokenize{features/index::doc}}\index{module@\spxentry{module}!feature@\spxentry{feature}}\index{feature@\spxentry{feature}!module@\spxentry{module}}
\sphinxstepscope


\section{Logging in Kerch}
\label{\detokenize{features/logger:logging-in-kerch}}\label{\detokenize{features/logger::doc}}
\sphinxAtStartPar
Many classes throughout this package display various messages during instantiation and usage. This abstract from
which they descend allows the dislpaying of those messages and control which are printed using the {\hyperref[\detokenize{features/logger:kerch.feature.Logger.logging_level}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{logging\_level}}}}}
attribute.


\subsection{Functionalities}
\label{\detokenize{features/logger:functionalities}}
\sphinxAtStartPar
This abstract class has only one purpose: adding a {\hyperref[\detokenize{features/logger:kerch.feature.Logger._logger}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_logger}}}}} attribute meant to log various
messages across the package.
Doing it this way allows to get the name of the class instantiated and print more precise messages.


\subsubsection{Logging Messages}
\label{\detokenize{features/logger:logging-messages}}
\sphinxAtStartPar
The private property \sphinxcode{\sphinxupquote{self.\_logger}} is an instance of the class
\sphinxhref{https://docs.python.org/3/library/logging.html\#logging.Logger}{\sphinxcode{\sphinxupquote{logging.Logger}}} of the Python standard library \sphinxhref{https://docs.python.org/3/library/logging.html}{logging — Logging facility for Python}.
This allows logging messages by calling, e.g., \sphinxcode{\sphinxupquote{self.\_logger.debug(message: str)}},
\sphinxcode{\sphinxupquote{self.\_logger.info(message: str)}}, \sphinxcode{\sphinxupquote{self.\_logger.warning(message: str)}} or \sphinxcode{\sphinxupquote{self.\_logger.error(message: str)}}. The
messages will be automatically formatted to reference the appropriate class and give file and line information in debug
mode.


\subsubsection{Logging Level}
\label{\detokenize{features/logger:logging-level}}
\sphinxAtStartPar
A particular log level can be set for each instance using the attribute {\hyperref[\detokenize{features/logger:kerch.feature.Logger.logging_level}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{logging\_level}}}}}.
If assigned to \sphinxcode{\sphinxupquote{None}},the default logging level will be used.
This value is also set during instantiation using the \sphinxcode{\sphinxupquote{logging\_level}} argument. If nothing is specified, \sphinxcode{\sphinxupquote{None}} is
passed leading to the genral default logging level.

\sphinxAtStartPar
The log level always corresponds to an integer. We refer to \sphinxhref{https://docs.python.org/3/library/logging.html}{logging — Logging facility for Python} for the
different values.


\subsubsection{Default Logging Level}
\label{\detokenize{features/logger:default-logging-level}}
\sphinxAtStartPar
This is the default logging level that is used if {\hyperref[\detokenize{features/logger:kerch.feature.Logger.logging_level}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{logging\_level}}}}} is never specified (which corresponds to setting it to \sphinxcode{\sphinxupquote{None}}).
By default, the logging level is 30, which corresponds to a \sphinxtitleref{warning} level. This default package value can also be changed
and read using the following functions.
\index{set\_logging\_level() (in module kerch)@\spxentry{set\_logging\_level()}\spxextra{in module kerch}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/logger:kerch.set_logging_level}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.}}\sphinxbfcode{\sphinxupquote{set\_logging\_level}}}{\emph{\DUrole{n}{level}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Changes the default logging level of the kerch package.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}) \textendash{} Default kerch logging value

\end{description}\end{quote}

\sphinxAtStartPar
Usage:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{logging}

\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{set\PYGZus{}logging\PYGZus{}level}\PYG{p}{(}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{DEBUG}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Changing the default logging value does not affect the already instantiated objects. We advise to set those values in
the beginning of the code.
\end{sphinxadmonition}

\end{fulllineitems}

\index{get\_logging\_level() (in module kerch)@\spxentry{get\_logging\_level()}\spxextra{in module kerch}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/logger:kerch.get_logging_level}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.}}\sphinxbfcode{\sphinxupquote{get\_logging\_level}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the default logging level of the kerch package.

\sphinxAtStartPar
Usage:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{logging}

\PYG{n}{default\PYGZus{}level} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{get\PYGZus{}logging\PYGZus{}level}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{default\PYGZus{}level} \PYG{o}{=} \PYG{n}{logging}\PYG{o}{.}\PYG{n}{getLevelName}\PYG{p}{(}\PYG{n}{default\PYGZus{}level}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{default\PYGZus{}level}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
WARNING
\end{sphinxVerbatim}

\end{fulllineitems}



\subsection{Abstract Class}
\label{\detokenize{features/logger:abstract-class}}\index{Logger (class in kerch.feature)@\spxentry{Logger}\spxextra{class in kerch.feature}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/logger:kerch.feature.Logger}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.feature.}}\sphinxbfcode{\sphinxupquote{Logger}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxhref{https://docs.python.org/3/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{description}\end{quote}
\index{\_logger (kerch.feature.Logger property)@\spxentry{\_logger}\spxextra{kerch.feature.Logger property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/logger:kerch.feature.Logger._logger}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{\_logger}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/logging.html\#logging.Logger}{logging.Logger}}}}
\pysigstopsignatures
\sphinxAtStartPar
Logger of the instance.

\sphinxAtStartPar
Usage:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{logging}

\PYG{k}{class} \PYG{n+nc}{MyClass}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{feature}\PYG{o}{.}\PYG{n}{Logger}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyClass}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}logger}\PYG{o}{.}\PYG{n}{info}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Instantiation done information.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}logger}\PYG{o}{.}\PYG{n}{warn}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Instantiation done warning.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{First class with default logging level:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{my\PYGZus{}class1} \PYG{o}{=} \PYG{n}{MyClass}\PYG{p}{(}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Second instance with logging.INFO logging level:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{my\PYGZus{}class2} \PYG{o}{=} \PYG{n}{MyClass}\PYG{p}{(}\PYG{n}{logging\PYGZus{}level}\PYG{o}{=}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{INFO}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
First class with default logging level:
WARNING:MyClass:Instantiation done warning.

Second instance with logging.INFO logging level:
INFO:MyClass:Instantiation done information.
WARNING:MyClass:Instantiation done warning.
\end{sphinxVerbatim}

\end{fulllineitems}

\index{logging\_level (kerch.feature.Logger property)@\spxentry{logging\_level}\spxextra{kerch.feature.Logger property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/logger:kerch.feature.Logger.logging_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{logging\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Logging level of this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log Level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default global kerch level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{fulllineitems}


\end{fulllineitems}



\subsection{Example}
\label{\detokenize{features/logger:example}}

\subsubsection{Default}
\label{\detokenize{features/logger:default}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{k}\PYG{o}{.}\PYG{n}{sample} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}                                          \PYG{c+c1}{\PYGZsh{} first warning (the sigma is defined)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
WARNING:RBF:The kernel bandwidth sigma has not been provided and is assigned by a heuristic (sigma=5.00e\PYGZhy{}01).
tensor([[1.0000e+00, 1.3534e\PYGZhy{}01, 3.3546e\PYGZhy{}04],
        [1.3534e\PYGZhy{}01, 1.0000e+00, 1.3534e\PYGZhy{}01],
        [3.3546e\PYGZhy{}04, 1.3534e\PYGZhy{}01, 1.0000e+00]])
\end{sphinxVerbatim}


\subsubsection{Info}
\label{\detokenize{features/logger:info}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{logging}

\PYG{n}{k} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{kernel}\PYG{o}{.}\PYG{n}{RBF}\PYG{p}{(}\PYG{n}{logging\PYGZus{}level}\PYG{o}{=}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{INFO}\PYG{p}{)}    \PYG{c+c1}{\PYGZsh{} first info (no sample initialized yet)}
\PYG{n}{k}\PYG{o}{.}\PYG{n}{sample} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}                                 \PYG{c+c1}{\PYGZsh{} second info (the sample is initialized)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{k}\PYG{o}{.}\PYG{n}{K}\PYG{p}{)}                                          \PYG{c+c1}{\PYGZsh{} first warning (the sigma is defined)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
INFO:RBF:The sample cannot be initialized because no sample data has been provided nor the sample dimensions have been initialized yet.
WARNING:RBF:The kernel bandwidth sigma has not been provided and is assigned by a heuristic (sigma=5.00e\PYGZhy{}01).
tensor([[1.0000e+00, 1.3534e\PYGZhy{}01, 3.3546e\PYGZhy{}04],
        [1.3534e\PYGZhy{}01, 1.0000e+00, 1.3534e\PYGZhy{}01],
        [3.3546e\PYGZhy{}04, 1.3534e\PYGZhy{}01, 1.0000e+00]])
\end{sphinxVerbatim}


\subsection{Inheritance}
\label{\detokenize{features/logger:inheritance}}
\sphinxAtStartPar
This is a base class that directly inherits from \sphinxhref{https://docs.python.org/3/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}.

\sphinxstepscope


\section{Kerch Module}
\label{\detokenize{features/module:kerch-module}}\label{\detokenize{features/module::doc}}
\sphinxAtStartPar
The {\hyperref[\detokenize{features/module:kerch.feature.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.Module}}}}} class is aimed at modules that must be trained trough gradient descent. It extends the
\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{\sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}} to add the logging
features of the {\hyperref[\detokenize{features/logger:kerch.feature.Logger}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.Logger}}}}} class.


\subsection{Functionalities}
\label{\detokenize{features/module:functionalities}}
\sphinxAtStartPar
It also adds the following functionalities necessary for more complex gradient descent than what PyTorch offers. In
particular:


\subsubsection{Before and After Step Operations}
\label{\detokenize{features/module:before-and-after-step-operations}}
\sphinxAtStartPar
The methods {\hyperref[\detokenize{features/module:kerch.feature.Module.before_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{before\_step()}}}}} and {\hyperref[\detokenize{features/module:kerch.feature.Module.after_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{after\_step()}}}}} for
operations that must be executed before and after a parameter update through an optimization step.


\subsubsection{Different Parameter Types}
\label{\detokenize{features/module:different-parameter-types}}
\sphinxAtStartPar
The support for various groups of parameters that require specific learning rates or lie on the Stiefel manifold. The
following group types are available
\begin{itemize}
\item {} \begin{description}
\item[{Euclidean}] \leavevmode
\sphinxAtStartPar
Parameters lying on the Euclidean manifold (standard optimization). The optimization is
done onto \(\mathbb{R}^{n \times m}\), \(n\) and \(m\) depending on the size of each parameter.

\end{description}

\item {} \begin{description}
\item[{Stiefel}] \leavevmode
\sphinxAtStartPar
Parameters that must lie on the Stiefel manifold (optimization is done onto that manifold).
The Stiefel manifold corresponds to the orthonormal parameters \(U \in \mathrm{St}(n,m)\), i.e., all
\(U \in \mathbb{R}^{n \times m}\) such that \(U^\top U = I\). The dimensions \(n\) and \(m\) are
proper to each parameter.

\end{description}

\item {} \begin{description}
\item[{Slow}] \leavevmode
\sphinxAtStartPar
Parameters lying on the Euclidean manifold (standard optimization). The optimization is
done onto \(\mathbb{R}^{n \times m}\), \(n\) and \(m\) depending on the size of each parameter.
The specificity of these slow Euclidean parameters is that they are better trained with a lower learning rate that the
others, hence their name and the necessity to group them apart.

\end{description}

\end{itemize}


\subsubsection{Hyperparameters Dictionaries}
\label{\detokenize{features/module:hyperparameters-dictionaries}}
\sphinxAtStartPar
This is relevant for automatically recording values before, during of after the training. All the relevant hyperparameters are
listed into two dictionaries.
\begin{itemize}
\item {} \begin{description}
\item[{Fixed Hyperparameters}] \leavevmode
\sphinxAtStartPar
The attribute {\hyperref[\detokenize{features/module:kerch.feature.Module.hparams_fixed}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_fixed}}}}} return the fixed hyperparameters of the module. By contrast with \sphinxcode{\sphinxupquote{hparams\_variable}}, these are the values that are fixed and
cannot possibly change during the training. If applicable, these can be specific architecture values for example.

\end{description}

\item {} \begin{description}
\item[{Variable Hyperparameters}] \leavevmode
\sphinxAtStartPar
The attribute {\hyperref[\detokenize{features/module:kerch.feature.Module.hparams_variable}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_variable}}}}} return the fixed hyperparameters of the module. By contrast with \sphinxcode{\sphinxupquote{hparams\_fixed}}, these are the values that are may change during
the training and may be monitored at various stages during the training. If applicable, these can be kernel bandwidth parameters for example.

\end{description}

\end{itemize}


\subsection{Abstract Class}
\label{\detokenize{features/module:abstract-class}}\index{Module (class in kerch.feature)@\spxentry{Module}\spxextra{class in kerch.feature}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.feature.}}\sphinxbfcode{\sphinxupquote{Module}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{features/logger:kerch.feature.Logger}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.logger.Logger}}}}}, \sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{\sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}}, \sphinxhref{https://docs.python.org/3/library/functions.html\#object}{\sphinxcode{\sphinxupquote{object}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{description}\end{quote}
\index{\_euclidean\_parameters() (kerch.feature.Module method)@\spxentry{\_euclidean\_parameters()}\spxextra{kerch.feature.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module._euclidean_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_euclidean\_parameters}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Iterator yielding all parameters lying on the Euclidean manifold (standard optimization). The optimization is
done onto \(\mathbb{R}^{n \times m}\), \(n\) and \(m\) depending on the size of each parameter.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, yields both the Euclidean parameters of this module and its potential children.
otherwise, it only yields Euclidean parameters from this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Euclidean parameters

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Iterator{[}torch.nn.Parameter{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_slow\_parameters() (kerch.feature.Module method)@\spxentry{\_slow\_parameters()}\spxextra{kerch.feature.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module._slow_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_slow\_parameters}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Iterator yielding all parameters lying on the Euclidean manifold (standard optimization). The optimization is
done onto \(\mathbb{R}^{n \times m}\), \(n\) and \(m\) depending on the size of each parameter.

\sphinxAtStartPar
The specificity of these slow Euclidean parameters is that they are better trained with a lower learning rate that the
others, hence their name and the necessity to group them apart.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, yields both the slow (Euclidean) parameters of this module and its potential children.
otherwise, it only yields slow (Euclidean) parameters from this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Slow (Euclidean) parameters

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Iterator{[}torch.nn.Parameter{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_stiefel\_parameters() (kerch.feature.Module method)@\spxentry{\_stiefel\_parameters()}\spxextra{kerch.feature.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module._stiefel_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_stiefel\_parameters}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Iterator yielding all parameters that must lie on the Stiefel manifold (optimization is done onto that manifold).
The Stiefel manifold corresponds to the orthonormal parameters \(U \in \mathrm{St}(n,m)\), i.e., all
\(U \in \mathbb{R}^{n \times m}\) such that \(U^\top U = I\). The dimensions \(n\) and \(m\) are
proper to each parameter.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, yields both the Stiefel parameters of this module and its potential children.
otherwise, it only yields Stiefel parameters from this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Stiefel parameters

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Iterator{[}torch.nn.Parameter{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{after\_step() (kerch.feature.Module method)@\spxentry{after\_step()}\spxextra{kerch.feature.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module.after_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{after\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Specific operations to be performed after a training step. We refer to the documentation of
{\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{before\_step() (kerch.feature.Module method)@\spxentry{before\_step()}\spxextra{kerch.feature.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module.before_step}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{before\_step}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Specific operations to be performed before a training step. We refer to the documentation of
{\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{hparams\_fixed (kerch.feature.Module property)@\spxentry{hparams\_fixed}\spxextra{kerch.feature.Module property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Fixed hyperparameters of the module. By contrast with {\hyperref[\detokenize{features/module:kerch.feature.Module.hparams_variable}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_variable}}}}}, these are the values that are fixed and
cannot possibly change during the training. If applicable, these can be specific architecture values for example.
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{hparams\_variable (kerch.feature.Module property)@\spxentry{hparams\_variable}\spxextra{kerch.feature.Module property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Variable hyperparameters of the module. By contrast with {\hyperref[\detokenize{features/module:kerch.feature.Module.hparams_fixed}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_fixed}}}}}, these are the values that are may change during
the training and may be monitored at various stages during the training.
If applicable, these can be kernel bandwidth parameters for example.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
All parameters that are potentially trainable, like a kernel bandwidth \(\sigma\) for example, are
included in this dictionary, even if the corresponding trainable argument is set to \sphinxcode{\sphinxupquote{False}}. In the
latter case, they will be not evolve during training iterations, but will still be included in this
dictionary.
\end{sphinxadmonition}

\sphinxAtStartPar
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{logging\_level (kerch.feature.Module property)@\spxentry{logging\_level}\spxextra{kerch.feature.Module property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module.logging_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{logging\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Logging level of this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log Level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default global kerch level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{fulllineitems}

\index{manifold\_parameters() (kerch.feature.Module method)@\spxentry{manifold\_parameters()}\spxextra{kerch.feature.Module method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/module:kerch.feature.Module.manifold_parameters}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{manifold\_parameters}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{type}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}euclidean\textquotesingle{}}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterator}{Iterator}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Iterator yielding the parameters of a specific type. A distinction is made between three types:
\begin{itemize}
\item {} \begin{description}
\item[{\sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}}}:}] \leavevmode
\sphinxAtStartPar
parameters lying on the Euclidean manifold (standard optimization). The optimization is
done onto \(\mathbb{R}^{n \times m}\), \(n\) and \(m\) depending on the size of each parameter.

\end{description}

\item {} \begin{description}
\item[{\sphinxcode{\sphinxupquote{\textquotesingle{}stiefel\textquotesingle{}}}:}] \leavevmode
\sphinxAtStartPar
parameters that must lie on the Stiefel manifold (optimization is done onto that manifold).
The Stiefel manifold corresponds to the orthonormal parameters \(U \in \mathrm{St}(n,m)\), i.e., all
\(U \in \mathbb{R}^{n \times m}\) such that \(U^\top U = I\). The dimensions \(n\) and \(m\) are
proper to each parameter.

\end{description}

\item {} \begin{description}
\item[{\sphinxcode{\sphinxupquote{\textquotesingle{}slow\textquotesingle{}}}:}] \leavevmode
\sphinxAtStartPar
parameters lying on the Euclidean manifold (standard optimization). The optimization is
done onto \(\mathbb{R}^{n \times m}\), \(n\) and \(m\) depending on the size of each parameter.
The specificity of these slow Euclidean parameters is that they are better trained with a lower learning rate that the
others, hence their name and the necessity to group them apart.

\end{description}

\end{itemize}

\sphinxAtStartPar
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{type}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Which parameters group the method must return. The three values above are accepted. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}euclidean\textquotesingle{}}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, yields both the specified parameters of this module and its potential children.
otherwise, it only yields the specified parameters from this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
Parameters of the specified type

\item[{Return type}] \leavevmode
\sphinxAtStartPar
Iterator{[}torch.nn.Parameter{]}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Examples}
\label{\detokenize{features/module:examples}}

\subsubsection{KPCA}
\label{\detokenize{features/module:kpca}}
\sphinxAtStartPar
In the following example, we create a {\hyperref[\detokenize{level/kpca:kerch.level.KPCA}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.level.KPCA}}}}} level based on a {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.RBF}}}}} kernel
where we specify that only the level parameters are trainable by gradient descent.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{kpca} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{level}\PYG{o}{.}\PYG{n}{KPCA}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,}                 \PYG{c+c1}{\PYGZsh{} random sample}
                        \PYG{n}{kernel\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rbf}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} we use a rbf kernel (this is the default value, but we specify it for clarity)}
                        \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}                  \PYG{c+c1}{\PYGZsh{} we specify a RBF bandwidth value}
                        \PYG{n}{representation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dual}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} we work in dual representation (also default value, but specified for clarity)}
                        \PYG{n}{dim\PYGZus{}output}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}             \PYG{c+c1}{\PYGZsh{} we want an output dimension of 2 (the input is 3)}
                        \PYG{n}{sample\PYGZus{}trainable}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} the sample can be trained, but we don\PYGZsq{}t want that: we want it fixed}
                        \PYG{n}{sigma\PYGZus{}trainable}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} the sigma can also be trained, but we don\PYGZsq{}t want that either}
                        \PYG{n}{level\PYGZus{}trainable}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}     \PYG{c+c1}{\PYGZsh{} the level parameters are trainable, meaning that the eigenvectors are trainable by gradient}
\end{sphinxVerbatim}

\sphinxAtStartPar
We indeed see that from all
parameters printed, only the eigenvectors \sphinxcode{\sphinxupquote{hidden}} (lying on the Stiefel manifold) have \sphinxcode{\sphinxupquote{requires\_grad=True}}. The Euclidean
parameters correspond here to the sample {\hyperref[\detokenize{features/sample:kerch.feature.Sample.sample}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{sample}}}}}, that we do not want to be trained. The slow (Euclidean) parameters correspond
to the bandwidth of the kernel {\hyperref[\detokenize{kernel/generic/rbf:kerch.kernel.RBF.sigma}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{sigma}}}}}, which we also want to remain fixed and not optimized through gradient descent.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Euclidean parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{EUCLIDEAN PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{kpca}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{euclidean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Stiefel parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{STIEFEL PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{kpca}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stiefel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Slow (Euclidean) parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{SLOW (EUCLIDEAN) PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{kpca}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slow}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
EUCLIDEAN PARAMETERS:
Parameter containing:
tensor([[\PYGZhy{}1.6809,  0.3609, \PYGZhy{}0.6566],
        [ 0.8784, \PYGZhy{}1.2979, \PYGZhy{}1.3302],
        [\PYGZhy{}0.8879, \PYGZhy{}0.6791, \PYGZhy{}0.3592],
        [ 1.7466,  0.7167, \PYGZhy{}0.0109],
        [\PYGZhy{}0.4657,  1.5356, \PYGZhy{}1.3600]])

STIEFEL PARAMETERS:
Parameter containing:
tensor([[\PYGZhy{}0.4833, \PYGZhy{}0.2549,  0.7117, \PYGZhy{}0.4410, \PYGZhy{}0.0210],
        [ 0.0759,  0.3720, \PYGZhy{}0.3470, \PYGZhy{}0.8551, \PYGZhy{}0.0658]], requires\PYGZus{}grad=True)

SLOW (EUCLIDEAN) PARAMETERS:
Parameter containing:
tensor(2.)
\end{sphinxVerbatim}

\sphinxAtStartPar
We can have a look at the hyperparameters. The parameter \sphinxcode{\sphinxupquote{sigma}}, even if not trainable is always listed in the
variable hyperparameters. Its value will just not change during the training.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FIXED HYPERPARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{kpca}\PYG{o}{.}\PYG{n}{hparams\PYGZus{}fixed}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{key}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{value}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{VARIABLE HYPERPARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{kpca}\PYG{o}{.}\PYG{n}{hparams\PYGZus{}variable}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{key}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{value}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
FIXED HYPERPARAMETERS:
Level type : KPCA
Level eta : 1.0
Kernel : Exponential
Squared exp. distance : True
Trainable sigma : False
Default kernel transforms : []
Output dimension : 2
Representation : dual
Parameters trainable : True
Constraint : soft
Input dimension : 3
Trainable sample : False
Default sample transforms : []

VARIABLE HYPERPARAMETERS:
Kernel parameter sigma : 2.0
\end{sphinxVerbatim}


\subsubsection{Creating a Module}
\label{\detokenize{features/module:creating-a-module}}
\sphinxAtStartPar
In this example, we create a module containing a parameter on the Euclidean manifold. We therefore overwrite the
{\hyperref[\detokenize{features/module:kerch.feature.Module._euclidean_parameters}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_euclidean\_parameters()}}}}} method and not forget to call the inherited classes to not forget to return all
parameters returned by the mother classes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{Iterator}


\PYG{k}{class} \PYG{n+nc}{MyModule}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{feature}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyModule}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we recover the parameter size by the argument param\PYGZus{}size}
        \PYG{n}{param\PYGZus{}size} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{param\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we create our parameter of float type kerch.FTYPE}
        \PYG{c+c1}{\PYGZsh{} (this value can be modified and ensures that all floating types are the same throughout the code)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{my\PYGZus{}param} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Parameter}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{param\PYGZus{}size}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{FTYPE}\PYG{p}{)}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}euclidean\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{recurse}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Iterator}\PYG{p}{[}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Parameter}\PYG{p}{]}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} important not to forget, otherwise the parameters returned by mother classes will be skipped}
        \PYG{k}{yield from} \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyModule}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n}{\PYGZus{}euclidean\PYGZus{}parameters}\PYG{p}{(}\PYG{n}{recurse}\PYG{o}{=}\PYG{n}{recurse}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} we yield our additional new parameter}
        \PYG{k}{yield} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{my\PYGZus{}param}

    \PYG{k}{def} \PYG{n+nf}{after\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} after each training step, we want the columns to be centered}
        \PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{my\PYGZus{}param}\PYG{o}{.}\PYG{n}{data} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{my\PYGZus{}param} \PYG{o}{\PYGZhy{}} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{my\PYGZus{}param}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{hparams\PYGZus{}fixed}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{dict}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} we add the shape of our parameter to the fixed hyperparameters}
        \PYG{c+c1}{\PYGZsh{} we don\PYGZsq{}t forget to return the other possible hyperparameters issued by parent classes}
        \PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{my\PYGZus{}param size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{my\PYGZus{}param}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{,}
                \PYG{o}{*}\PYG{o}{*}\PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyModule}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n}{hparams\PYGZus{}fixed}\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} We instantiate our class}
\PYG{n}{my\PYGZus{}module} \PYG{o}{=} \PYG{n}{MyModule}\PYG{p}{(}\PYG{n}{param\PYGZus{}size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We can have a look at the parameters. The same can be done for the {\hyperref[\detokenize{features/module:kerch.feature.Module._stiefel_parameters}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_stiefel\_parameters()}}}}} and
{\hyperref[\detokenize{features/module:kerch.feature.Module._slow_parameters}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_slow\_parameters()}}}}} methods.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Euclidean parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{EUCLIDEAN PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{euclidean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Stiefel parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{STIEFEL PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{stiefel}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Slow (Euclidean) parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{SLOW (EUCLIDEAN) PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slow}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
EUCLIDEAN PARAMETERS:
Parameter containing:
tensor([[ 1.5410, \PYGZhy{}0.2934, \PYGZhy{}2.1788],
        [ 0.5684, \PYGZhy{}1.0845, \PYGZhy{}1.3986]], requires\PYGZus{}grad=True)

STIEFEL PARAMETERS:

SLOW (EUCLIDEAN) PARAMETERS:
\end{sphinxVerbatim}

\sphinxAtStartPar
If {\hyperref[\detokenize{features/module:kerch.feature.Module.after_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{after\_step()}}}}} is called, we can observe that the parameter is centered along the columns.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} we suppose that an optimization step has been performed}
\PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{after\PYGZus{}step}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Euclidean parameters}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{EUCLIDEAN PARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{manifold\PYGZus{}parameters}\PYG{p}{(}\PYG{n+nb}{type}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{euclidean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
EUCLIDEAN PARAMETERS:
Parameter containing:
tensor([[ 0.4863,  0.3955, \PYGZhy{}0.3901],
        [\PYGZhy{}0.4863, \PYGZhy{}0.3955,  0.3901]], requires\PYGZus{}grad=True)
\end{sphinxVerbatim}

\sphinxAtStartPar
Similarly, let us print the hyperparameters:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FIXED HYPERPARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{hparams\PYGZus{}fixed}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{key}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{value}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{VARIABLE HYPERPARAMETERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{my\PYGZus{}module}\PYG{o}{.}\PYG{n}{hparams\PYGZus{}variable}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{key}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{value}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
FIXED HYPERPARAMETERS:
my\PYGZus{}param size : torch.Size([2, 3])

VARIABLE HYPERPARAMETERS:
\end{sphinxVerbatim}


\subsection{Inheritance Diagram}
\label{\detokenize{features/module:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-f112777c08b8b640aa35fa32aa16b1448848f853.pdf}

\sphinxstepscope


\section{Cache Management}
\label{\detokenize{features/cache:cache-management}}\label{\detokenize{features/cache::doc}}
\sphinxAtStartPar
When performing various operations on the Kerch modules, it may happen that some results are necessary multiple times.
Some of these operations are however expensive to compute and it would be ideal to avoid recomputation and load them
from memory when already computed previously. This justifies the addition of a cache manager. The purpose of the
{\hyperref[\detokenize{features/cache:kerch.feature.Cache}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.Cache}}}}} class is to extend the {\hyperref[\detokenize{features/module:kerch.feature.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.Module}}}}} class with a cache manager.
To illustrate its relevance, we can consider two example use cases:
\begin{itemize}
\item {} \begin{description}
\item[{Kernel Matrix:}] \leavevmode
\sphinxAtStartPar
Due to its quadratic complexity, computing the kernel matrix may be very expensive, in particular if
not computed through explicit feature maps. Let us suppose that the kernel matrix has already been computed
in order to be plotted for example. If one wants to then compute its eigendecomposition for KPCA, the matrix is
reloaded from memory and not computed a second time.

\end{description}

\item {} \begin{description}
\item[{Data Transformations:}] \leavevmode
\sphinxAtStartPar
We consider a big sample dataset that we want to be centered and normalized. When working with out\sphinxhyphen{}of\sphinxhyphen{}sample
datasets, these have to be centered using the same statistics as sample in order to keep the model consistent.
These statistics can be stored and re\sphinxhyphen{}used every time computations have to be performed on out\sphinxhyphen{}of\sphinxhyphen{}sample datasets.

\end{description}

\end{itemize}

\sphinxAtStartPar
In other words, this is a way to bypass the garbage\sphinxhyphen{}collector, but with a lot of granularity.


\subsection{Functionalities}
\label{\detokenize{features/cache:functionalities}}

\subsubsection{Automatic Getter and Saver}
\label{\detokenize{features/cache:automatic-getter-and-saver}}
\sphinxAtStartPar
Essentially most of the work is done through the {\hyperref[\detokenize{features/cache:kerch.feature.Cache._get}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_get()}}}}} method that allows to check in
one line if the result of an operation (a kernel matrix, sample statistics…) have already been computed and return
it. If not computed, the information is stored in the cache, ready for further re\sphinxhyphen{}use (through the
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}} method that is automatically called by {\hyperref[\detokenize{features/cache:kerch.feature.Cache._get}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_get()}}}}} in
if not already present in the cache).


\subsubsection{Cache Levels}
\label{\detokenize{features/cache:cache-levels}}
\sphinxAtStartPar
It would be totally inefficient to store everything in the cache. Therefore, different cache levels exist meant for
storing different values. Each value is assigned to a specific value when computed for the first time. Each module
has a internal cache level ({\hyperref[\detokenize{features/cache:kerch.feature.Cache.cache_level}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{cache\_level}}}}}) that serves as a threshold when new values
are computed. If the specified cache level of the newly computed result exceeds the default cache level of the module,
the information is not saved. The next time that the same computation is required, it will thus not be loaded from the
cache, but computed again. We distinguish the following cache levels:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go. This is the lightest for the memory.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved, not the kernel matrices.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved. This is very heavy on the memory.

\end{itemize}

\sphinxAtStartPar
The higher the cache level, the more will be stored into memory and the less redundancy will be introduced in the need
for recomputations. The module’s {\hyperref[\detokenize{features/cache:kerch.feature.Cache.cache_level}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{cache\_level}}}}} attribute therefore controls a time versus
memory trade\sphinxhyphen{}off.


\subsubsection{Resetter}
\label{\detokenize{features/cache:resetter}}
\sphinxAtStartPar
If the sample changes for example, most of the cache entries require te be recomputed. The two
private methods {\hyperref[\detokenize{features/cache:kerch.feature.Cache._reset_cache}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_reset\_cache()}}}}} and {\hyperref[\detokenize{features/cache:kerch.feature.Cache._clean_cache}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_clean\_cache()}}}}} therefore
exists to reset and clean the cache. This is done automatically when necessary. In practice, unless wanting to tweak
the package’s internal working, these methods should not be called by the user. If the user really wants to reset the
cache, he may use the {\hyperref[\detokenize{features/cache:kerch.feature.Cache.reset}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{reset()}}}}} method. He may also visualize the current cache entries by
calling the {\hyperref[\detokenize{features/cache:kerch.feature.Cache.print_cache}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{print\_cache()}}}}} method or the {\hyperref[\detokenize{features/cache:kerch.feature.Cache.cache_keys}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{cache\_keys()}}}}}
method to retrieve the keys of the cached values.


\subsection{Default Cache Levels}
\label{\detokenize{features/cache:default-cache-levels}}
\sphinxAtStartPar
For each possible value to be stored, default cache levels are saved defined in \sphinxcode{\sphinxupquote{kerch.DEFAULT\_CACHE\_LEVELS}}.
These values can be changed if the user wants to customize the granularity. Here follows a summary.


\subsubsection{Sample\sphinxhyphen{}Specific}
\label{\detokenize{features/cache:sample-specific}}
\sphinxAtStartPar
The Transformation Tree refers to an instance of {\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.transform.TransformTree}}}}} and does not hold any data in
itself apart from the successive operations in themselves to perform the transformations, i.e., the tree of the transformations.
We refer further the cache levels inside the tree.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Key
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}none\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}light\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}heavy\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}total\textquotesingle{}}}
\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"sample\_transform"}}
&
\sphinxAtStartPar
Sample Transformation Tree
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Kernel\sphinxhyphen{}Specific}
\label{\detokenize{features/cache:kernel-specific}}
\sphinxAtStartPar
This is relevant for all classes who inherit from {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.kernel.Kernel}}}}}. The attributes {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.K}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{K}}}}},
{\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.Phi}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Phi}}}}} and {\hyperref[\detokenize{kernel/abstract/kernel:kerch.kernel.Kernel.C}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{C}}}}} are always saved once computed until the sample
changes. The Transformation Trees refer to instances {\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.transform.TransformTree}}}}} (one for the explicit and one for the implicit)
and do not hold any data in
themselves apart from the successive operations themselves to perform the transformations, i.e., the tree of the transformations.
We refer further the cache levels inside the trees.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Key
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}none\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}light\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}heavy\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}total\textquotesingle{}}}
\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"sample\_phi"}}
&
\sphinxAtStartPar
Explicit feature map of the sample
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"sample\_C"}}
&
\sphinxAtStartPar
Explicit matrix of the sample
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"sample\_K"}}
&
\sphinxAtStartPar
Kernel matrix of the sample
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"kernel\_explicit\_transform"}}
&
\sphinxAtStartPar
Explicit Transformation Tree
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"kernel\_implicit\_transform"}}
&
\sphinxAtStartPar
Implicit Transformation Tree
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Transformation\sphinxhyphen{}Specific}
\label{\detokenize{features/cache:transformation-specific}}
\sphinxAtStartPar
The tree itself contains the transformations in themselves. These values refer to which values are stored inside a
transformation tree instance {\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.transform.TransformTree}}}}}. The tree can store both the statistics
(average, variance, minimum…) required to perform the transformations and the transformed values themselves
(centered valued, normalized values…). De \sphinxtitleref{default} refers to the default transformation. We refer to the
documentation of {\hyperref[\detokenize{features/transform::doc}]{\sphinxcrossref{\DUrole{doc}{Data and Kernel Transformations}}}} for further information.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Key
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}none\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}light\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}heavy\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}total\textquotesingle{}}}
\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_sample\_data\_default"}}
&
\sphinxAtStartPar
Transformed sample value of the default transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_sample\_data\_nondefault"}}
&
\sphinxAtStartPar
Transformed sample value of  another transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_sample\_statistics\_default"}}
&
\sphinxAtStartPar
Transformed sample statistics of the default transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_sample\_statistics\_nondefault"}}
&
\sphinxAtStartPar
Transformed sample statistics of another transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_oos\_data\_default"}}
&
\sphinxAtStartPar
Transformed out\sphinxhyphen{}of\sphinxhyphen{}sample value of the default transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_oos\_data\_nondefault"}}
&
\sphinxAtStartPar
Transformed out\sphinxhyphen{}of\sphinxhyphen{}sample value of  another transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_oos\_statistics\_default"}}
&
\sphinxAtStartPar
Transformed out\sphinxhyphen{}of\sphinxhyphen{}sample statistics of the default transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"transform\_oos\_statistics\_nondefault"}}
&
\sphinxAtStartPar
Transformed out\sphinxhyphen{}of\sphinxhyphen{}sample statistics of another transformation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsubsection{Level\sphinxhyphen{}Specific}
\label{\detokenize{features/cache:level-specific}}
\sphinxAtStartPar
The output of a level is also saved by default, until the sample or the model parameters change. The default representation
refers to \sphinxcode{\sphinxupquote{primal}} or \sphinxcode{\sphinxupquote{dual}}. We refer to the documentation of the {\hyperref[\detokenize{level/index::doc}]{\sphinxcrossref{\DUrole{doc}{Level Module}}}} for further information.
Many models require an identity matrix to solve the model. This matrix can be stored for further usage, unless the
dimensions change. The different constituents of the loss (regularization term, recontruction term…), referred to as \sphinxtitleref{sublosses}, are saved
independently from the total loss for monitoring. These are also resetted once the {\hyperref[\detokenize{features/module:kerch.feature.Module.before_step}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{before\_step()}}}}}
method is called.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Key
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}none\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}light\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}heavy\textquotesingle{}}}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{\textquotesingle{}total\textquotesingle{}}}
\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"forward\_sample\_default\_representation"}}
&
\sphinxAtStartPar
Output value of the sample in the default representation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"forward\_sample\_other\_representation"}}
&
\sphinxAtStartPar
Output value of the sample in the other representation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"forward\_oos\_default\_representation"}}
&
\sphinxAtStartPar
Output value of an out\sphinxhyphen{}of\sphinxhyphen{}sample in the default representation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"forward\_oos\_other\_representation"}}
&
\sphinxAtStartPar
Output value of an out\sphinxhyphen{}of\sphinxhyphen{}sample in the other representation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"Level\_I\_default\_respresentation"}}
&
\sphinxAtStartPar
Identity matrix in the default representation dimension
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"Level\_I\_other\_respresentation"}}
&
\sphinxAtStartPar
Identity matrix in the other representation dimension
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"Level\_subloss\_default\_respresentation"}}
&
\sphinxAtStartPar
Individual sublosses in the default representation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"Level\_subloss\_other\_respresentation"}}
&
\sphinxAtStartPar
Individual sublosses in the other representation
&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

&
\sphinxAtStartPar

\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{Abstract Class}
\label{\detokenize{features/cache:abstract-class}}\index{Cache (class in kerch.feature)@\spxentry{Cache}\spxextra{class in kerch.feature}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.feature.}}\sphinxbfcode{\sphinxupquote{Cache}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{features/module:kerch.feature.Module}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.module.Module}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{\_apply() (kerch.feature.Cache method)@\spxentry{\_apply()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache._apply}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_apply}}}{\emph{\DUrole{n}{fn}}, \emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
This if the native function by \sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.Module.html\#torch.nn.Module}{\sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}}, used when porting the
module. This ensures that the cache is also ported. This is used for example to port the data to the GPU or CPU.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This method is documented for completeness, but it should never be required to call it directly.
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_clean\_cache() (kerch.feature.Cache method)@\spxentry{\_clean\_cache()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache._clean_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_clean\_cache}}}{\emph{\DUrole{n}{max\_level}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}\DUrole{p}{{]}}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Cleans all cache elements above a certain level. This is relevant for cleaning the cache elements that have
been forced (see {\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{max\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{ | }}\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} all levels above this level will be cleaned, \sphinxcode{\sphinxupquote{max\_level}} excluded. Defaults to the default
cache level.

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This method is documented for completeness, but it should never be required to call it directly.
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_get() (kerch.feature.Cache method)@\spxentry{\_get()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache._get}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_get}}}{\emph{\DUrole{n}{key}}, \emph{\DUrole{n}{fun}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{level\_key}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{default\_level}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}normal\textquotesingle{}}}, \emph{\DUrole{n}{force}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{overwrite}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{persisting}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{destroy}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Any}{Any}}}
\pysigstopsignatures
\sphinxAtStartPar
Retrieves an element from the cache. If the element is not present, it saved to the cache provided its level
is lower or equal to the default level. This can be overwritten by the overwrite argument.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{key}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} key of the cache element.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{fun}} (\sphinxstyleliteralemphasis{\sphinxupquote{function handle}}) \textendash{} function to compute the element if not in the cache already.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{level\_key}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} key referencing the default level to use in \sphinxcode{\sphinxupquote{kerch.DEFAULT\_CACHE\_LEVELS}}. If not specified, the \sphinxcode{\sphinxupquote{default\_level}} argument is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{default\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} level where to save the cache element. Defaults to ‘normal’.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{force}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} if the value is \sphinxcode{\sphinxupquote{True}}, the element will nevertheless be saved whatever level is specified. Defaults to \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} These values are meant to persist after a cache reset when calling
{\hyperref[\detokenize{features/cache:kerch.feature.Cache.reset}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{reset()}}}}} with \sphinxcode{\sphinxupquote{reset\_persisting=False}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{destroy}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} This destroys the value from the cache after being read/computed. This is meant for short\sphinxhyphen{}term
memory. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
The result of \sphinxcode{\sphinxupquote{fun()}}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_remove\_from\_cache() (kerch.feature.Cache method)@\spxentry{\_remove\_from\_cache()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache._remove_from_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_remove\_from\_cache}}}{\emph{\DUrole{n}{key}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.List}{List}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}\DUrole{p}{{]}}}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Removes one or more specific element(s) from the cache.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{key}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{ | }}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{\sphinxstyleliteralemphasis{\sphinxupquote{list}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Key(s) of the cache element to be removed.

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This method is documented for completeness, but it should never be required to call it directly.
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_reset\_cache() (kerch.feature.Cache method)@\spxentry{\_reset\_cache()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache._reset_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_reset\_cache}}}{\emph{\DUrole{n}{reset\_persisting}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{True}}, \emph{\DUrole{n}{avoid\_classes}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
This just resets the cache and makes it empty.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset
(see {\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{avoid\_classes}} (list(type({\hyperref[\detokenize{features/cache:kerch.feature.Cache}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Cache}}}}})), optional) \textendash{} Class of which the elements must be avoided to be resetted. Default to \sphinxcode{\sphinxupquote{{[}{]}}}.

\end{itemize}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This method is documented for completeness, but it should never be required to call it directly.
\end{sphinxadmonition}

\end{fulllineitems}

\index{\_save() (kerch.feature.Cache method)@\spxentry{\_save()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache._save}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_save}}}{\emph{\DUrole{n}{key}}, \emph{\DUrole{n}{fun}}, \emph{\DUrole{n}{level\_key}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{default\_level}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{\textquotesingle{}total\textquotesingle{}}}, \emph{\DUrole{n}{force}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}, \emph{\DUrole{n}{persisting}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Any}{Any}}}
\pysigstopsignatures
\sphinxAtStartPar
Saves an element in the cache.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{key}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}) \textendash{} key of the cache element.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{fun}} (\sphinxstyleliteralemphasis{\sphinxupquote{function handle}}) \textendash{} function to compute the element if not in the cache already.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{level\_key}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} key referencing the default level to use in \sphinxcode{\sphinxupquote{kerch.DEFAULT\_CACHE\_LEVELS}}. If not specified, the \sphinxcode{\sphinxupquote{default\_level}} argument is used.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{default\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} level where to save the cache element. Defaults to ‘total’.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{force}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} if the value is \sphinxcode{\sphinxupquote{True}}, the element will nevertheless be saved whatever level is specified. Defaults to \sphinxcode{\sphinxupquote{False}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} These values are meant to persist after a cache reset when calling
{\hyperref[\detokenize{features/cache:kerch.feature.Cache.reset}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{reset()}}}}} with \sphinxcode{\sphinxupquote{reset\_persisting=False}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{itemize}

\item[{Returns}] \leavevmode
\sphinxAtStartPar
The result of \sphinxcode{\sphinxupquote{fun()}}

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.feature.Cache method)@\spxentry{cache\_keys()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.feature.Cache property)@\spxentry{cache\_level}\spxextra{kerch.feature.Cache property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{print\_cache() (kerch.feature.Cache method)@\spxentry{print\_cache()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.feature.Cache method)@\spxentry{reset()}\spxextra{kerch.feature.Cache method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/cache:kerch.feature.Cache.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\subsection{Examples}
\label{\detokenize{features/cache:examples}}
\sphinxAtStartPar
For a proper usage of the Kerch package, there is no need to manage the cache. For the sake of completeness, we however
provide two examples. The first one shows how the cache works on an existing implementation. The second example shows
how one can manage cache elements by itself.


\subsubsection{KPCA}
\label{\detokenize{features/cache:kpca}}
\sphinxAtStartPar
The following example illustrates the working of the cache. We will consider two examples of a
{\hyperref[\detokenize{level/kpca:kerch.level.KPCA}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.level.KPCA}}}}}, one with a \sphinxcode{\sphinxupquote{light}} cache level and another with a \sphinxcode{\sphinxupquote{total}} cache level.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}

\PYG{n}{torch}\PYG{o}{.}\PYG{n}{manual\PYGZus{}seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{n}{sample} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{oos} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{level}\PYG{o}{.}\PYG{n}{KPCA}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,}                  \PYG{c+c1}{\PYGZsh{} random sample}
                                    \PYG{n}{dim\PYGZus{}output}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}                   \PYG{c+c1}{\PYGZsh{} we want an output dimension of 2 (the input is 3)}
                                    \PYG{n}{sample\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{min}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} we want the input to be normalized (based on the statistics of the sample)}
                                    \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} we want the kernel to be center}
                                    \PYG{n}{cache\PYGZus{}level}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{light}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} a \PYGZsq{}light\PYGZsq{} cache level (only related to the sample)}

\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache} \PYG{o}{=} \PYG{n}{kerch}\PYG{o}{.}\PYG{n}{level}\PYG{o}{.}\PYG{n}{KPCA}\PYG{p}{(}\PYG{n}{sample}\PYG{o}{=}\PYG{n}{sample}\PYG{p}{,}                  \PYG{c+c1}{\PYGZsh{} idem}
                                    \PYG{n}{dim\PYGZus{}output}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,}                   \PYG{c+c1}{\PYGZsh{} idem}
                                    \PYG{n}{sample\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{min}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}       \PYG{c+c1}{\PYGZsh{} idem}
                                    \PYG{n}{kernel\PYGZus{}transform}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} idem}
                                    \PYG{n}{cache\PYGZus{}level}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{total}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}            \PYG{c+c1}{\PYGZsh{} a \PYGZsq{}total\PYGZsq{} cache level (saves everything)}
\end{sphinxVerbatim}

\sphinxAtStartPar
If we plot the cache now, nothing is printed: the cache is empty. The advantage of this package is that it does not perform any
unnecessary computations. Depending of what is required, it will compute what is strictly necessary.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
kernel\PYGZus{}implicit\PYGZus{}transform [none] : Transforms: 
	Mean centering (default)

KERNEL\PYGZus{}IMPLICIT\PYGZus{}TRANSFORM:
\end{sphinxVerbatim}

\sphinxAtStartPar
After solving the model and passing an out\sphinxhyphen{}of\sphinxhyphen{}sample through the model, we can see that the cache is pretty much
loaded. Nothing however has been saved on related to the out\sphinxhyphen{}of\sphinxhyphen{}sample, even if it has been computed. This is a
consequence of \sphinxcode{\sphinxupquote{light}} cache level of the module. Similarly, nor has the original non\sphinxhyphen{}centered kernel matrix, nor has
the original non\sphinxhyphen{}transformed sample been saved.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}
\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sample\PYGZus{}transform [none] : Transforms: 
	Minimum Centering (default)
K [light] : tensor([[ 0.7424,  0.0142, \PYGZhy{}0.2422, \PYGZhy{}0.2459, \PYGZhy{}0.2685],
        [ 0.0142,  0.6743, \PYGZhy{}0.2509, \PYGZhy{}0.1506, \PYGZhy{}0.2870],
        [\PYGZhy{}0.2422, \PYGZhy{}0.2509,  0.6180, \PYGZhy{}0.1665,  0.0415],
        [\PYGZhy{}0.2459, \PYGZhy{}0.1506, \PYGZhy{}0.1665,  0.7539, \PYGZhy{}0.1910],
        [\PYGZhy{}0.2685, \PYGZhy{}0.2870,  0.0415, \PYGZhy{}0.1910,  0.7049]])
kernel\PYGZus{}implicit\PYGZus{}transform [none] : Transforms: 
	Mean centering (default)

SAMPLE\PYGZus{}TRANSFORM:
Minimum Centering statistics\PYGZus{}sample [light] : tensor([\PYGZhy{}0.8567, \PYGZhy{}1.0845, \PYGZhy{}2.1788])
Minimum Centering data\PYGZus{}sample [light] : tensor([[2.3977, 0.7911, 0.0000],
        [1.4251, 0.0000, 0.7802],
        [1.2600, 1.9225, 1.4595],
        [0.4533, 0.4879, 2.3608],
        [0.0000, 2.1851, 1.1076]])

KERNEL\PYGZus{}IMPLICIT\PYGZus{}TRANSFORM:
Mean centering statistics\PYGZus{}sample [light] : (tensor([[0.2794],
        [0.3135],
        [0.3416],
        [0.2737],
        [0.2982]]), tensor(0.3013))
\end{sphinxVerbatim}

\sphinxAtStartPar
We can optionally reset the cache. Nothing is printed: the cache is empty again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{kpca\PYGZus{}light\PYGZus{}cache}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
We can now have a look at the \sphinxcode{\sphinxupquote{\textquotesingle{}total\textquotesingle{}}} version. Again, before anything is required, the cache remains empty: nothing
has computed yet.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
kernel\PYGZus{}implicit\PYGZus{}transform [none] : Transforms: 
	Mean centering (default)

KERNEL\PYGZus{}IMPLICIT\PYGZus{}TRANSFORM:
\end{sphinxVerbatim}

\sphinxAtStartPar
We now see that everything is saved.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache}\PYG{o}{.}\PYG{n}{solve}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{oos}\PYG{p}{)}
\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
sample\PYGZus{}transform [none] : Transforms: 
	Minimum Centering (default)
K [light] : tensor([[ 0.7424,  0.0142, \PYGZhy{}0.2422, \PYGZhy{}0.2459, \PYGZhy{}0.2685],
        [ 0.0142,  0.6743, \PYGZhy{}0.2509, \PYGZhy{}0.1506, \PYGZhy{}0.2870],
        [\PYGZhy{}0.2422, \PYGZhy{}0.2509,  0.6180, \PYGZhy{}0.1665,  0.0415],
        [\PYGZhy{}0.2459, \PYGZhy{}0.1506, \PYGZhy{}0.1665,  0.7539, \PYGZhy{}0.1910],
        [\PYGZhy{}0.2685, \PYGZhy{}0.2870,  0.0415, \PYGZhy{}0.1910,  0.7049]])
kernel\PYGZus{}implicit\PYGZus{}transform [none] : Transforms: 
	Mean centering (default)
forward\PYGZus{}140404332271072\PYGZus{}dual [normal] : tensor([[\PYGZhy{}0.0591,  0.6962],
        [\PYGZhy{}0.0009,  0.4885]])

SAMPLE\PYGZus{}TRANSFORM:
base data\PYGZus{}sample [normal] : tensor([[ 1.5410, \PYGZhy{}0.2934, \PYGZhy{}2.1788],
        [ 0.5684, \PYGZhy{}1.0845, \PYGZhy{}1.3986],
        [ 0.4033,  0.8380, \PYGZhy{}0.7193],
        [\PYGZhy{}0.4033, \PYGZhy{}0.5966,  0.1820],
        [\PYGZhy{}0.8567,  1.1006, \PYGZhy{}1.0712]])
base data\PYGZus{}oos\PYGZus{}140404332271072 [normal] : tensor([[ 0.1227, \PYGZhy{}0.5663,  0.3731],
        [\PYGZhy{}0.8920, \PYGZhy{}1.5091,  0.3704]])
Minimum Centering statistics\PYGZus{}sample [light] : tensor([\PYGZhy{}0.8567, \PYGZhy{}1.0845, \PYGZhy{}2.1788])
Minimum Centering data\PYGZus{}sample [light] : tensor([[2.3977, 0.7911, 0.0000],
        [1.4251, 0.0000, 0.7802],
        [1.2600, 1.9225, 1.4595],
        [0.4533, 0.4879, 2.3608],
        [0.0000, 2.1851, 1.1076]])
Minimum Centering statistics\PYGZus{}oos\PYGZus{}140404332271072 [normal] : tensor([\PYGZhy{}0.8567, \PYGZhy{}1.0845, \PYGZhy{}2.1788])
Minimum Centering data\PYGZus{}oos\PYGZus{}140404332271072 [normal] : tensor([[ 0.9794,  0.5182,  2.5519],
        [\PYGZhy{}0.0353, \PYGZhy{}0.4246,  2.5492]])

KERNEL\PYGZus{}IMPLICIT\PYGZus{}TRANSFORM:
base data\PYGZus{}oos\PYGZus{}140401075513104 [normal] : tensor([[9.3538e\PYGZhy{}03, 1.4093e\PYGZhy{}01, 1.7157e\PYGZhy{}01, 8.4308e\PYGZhy{}01, 4.2237e\PYGZhy{}02],
        [5.2590e\PYGZhy{}04, 5.1963e\PYGZhy{}02, 1.0564e\PYGZhy{}02, 5.4803e\PYGZhy{}01, 7.9825e\PYGZhy{}03]])
base data\PYGZus{}sample [normal] : tensor([[1.0000, 0.3058, 0.0776, 0.0059, 0.0079],
        [0.3058, 1.0000, 0.1029, 0.1353, 0.0234],
        [0.0776, 0.1029, 1.0000, 0.1476, 0.3801],
        [0.0059, 0.1353, 0.1476, 1.0000, 0.0796],
        [0.0079, 0.0234, 0.3801, 0.0796, 1.0000]])
Mean centering statistics\PYGZus{}sample [light] : (tensor([[0.2794],
        [0.3135],
        [0.3416],
        [0.2737],
        [0.2982]]), tensor(0.3013))
Mean centering statistics\PYGZus{}oos\PYGZus{}140401075513104 [normal] : (tensor([[0.2414],
        [0.1238]]), tensor(0.3013))
Mean centering data\PYGZus{}oos\PYGZus{}140401075513104 [normal] : tensor([[\PYGZhy{}0.2102, \PYGZhy{}0.1127, \PYGZhy{}0.1102,  0.6292, \PYGZhy{}0.1961],
        [\PYGZhy{}0.1014, \PYGZhy{}0.0841, \PYGZhy{}0.1536,  0.4518, \PYGZhy{}0.1127]])
\end{sphinxVerbatim}

\sphinxAtStartPar
We can optionally reset the cache again. Nothing is printed: the cache is empty again.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{kpca\PYGZus{}total\PYGZus{}cache}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsubsection{Managing the Cache}
\label{\detokenize{features/cache:managing-the-cache}}
\sphinxAtStartPar
In the following example, we show how we can add an element to the cache and recover it when called.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{kerch}
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{time}

\PYG{k}{class} \PYG{n+nc}{MyCacheExample}\PYG{p}{(}\PYG{n}{kerch}\PYG{o}{.}\PYG{n}{feature}\PYG{o}{.}\PYG{n}{Cache}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{MyCacheExample}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{big\PYGZus{}matrix} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{big\PYGZus{}matrix}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}compute\PYGZus{}qr}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{def} \PYG{n+nf}{qr\PYGZus{}fun}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{qr}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{big\PYGZus{}matrix}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}get}\PYG{p}{(}\PYG{n}{key}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{qr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fun}\PYG{o}{=}\PYG{n}{qr\PYGZus{}fun}\PYG{p}{)}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{q}\PYG{p}{,} \PYG{n}{r} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}compute\PYGZus{}qr}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{q}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{R}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{q}\PYG{p}{,} \PYG{n}{r} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}compute\PYGZus{}qr}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{r}

\PYG{c+c1}{\PYGZsh{} we instantiate our new class}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{my\PYGZus{}example} \PYG{o}{=} \PYG{n}{MyCacheExample}\PYG{p}{(}\PYG{n}{big\PYGZus{}matrix}\PYG{o}{=}\PYG{n}{m}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we time our Q property}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{my\PYGZus{}example}\PYG{o}{.}\PYG{n}{Q}
\PYG{n}{end} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{First access: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we time it again}
\PYG{n}{start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{my\PYGZus{}example}\PYG{o}{.}\PYG{n}{Q}
\PYG{n}{end} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Second access: }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} we now have a look at our cache}
\PYG{n}{my\PYGZus{}example}\PYG{o}{.}\PYG{n}{print\PYGZus{}cache}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
First access: 0.01804661750793457

Second access: 1.3589859008789062e\PYGZhy{}05

qr [normal] : torch.return\PYGZus{}types.linalg\PYGZus{}qr(
Q=tensor([[\PYGZhy{}0.0481,  0.0721, \PYGZhy{}0.0945,  ..., \PYGZhy{}0.0426,  0.1056, \PYGZhy{}0.0288],
        [ 0.0473, \PYGZhy{}0.0368,  0.0504,  ...,  0.0244,  0.1189, \PYGZhy{}0.1762],
        [\PYGZhy{}0.0010,  0.0414, \PYGZhy{}0.1085,  ...,  0.0197, \PYGZhy{}0.0063,  0.0128],
        ...,
        [ 0.0052,  0.0495,  0.0208,  ..., \PYGZhy{}0.0446,  0.0654, \PYGZhy{}0.0508],
        [\PYGZhy{}0.0971,  0.0800, \PYGZhy{}0.0674,  ..., \PYGZhy{}0.0472,  0.0399, \PYGZhy{}0.0133],
        [ 0.0359, \PYGZhy{}0.0140,  0.0036,  ..., \PYGZhy{}0.0727, \PYGZhy{}0.0499,  0.0026]]),
R=tensor([[\PYGZhy{}1.4523e+01, \PYGZhy{}1.3529e+00,  1.0283e+00,  ...,  1.3318e+00,
          1.0005e\PYGZhy{}03, \PYGZhy{}5.0223e\PYGZhy{}01],
        [ 0.0000e+00,  1.4382e+01, \PYGZhy{}1.0619e\PYGZhy{}02,  ..., \PYGZhy{}1.8346e+00,
         \PYGZhy{}7.0990e\PYGZhy{}02,  5.5942e\PYGZhy{}01],
        [ 0.0000e+00,  0.0000e+00,  1.3398e+01,  ...,  7.2516e\PYGZhy{}01,
          9.8586e\PYGZhy{}02, \PYGZhy{}6.8472e\PYGZhy{}01],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0209e+01,
         \PYGZhy{}5.2387e\PYGZhy{}01,  1.0148e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          9.7442e+00, \PYGZhy{}1.3787e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, \PYGZhy{}9.6815e+00]]))
\end{sphinxVerbatim}


\subsection{Inheritance Diagram}
\label{\detokenize{features/cache:inheritance-diagram}}
\sphinxincludegraphics[]{inheritance-c4ce678f399ebf46512fe3bd8611e6c8a5674747.pdf}

\sphinxstepscope


\section{Stochastic Training}
\label{\detokenize{features/stochastic:stochastic-training}}\label{\detokenize{features/stochastic::doc}}\index{Stochastic (class in kerch.feature)@\spxentry{Stochastic}\spxextra{class in kerch.feature}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.feature.}}\sphinxbfcode{\sphinxupquote{Stochastic}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{features/cache:kerch.feature.Cache}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.cache.Cache}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{cache\_keys() (kerch.feature.Stochastic method)@\spxentry{cache\_keys()}\spxextra{kerch.feature.Stochastic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.feature.Stochastic property)@\spxentry{cache\_level}\spxextra{kerch.feature.Stochastic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{idx (kerch.feature.Stochastic property)@\spxentry{idx}\spxextra{kerch.feature.Stochastic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{num\_idx (kerch.feature.Stochastic property)@\spxentry{num\_idx}\spxextra{kerch.feature.Stochastic property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{print\_cache() (kerch.feature.Stochastic method)@\spxentry{print\_cache()}\spxextra{kerch.feature.Stochastic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.feature.Stochastic method)@\spxentry{reset()}\spxextra{kerch.feature.Stochastic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{stochastic() (kerch.feature.Stochastic method)@\spxentry{stochastic()}\spxextra{kerch.feature.Stochastic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.feature.Stochastic method)@\spxentry{train()}\spxextra{kerch.feature.Stochastic method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/stochastic:kerch.feature.Stochastic.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\section{Sample}
\label{\detokenize{features/sample:sample}}\label{\detokenize{features/sample::doc}}\index{Sample (class in kerch.feature)@\spxentry{Sample}\spxextra{class in kerch.feature}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.feature.}}\sphinxbfcode{\sphinxupquote{Sample}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{features/stochastic:kerch.feature.Stochastic}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.feature.stochastic.Stochastic}}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{(}}\sphinxstyleliteralemphasis{\sphinxupquote{num\_sample}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{dim\_input}}\sphinxstyleliteralemphasis{\sphinxupquote{)}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used to compute the kernel matrix. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it will
be given relative to these samples., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_trainable}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} \sphinxtitleref{True} if the gradients of the sample points are to be computed. If so, a graph is
computed and the sample can be updated. \sphinxtitleref{False} just leads to a static computation., defaults to \sphinxtitleref{False}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{num\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Number of sample points. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and overwritten by
the number of points contained in sample., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{dim\_input}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Dimension of each sample point. This parameter is neglected if \sphinxtitleref{sample} is not \sphinxtitleref{None} and
overwritten by the dimension of the sample points., defaults to 1

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{\sphinxstyleliteralemphasis{\sphinxupquote{float}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_stochastic} and
\sphinxtitleref{prop\_stochastic} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} TODO

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{cache\_level}} (\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}}. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further
information.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{logging\_level}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Logging level for this specific instance.
If the value is \sphinxcode{\sphinxupquote{None}}, the current default kerch global log level will be used.
Defaults to \sphinxcode{\sphinxupquote{None}} (default kerch logging level).
We refer to the {\hyperref[\detokenize{features/logger::doc}]{\sphinxcrossref{\DUrole{doc}{Logging in Kerch}}}} documentation for further information.

\end{itemize}

\end{description}\end{quote}
\index{cache\_keys() (kerch.feature.Sample method)@\spxentry{cache\_keys()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.feature.Sample property)@\spxentry{cache\_level}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{current\_sample (kerch.feature.Sample property)@\spxentry{current\_sample}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.current_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{current\_sample\_projected (kerch.feature.Sample property)@\spxentry{current\_sample\_projected}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.current_sample_projected}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{current\_sample\_projected}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns the sample that is currently used in the computations and for the normalizing and centering statistics
if relevant.

\end{fulllineitems}

\index{dim\_input (kerch.feature.Sample property)@\spxentry{dim\_input}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.dim_input}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{dim\_input}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Dimension of each datapoint.

\end{fulllineitems}

\index{empty\_sample (kerch.feature.Sample property)@\spxentry{empty\_sample}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.empty_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{empty\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean specifying if the sample is empty or not.

\end{fulllineitems}

\index{hparams\_fixed (kerch.feature.Sample property)@\spxentry{hparams\_fixed}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.hparams_fixed}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_fixed}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Fixed hyperparameters of the module. By contrast with {\hyperref[\detokenize{features/sample:kerch.feature.Sample.hparams_variable}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_variable}}}}}, these are the values that are fixed and
cannot possibly change during the training. If applicable, these can be specific architecture values for example.
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{hparams\_variable (kerch.feature.Sample property)@\spxentry{hparams\_variable}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.hparams_variable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{hparams\_variable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\sphinxAtStartPar
Variable hyperparameters of the module. By contrast with {\hyperref[\detokenize{features/sample:kerch.feature.Sample.hparams_fixed}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{hparams\_fixed}}}}}, these are the values that are may change during
the training and may be monitored at various stages during the training.
If applicable, these can be kernel bandwidth parameters for example.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
All parameters that are potentially trainable, like a kernel bandwidth \(\sigma\) for example, are
included in this dictionary, even if the corresponding trainable argument is set to \sphinxcode{\sphinxupquote{False}}. In the
latter case, they will be not evolve during training iterations, but will still be included in this
dictionary.
\end{sphinxadmonition}

\sphinxAtStartPar
We refer to the documentation of {\hyperref[\detokenize{features/module::doc}]{\sphinxcrossref{\DUrole{doc}{Kerch Module}}}} for further information.

\end{fulllineitems}

\index{idx (kerch.feature.Sample property)@\spxentry{idx}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Indices used when performing various operations. This is only relevant in the case of stochastic training.

\end{fulllineitems}

\index{init\_sample() (kerch.feature.Sample method)@\spxentry{init\_sample()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.init_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{init\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Initializes the sample set (and the stochastic indices).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Sample points used for the various computations. When an out\sphinxhyphen{}of\sphinxhyphen{}sample computation is asked, it
will be given relative to these samples. In case of overwriting a current sample, \sphinxtitleref{num\_sample} and
\sphinxtitleref{dim\_input} are also overwritten. If \sphinxtitleref{None} is specified, the sample data will be initialized according
to \sphinxtitleref{num\_sample} and \sphinxtitleref{dim\_input} specified during the construction. If a previous sample set has been used,
it will keep the same dimension by consequence. A last case occurs when \sphinxtitleref{sample} is of the class
\sphinxtitleref{torch.nn.Parameter}: the sample will then use those values, and they can thus be shared with the level
calling this method., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Initializes the indices of the samples to be updated. All indices are considered if both
\sphinxtitleref{idx\_sample} and \sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop\_sample}} \textendash{} Instead of giving indices, specifying a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_sample} \(\leq 1\). All indices are considered if both \sphinxtitleref{idx\_sample} and
\sphinxtitleref{prop\_sample} are \sphinxtitleref{None}., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_idx (kerch.feature.Sample property)@\spxentry{num\_idx}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.num_idx}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_idx}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of selected indices when performing various operations. This is only relevant in the case of stochastic
training.

\end{fulllineitems}

\index{num\_sample (kerch.feature.Sample property)@\spxentry{num\_sample}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.num_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{num\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{int}}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of datapoints in the sample set.

\end{fulllineitems}

\index{print\_cache() (kerch.feature.Sample method)@\spxentry{print\_cache()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (kerch.feature.Sample method)@\spxentry{reset()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.feature.Sample property)@\spxentry{sample}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\#torch.nn.parameter.Parameter}{torch.nn.parameter.Parameter}}}}
\pysigstopsignatures
\sphinxAtStartPar
Full original raw sample without any transform or potential stochastic selection.

\end{fulllineitems}

\index{sample\_trainable (kerch.feature.Sample property)@\spxentry{sample\_trainable}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.sample_trainable}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_trainable}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\sphinxAtStartPar
Boolean if the sample data can be trained.

\end{fulllineitems}

\index{sample\_transform (kerch.feature.Sample property)@\spxentry{sample\_transform}\spxextra{kerch.feature.Sample property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.sample_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree}]{\sphinxcrossref{kerch.transform.TransformTree.TransformTree}}}}}}
\pysigstopsignatures
\sphinxAtStartPar
Default transform used by the sample.

\end{fulllineitems}

\index{stochastic() (kerch.feature.Sample method)@\spxentry{stochastic()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.stochastic}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{stochastic}}}{\emph{\DUrole{n}{idx}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{prop}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Resets which subset of the samples are to be used until the next call of this function. This is relevant in the
case of stochastic training.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the sample subset relative to the original sample set., defaults to \sphinxtitleref{None}

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{prop}} (\sphinxstyleliteralemphasis{\sphinxupquote{double}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Instead of giving indices, passing a proportion of the original sample set is also
possible. The indices will be uniformly randomly chosen without replacement. The value must be chosen
such that \(0 <\) \sphinxtitleref{prop\_stochastic} \(\leq 1\)., defaults to \sphinxtitleref{None}.

\end{itemize}

\end{description}\end{quote}

\sphinxAtStartPar
If \sphinxtitleref{None} is specified for both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic}, all samples are used and the subset equals the
original sample set. This is also the default behavior if this function is never called, nor the parameters
specified during initialization.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Both \sphinxtitleref{idx\_stochastic} and \sphinxtitleref{prop\_stochastic} cannot be filled together as conflict would arise.
\end{sphinxadmonition}

\end{fulllineitems}

\index{train() (kerch.feature.Sample method)@\spxentry{train()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{train}}}{\emph{\DUrole{n}{mode}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Activates the training mode, which disables the gradients computation and disables stochasticity. For the
gradients and other things, we refer to the \sphinxtitleref{torch.nn.Module} documentation. For the stochastic part, when put
in evaluation mode (\sphinxtitleref{False}), all the sample points are used for the computations, regardless of
the previously specified indices.

\end{fulllineitems}

\index{transform\_input() (kerch.feature.Sample method)@\spxentry{transform\_input()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.transform_input}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_input}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Optional}{Optional}\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Apply to value the same transform as on the sample.

\end{fulllineitems}

\index{transform\_sample\_revert() (kerch.feature.Sample method)@\spxentry{transform\_sample\_revert()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.transform_sample_revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{transform\_sample\_revert}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Get back the original value from a projected value, by using the same transform as the sample,
but in reverse. This is not always feasible, depending on the transform used (normalizations are
typically not invertible as they are transform which are not bijective).

\end{fulllineitems}

\index{update\_sample() (kerch.feature.Sample method)@\spxentry{update\_sample()}\spxextra{kerch.feature.Sample method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/sample:kerch.feature.Sample.update_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{update\_sample}}}{\emph{\DUrole{n}{sample\_values}}, \emph{\DUrole{n}{idx\_sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Updates the sample set. In contradiction to \sphinxtitleref{init\_samples}, this only updates the values of the sample and sets
the gradients of the updated values to zero if relevant.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample\_values}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Values given to the updated samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{idx\_sample}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#int}{\sphinxstyleliteralemphasis{\sphinxupquote{int}}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Indices of the samples to be updated. All indices are considered if \sphinxtitleref{None}., defaults to
\sphinxtitleref{None}

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\section{Data and Kernel Transformations}
\label{\detokenize{features/transform:module-transform}}\label{\detokenize{features/transform:data-and-kernel-transformations}}\label{\detokenize{features/transform::doc}}\index{module@\spxentry{module}!transform@\spxentry{transform}}\index{transform@\spxentry{transform}!module@\spxentry{module}}\index{TransformTree (class in kerch.transform)@\spxentry{TransformTree}\spxextra{class in kerch.transform}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.transform.}}\sphinxbfcode{\sphinxupquote{TransformTree}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxcode{\sphinxupquote{kerch.transform.Transform.Transform}}

\sphinxAtStartPar
Creates a tree of transform for efficient computing, with cache management.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{explicit}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}) \textendash{} True is the transform are to be computed in the explicit formulation, False instead.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{sample}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}\sphinxstyleliteralemphasis{\sphinxupquote{ or }}\sphinxstyleliteralemphasis{\sphinxupquote{function handle}}) \textendash{} Default sample on which the statistics are to be computed.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{default\_transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Optional default list of transform., defaults to None.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{diag\_fun}} (\sphinxstyleliteralemphasis{\sphinxupquote{Function handle}}) \textendash{} Optional function handle to directly compute the diagonal of the implicit formulation to increase
computation speed.

\end{itemize}

\end{description}\end{quote}
\index{add\_offspring() (kerch.transform.TransformTree method)@\spxentry{add\_offspring()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.add_offspring}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_offspring}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{kerch.transform.Transform.Transform}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{add\_parent() (kerch.transform.TransformTree method)@\spxentry{add\_parent()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.add_parent}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_parent}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{kerch.transform.Transform.Transform}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{apply() (kerch.transform.TransformTree method)@\spxentry{apply()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.apply}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{apply}}}{\emph{\DUrole{n}{oos}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{transform}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.List}{List}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Applies the transform to the value to out\sphinxhyphen{}of\sphinxhyphen{}sample data. Either value is a function handle and you can use
x (explicit) and x, y (explicit) to specify the data. Either directly give a Tensor.

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
If value is a Tensor, some transform may not work in implicit formulation. For example, the unit sphere
normalization requires k(x,x) for all out\sphinxhyphen{}of\sphinxhyphen{}sample points. Some combinations may be even more intricate.
\end{sphinxadmonition}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Relevant if using a function handle for value.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{y}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Relevant if using a function handle for value in implicit mode.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Transforms to be used. If none are to be used, i.e. getting the raw data back, please
specify {[}{]}, not None, which will return the default transform used for the sample., defaults to None,
i.e., the default transform.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{beautify\_transform() (kerch.transform.TransformTree static method)@\spxentry{beautify\_transform()}\spxextra{kerch.transform.TransformTree static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.beautify_transform}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{beautify\_transform}}}{\emph{\DUrole{n}{transform}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#list}{list}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Union}{Union}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.List}{List}\DUrole{p}{{[}}kerch.transform.Transform.Transform\DUrole{p}{{]}}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Creates a list of \_Transform classes and removes duplicates.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} list of the different transform.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_keys() (kerch.transform.TransformTree method)@\spxentry{cache\_keys()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.cache_keys}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{cache\_keys}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/typing.html\#typing.Iterable}{Iterable}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns an iterable containing the different cache keys.
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{cache\_level (kerch.transform.TransformTree property)@\spxentry{cache\_level}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.cache_level}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{cache\_level}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}
\pysigstopsignatures
\sphinxAtStartPar
Cache level for saving temporary execution results during the execution. The higher the cache,
the more is saved. Defaults to \sphinxcode{\sphinxupquote{\textquotesingle{}normal\textquotesingle{}}} unless set otherwise during instantiation. The different possible
values are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"none"}}: the cache is non\sphinxhyphen{}existent and everything is computed on the go.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"light"}}: the cache is very light. For example, only the kernel matrix and statistics of the sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"normal"}}: same as light, but the statistics of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are also saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"heavy"}}: in addition to the statistics, the final kernel matrices of the out\sphinxhyphen{}of\sphinxhyphen{}sample points are saved.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{"total"}}: every step of any computation is saved.

\end{itemize}

\sphinxAtStartPar
We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.

\end{fulllineitems}

\index{default (kerch.transform.TransformTree property)@\spxentry{default}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.default}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{default}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{default\_transforms (kerch.transform.TransformTree property)@\spxentry{default\_transforms}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.default_transforms}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{default\_transforms}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }List}}}
\pysigstopsignatures
\sphinxAtStartPar
Default list of transforms to be applied.

\end{fulllineitems}

\index{explicit (kerch.transform.TransformTree property)@\spxentry{explicit}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.explicit}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{explicit}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{final\_transform (kerch.transform.TransformTree property)@\spxentry{final\_transform}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.final_transform}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{final\_transform}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/abc.html\#abc.ABCMeta}{abc.ABCMeta}}}}
\pysigstopsignatures
\sphinxAtStartPar
Final transform to be applied, which is the last element of
{\hyperref[\detokenize{features/transform:kerch.transform.TransformTree.default_transforms}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{default\_transforms}}}}}.

\end{fulllineitems}

\index{is\_leaf (kerch.transform.TransformTree property)@\spxentry{is\_leaf}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.is_leaf}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{is\_leaf}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{is\_root (kerch.transform.TransformTree property)@\spxentry{is\_root}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.is_root}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{is\_root}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{offspring (kerch.transform.TransformTree property)@\spxentry{offspring}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.offspring}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{offspring}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{dict}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{oos() (kerch.transform.TransformTree method)@\spxentry{oos()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.oos}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{oos}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}

\index{parent (kerch.transform.TransformTree property)@\spxentry{parent}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.parent}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{parent}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }Optional\DUrole{p}{{[}}kerch.transform.Transform.Transform\DUrole{p}{{]}}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{print\_cache() (kerch.transform.TransformTree method)@\spxentry{print\_cache()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.print_cache}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_cache}}}{\emph{\DUrole{n}{private}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Prints the cache content. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for further information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{private}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Some cache elements are private and are not returned unless set to \sphinxcode{\sphinxupquote{True}}. Defaults to \sphinxcode{\sphinxupquote{False}}.

\end{description}\end{quote}

\end{fulllineitems}

\index{projected\_sample (kerch.transform.TransformTree property)@\spxentry{projected\_sample}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.projected_sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{projected\_sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\sphinxAtStartPar
Sample after transform. Retrieved from cache if relevant.

\end{fulllineitems}

\index{reset() (kerch.transform.TransformTree method)@\spxentry{reset()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.reset}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{\DUrole{n}{recurse}\DUrole{o}{=}\DUrole{default_value}{False}}, \emph{\DUrole{n}{reset\_persisting}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\sphinxAtStartPar
Resets the cache to be empty. We refer to the {\hyperref[\detokenize{features/cache::doc}]{\sphinxcrossref{\DUrole{doc}{Cache Management}}}} documentation for more information.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{recurse}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} If \sphinxcode{\sphinxupquote{True}}, resets the cache of this module and also of its potential children. otherwise,
it only resets the cache for this module. Defaults to \sphinxcode{\sphinxupquote{True}}.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{reset\_persisting}} (\sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{\sphinxstyleliteralemphasis{\sphinxupquote{bool}}}\sphinxstyleliteralemphasis{\sphinxupquote{, }}\sphinxstyleliteralemphasis{\sphinxupquote{optional}}) \textendash{} Persisting elements are meant to resist to a cache reset (see
{\hyperref[\detokenize{features/cache:kerch.feature.Cache._save}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{\_save()}}}}}). The option allows to also reset them if \sphinxcode{\sphinxupquote{True}}. Defaults to
\sphinxcode{\sphinxupquote{True}}.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{revert() (kerch.transform.TransformTree method)@\spxentry{revert()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.revert}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{revert}}}{\emph{\DUrole{n}{value}}, \emph{\DUrole{n}{transform}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/typing.html\#typing.List}{List}\DUrole{p}{{[}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}\DUrole{p}{{]}}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\sphinxAtStartPar
Reverts the transform (runs the tree backwards) to the value to out\sphinxhyphen{}of\sphinxhyphen{}sample data.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{value}} (\sphinxstyleliteralemphasis{\sphinxupquote{Tensor}}) \textendash{} Out\sphinxhyphen{}of\sphinxhyphen{}sample data.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{transform}} (\sphinxstyleliteralemphasis{\sphinxupquote{List}}\sphinxstyleliteralemphasis{\sphinxupquote{{[}}}\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxstyleliteralemphasis{\sphinxupquote{str}}}\sphinxstyleliteralemphasis{\sphinxupquote{{]}}}) \textendash{} Transforms to be used. If none are to be used, i.e. getting the raw data back, please
specify {[}{]}, not None, which will return the default transform used for the sample., defaults to None,
i.e., the default transform.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{sample (kerch.transform.TransformTree property)@\spxentry{sample}\spxextra{kerch.transform.TransformTree property}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.sample}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{property\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{sample}}\sphinxbfcode{\sphinxupquote{\DUrole{p}{:}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{statistics\_oos() (kerch.transform.TransformTree method)@\spxentry{statistics\_oos()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.statistics_oos}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{statistics\_oos}}}{\emph{\DUrole{n}{x}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{y}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{oos}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ Union\DUrole{p}{{[}}\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{,}\DUrole{w}{  }\sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{p}{{]}}}}
\pysigstopsignatures
\end{fulllineitems}

\index{statistics\_sample() (kerch.transform.TransformTree method)@\spxentry{statistics\_sample()}\spxextra{kerch.transform.TransformTree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{features/transform:kerch.transform.TransformTree.statistics_sample}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{statistics\_sample}}}{\emph{\DUrole{n}{sample}\DUrole{o}{=}\DUrole{default_value}{None}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}}}
\pysigstopsignatures
\end{fulllineitems}


\end{fulllineitems}


\sphinxstepscope


\chapter{Views and Levels}
\label{\detokenize{views/index:views-and-levels}}\label{\detokenize{views/index::doc}}
\sphinxstepscope


\chapter{Utilitaries}
\label{\detokenize{general/utils:utilitaries}}\label{\detokenize{general/utils::doc}}

\section{Defaults}
\label{\detokenize{general/utils:module-kerch.utils.defaults}}\label{\detokenize{general/utils:defaults}}\index{module@\spxentry{module}!kerch.utils.defaults@\spxentry{kerch.utils.defaults}}\index{kerch.utils.defaults@\spxentry{kerch.utils.defaults}!module@\spxentry{module}}

\section{Casting}
\label{\detokenize{general/utils:module-kerch.utils.cast}}\label{\detokenize{general/utils:casting}}\index{module@\spxentry{module}!kerch.utils.cast@\spxentry{kerch.utils.cast}}\index{kerch.utils.cast@\spxentry{kerch.utils.cast}!module@\spxentry{module}}\index{capitalize\_only\_first() (in module kerch.utils.cast)@\spxentry{capitalize\_only\_first()}\spxextra{in module kerch.utils.cast}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.cast.capitalize_only_first}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.cast.}}\sphinxbfcode{\sphinxupquote{capitalize\_only\_first}}}{\emph{\DUrole{n}{val}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}}
\pysigstopsignatures
\end{fulllineitems}

\index{castf() (in module kerch.utils.cast)@\spxentry{castf()}\spxextra{in module kerch.utils.cast}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.cast.castf}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.cast.}}\sphinxbfcode{\sphinxupquote{castf}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{dev}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{tensor}\DUrole{o}{=}\DUrole{default_value}{True}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{casti() (in module kerch.utils.cast)@\spxentry{casti()}\spxextra{in module kerch.utils.cast}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.cast.casti}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.cast.}}\sphinxbfcode{\sphinxupquote{casti}}}{\emph{\DUrole{n}{x}}, \emph{\DUrole{n}{dev}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{tensor}\DUrole{o}{=}\DUrole{default_value}{False}}}{{ $\rightarrow$ \sphinxhref{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{torch.Tensor}\DUrole{w}{  }\DUrole{p}{|}\DUrole{w}{  }\sphinxhref{https://docs.python.org/3/library/constants.html\#None}{None}}}
\pysigstopsignatures
\end{fulllineitems}

\index{check\_representation() (in module kerch.utils.cast)@\spxentry{check\_representation()}\spxextra{in module kerch.utils.cast}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.cast.check_representation}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.cast.}}\sphinxbfcode{\sphinxupquote{check\_representation}}}{\emph{\DUrole{n}{representation}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{default}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{str}}\DUrole{w}{  }\DUrole{o}{=}\DUrole{w}{  }\DUrole{default_value}{None}}, \emph{\DUrole{n}{cls}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\end{fulllineitems}



\section{Decorators}
\label{\detokenize{general/utils:module-kerch.utils.decorators}}\label{\detokenize{general/utils:decorators}}\index{module@\spxentry{module}!kerch.utils.decorators@\spxentry{kerch.utils.decorators}}\index{kerch.utils.decorators@\spxentry{kerch.utils.decorators}!module@\spxentry{module}}\index{extend\_docstring (class in kerch.utils.decorators)@\spxentry{extend\_docstring}\spxextra{class in kerch.utils.decorators}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.decorators.extend_docstring}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.decorators.}}\sphinxbfcode{\sphinxupquote{extend\_docstring}}}{\emph{\DUrole{n}{method}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{kwargs\_decorator() (in module kerch.utils.decorators)@\spxentry{kwargs\_decorator()}\spxextra{in module kerch.utils.decorators}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.decorators.kwargs_decorator}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.decorators.}}\sphinxbfcode{\sphinxupquote{kwargs\_decorator}}}{\emph{\DUrole{n}{dict\_kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\begin{quote}\begin{description}
\item[{special\sphinxhyphen{}members}] \leavevmode\begin{quote}\begin{description}
\item[{show\sphinxhyphen{}inheritance}] \leavevmode
\end{description}\end{quote}

\end{description}\end{quote}


\section{Mathematics}
\label{\detokenize{general/utils:module-kerch.utils.math}}\label{\detokenize{general/utils:mathematics}}\index{module@\spxentry{module}!kerch.utils.math@\spxentry{kerch.utils.math}}\index{kerch.utils.math@\spxentry{kerch.utils.math}!module@\spxentry{module}}\index{eigs() (in module kerch.utils.math)@\spxentry{eigs()}\spxextra{in module kerch.utils.math}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.math.eigs}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.math.}}\sphinxbfcode{\sphinxupquote{eigs}}}{\emph{\DUrole{n}{A}}, \emph{\DUrole{n}{k}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{B}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{psd}\DUrole{o}{=}\DUrole{default_value}{True}}, \emph{\DUrole{n}{sym}\DUrole{o}{=}\DUrole{default_value}{True}}}{}
\pysigstopsignatures
\end{fulllineitems}



\section{Typing}
\label{\detokenize{general/utils:module-kerch.utils.type}}\label{\detokenize{general/utils:typing}}\index{module@\spxentry{module}!kerch.utils.type@\spxentry{kerch.utils.type}}\index{kerch.utils.type@\spxentry{kerch.utils.type}!module@\spxentry{module}}\index{gpu\_available() (in module kerch.utils.type)@\spxentry{gpu\_available()}\spxextra{in module kerch.utils.type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.type.gpu_available}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.type.}}\sphinxbfcode{\sphinxupquote{gpu\_available}}}{}{{ $\rightarrow$ \sphinxhref{https://docs.python.org/3/library/functions.html\#bool}{bool}}}
\pysigstopsignatures
\sphinxAtStartPar
Returns whether GPU\sphinxhyphen{}enhanced computation is possible and configured on this machine.

\end{fulllineitems}

\index{set\_eps() (in module kerch.utils.type)@\spxentry{set\_eps()}\spxextra{in module kerch.utils.type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.type.set_eps}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.type.}}\sphinxbfcode{\sphinxupquote{set\_eps}}}{\emph{\DUrole{n}{eps}\DUrole{p}{:}\DUrole{w}{  }\DUrole{n}{\sphinxhref{https://docs.python.org/3/library/functions.html\#float}{float}}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{set\_ftype() (in module kerch.utils.type)@\spxentry{set\_ftype()}\spxextra{in module kerch.utils.type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.type.set_ftype}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.type.}}\sphinxbfcode{\sphinxupquote{set\_ftype}}}{\emph{\DUrole{n}{type}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{set\_itype() (in module kerch.utils.type)@\spxentry{set\_itype()}\spxextra{in module kerch.utils.type}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.type.set_itype}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{kerch.utils.type.}}\sphinxbfcode{\sphinxupquote{set\_itype}}}{\emph{\DUrole{n}{type}}}{}
\pysigstopsignatures
\end{fulllineitems}



\section{Errors}
\label{\detokenize{general/utils:module-kerch.utils.errors}}\label{\detokenize{general/utils:errors}}\index{module@\spxentry{module}!kerch.utils.errors@\spxentry{kerch.utils.errors}}\index{kerch.utils.errors@\spxentry{kerch.utils.errors}!module@\spxentry{module}}\index{BijectionError@\spxentry{BijectionError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.BijectionError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{BijectionError}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{general/utils:kerch.utils.errors.KerchError}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.utils.errors.KerchError}}}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.BijectionError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.BijectionError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.BijectionError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.BijectionError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.BijectionError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.BijectionError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.BijectionError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.BijectionError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.BijectionError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}


\end{fulllineitems}

\index{ExplicitError@\spxentry{ExplicitError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ExplicitError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{ExplicitError}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{general/utils:kerch.utils.errors.KerchError}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.utils.errors.KerchError}}}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.ExplicitError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.ExplicitError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ExplicitError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_annotations\_\_ (kerch.utils.errors.ExplicitError attribute)@\spxentry{\_\_annotations\_\_}\spxextra{kerch.utils.errors.ExplicitError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ExplicitError.__annotations__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_annotations\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\{\}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.ExplicitError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.ExplicitError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ExplicitError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.ExplicitError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.ExplicitError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ExplicitError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}


\end{fulllineitems}

\index{ImplicitError@\spxentry{ImplicitError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ImplicitError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{ImplicitError}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{general/utils:kerch.utils.errors.KerchError}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.utils.errors.KerchError}}}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.ImplicitError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.ImplicitError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ImplicitError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_annotations\_\_ (kerch.utils.errors.ImplicitError attribute)@\spxentry{\_\_annotations\_\_}\spxextra{kerch.utils.errors.ImplicitError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ImplicitError.__annotations__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_annotations\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\{\}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.ImplicitError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.ImplicitError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ImplicitError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.ImplicitError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.ImplicitError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.ImplicitError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}


\end{fulllineitems}

\index{KerchError@\spxentry{KerchError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.KerchError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{KerchError}}}{\emph{\DUrole{n}{cls}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{message}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: \sphinxhref{https://docs.python.org/3/library/exceptions.html\#Exception}{\sphinxcode{\sphinxupquote{Exception}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.KerchError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.KerchError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.KerchError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\textquotesingle{}\_\_init\_\_\textquotesingle{}\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_annotations\_\_ (kerch.utils.errors.KerchError attribute)@\spxentry{\_\_annotations\_\_}\spxextra{kerch.utils.errors.KerchError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.KerchError.__annotations__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_annotations\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\{\}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.KerchError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.KerchError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.KerchError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{abstract\DUrole{w}{  }}}\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{n}{cls}\DUrole{o}{=}\DUrole{default_value}{None}}, \emph{\DUrole{n}{message}\DUrole{o}{=}\DUrole{default_value}{\textquotesingle{}\textquotesingle{}}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.KerchError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.KerchError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.KerchError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}

\index{\_\_weakref\_\_ (kerch.utils.errors.KerchError attribute)@\spxentry{\_\_weakref\_\_}\spxextra{kerch.utils.errors.KerchError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.KerchError.__weakref__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_weakref\_\_}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/types.html\#types.GetSetDescriptorType}{\sphinxcode{\sphinxupquote{GetSetDescriptorType}}}

\sphinxAtStartPar
list of weak references to the object

\end{fulllineitems}


\end{fulllineitems}

\index{MultiViewError@\spxentry{MultiViewError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.MultiViewError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{MultiViewError}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{general/utils:kerch.utils.errors.KerchError}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.utils.errors.KerchError}}}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.MultiViewError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.MultiViewError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.MultiViewError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_annotations\_\_ (kerch.utils.errors.MultiViewError attribute)@\spxentry{\_\_annotations\_\_}\spxextra{kerch.utils.errors.MultiViewError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.MultiViewError.__annotations__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_annotations\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\{\}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.MultiViewError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.MultiViewError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.MultiViewError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.MultiViewError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.MultiViewError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.MultiViewError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}


\end{fulllineitems}

\index{NotInitializedError@\spxentry{NotInitializedError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.NotInitializedError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{NotInitializedError}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{general/utils:kerch.utils.errors.KerchError}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.utils.errors.KerchError}}}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.NotInitializedError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.NotInitializedError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.NotInitializedError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_annotations\_\_ (kerch.utils.errors.NotInitializedError attribute)@\spxentry{\_\_annotations\_\_}\spxextra{kerch.utils.errors.NotInitializedError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.NotInitializedError.__annotations__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_annotations\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\{\}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.NotInitializedError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.NotInitializedError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.NotInitializedError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.NotInitializedError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.NotInitializedError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.NotInitializedError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}


\end{fulllineitems}

\index{RepresentationError@\spxentry{RepresentationError}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.RepresentationError}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{exception\DUrole{w}{  }}}\sphinxcode{\sphinxupquote{kerch.utils.errors.}}\sphinxbfcode{\sphinxupquote{RepresentationError}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Bases: {\hyperref[\detokenize{general/utils:kerch.utils.errors.KerchError}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{kerch.utils.errors.KerchError}}}}}
\index{\_\_abstractmethods\_\_ (kerch.utils.errors.RepresentationError attribute)@\spxentry{\_\_abstractmethods\_\_}\spxextra{kerch.utils.errors.RepresentationError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.RepresentationError.__abstractmethods__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_abstractmethods\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }frozenset(\{\})}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#frozenset}{\sphinxcode{\sphinxupquote{frozenset}}}

\end{fulllineitems}

\index{\_\_annotations\_\_ (kerch.utils.errors.RepresentationError attribute)@\spxentry{\_\_annotations\_\_}\spxextra{kerch.utils.errors.RepresentationError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.RepresentationError.__annotations__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_annotations\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\{\}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#dict}{\sphinxcode{\sphinxupquote{dict}}}

\end{fulllineitems}

\index{\_\_init\_\_() (kerch.utils.errors.RepresentationError method)@\spxentry{\_\_init\_\_()}\spxextra{kerch.utils.errors.RepresentationError method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.RepresentationError.__init__}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{\_\_init\_\_}}}{\emph{\DUrole{o}{*}\DUrole{n}{args}}, \emph{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\end{fulllineitems}

\index{\_\_module\_\_ (kerch.utils.errors.RepresentationError attribute)@\spxentry{\_\_module\_\_}\spxextra{kerch.utils.errors.RepresentationError attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{general/utils:kerch.utils.errors.RepresentationError.__module__}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{\_\_module\_\_}}\sphinxbfcode{\sphinxupquote{\DUrole{w}{  }\DUrole{p}{=}\DUrole{w}{  }\textquotesingle{}kerch.utils.errors\textquotesingle{}}}}
\pysigstopsignatures
\sphinxAtStartPar
\sphinxstylestrong{Type:}    \sphinxhref{https://docs.python.org/3/library/stdtypes.html\#str}{\sphinxcode{\sphinxupquote{str}}}

\end{fulllineitems}


\end{fulllineitems}



\section{Tensors}
\label{\detokenize{general/utils:tensors}}
\sphinxstepscope


\chapter{Structure}
\label{\detokenize{structure/index:structure}}\label{\detokenize{structure/index::doc}}

\chapter{References}
\label{\detokenize{index:references}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{f}
\item\relax\sphinxstyleindexentry{feature}\sphinxstyleindexpageref{features/index:\detokenize{module-feature}}
\indexspace
\bigletter{k}
\item\relax\sphinxstyleindexentry{kerch.utils.defaults}\sphinxstyleindexpageref{general/utils:\detokenize{module-kerch.utils.defaults}}
\item\relax\sphinxstyleindexentry{kerch.utils.errors}\sphinxstyleindexpageref{general/utils:\detokenize{module-kerch.utils.errors}}
\indexspace
\bigletter{t}
\item\relax\sphinxstyleindexentry{transform}\sphinxstyleindexpageref{features/transform:\detokenize{module-transform}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}